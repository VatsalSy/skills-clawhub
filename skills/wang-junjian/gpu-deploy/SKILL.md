---
name: gpu-deploy
description: "åœ¨ GPU æœåŠ¡å™¨ä¸Šéƒ¨ç½² vLLM æ¨¡å‹æœåŠ¡ã€‚æ”¯æŒå¤šæœåŠ¡å™¨é…ç½®ï¼Œè‡ªåŠ¨æ£€æŸ¥ GPU å’Œç«¯å£å ç”¨ï¼Œä¸€é”®éƒ¨ç½²æµè¡Œçš„å¼€æºæ¨¡å‹ã€‚"
homepage: https://github.com/vllm-project/vllm
metadata: { "openclaw": { "emoji": "ğŸš€", "requires": { "bins": ["ssh"] } } }
---

# ğŸš€ GPU éƒ¨ç½²æŠ€èƒ½

åœ¨ GPU æœåŠ¡å™¨ä¸Šå¿«é€Ÿéƒ¨ç½² vLLM æ¨¡å‹æœåŠ¡ã€‚

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- ğŸ–¥ï¸ **å¤šæœåŠ¡å™¨æ”¯æŒ** - é…ç½®å¤šä¸ª GPU æœåŠ¡å™¨ï¼Œçµæ´»é€‰æ‹©
- ğŸ” **è‡ªåŠ¨æ£€æŸ¥** - ä¸€é”®æ£€æŸ¥ GPU çŠ¶æ€å’Œç«¯å£å ç”¨
- ğŸ¤– **æ¨¡å‹åº“** - é¢„ç½®æµè¡Œæ¨¡å‹é…ç½®
- âš¡ **å¿«é€Ÿéƒ¨ç½²** - ç®€å•å‘½ä»¤å³å¯å¯åŠ¨æœåŠ¡

---

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### 1. é…ç½®æœåŠ¡å™¨

åˆ›å»º `~/.config/gpu-deploy/servers.json`ï¼š

```json
{
  "servers": {
    "gpu1": {
      "host": "gpu1",
      "user": "lnsoft",
      "gpu_count": 4,
      "model_path": "/data/models/llm"
    },
    "my-gpu": {
      "host": "192.168.1.100",
      "user": "ubuntu",
      "gpu_count": 2,
      "model_path": "/home/ubuntu/models"
    }
  },
  "default_server": "gpu1"
}
```

### 2. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€

```bash
# ä½¿ç”¨é»˜è®¤æœåŠ¡å™¨
gpu-deploy check

# æŒ‡å®šæœåŠ¡å™¨
gpu-deploy check --server gpu1
```

### 3. éƒ¨ç½²æ¨¡å‹

```bash
# éƒ¨ç½²é¢„è®¾æ¨¡å‹
gpu-deploy deploy deepseek-r1-32b

# æŒ‡å®šç«¯å£
gpu-deploy deploy deepseek-r1-32b --port 8112
```

---

## ğŸ›ï¸ å¯ç”¨å‘½ä»¤

### `check` - æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€

æ£€æŸ¥ GPU æ˜¾å­˜å’Œç«¯å£å ç”¨æƒ…å†µã€‚

```bash
gpu-deploy check [--server NAME] [--port PORT]
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
âœ… GPU çŠ¶æ€æ­£å¸¸
- 4 Ã— Tesla T4 (15GB)
- æ˜¾å­˜å ç”¨: 12.6GB/å¡
- æ¸©åº¦: 51-55Â°C

âœ… ç«¯å£ 8111 å¯ç”¨
```

### `deploy` - éƒ¨ç½²æ¨¡å‹

å¯åŠ¨ vLLM æ¨¡å‹æœåŠ¡ã€‚

```bash
gpu-deploy deploy <MODEL_NAME> [--server NAME] [--port PORT]
```

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `deepseek-r1-32b` - DeepSeek-R1-Distill-Qwen-32B-AWQ
- `llama-3-8b` - Llama 3 8B
- `qwen-7b` - Qwen 7B
- `mistral-7b` - Mistral 7B

### `list` - åˆ—å‡ºå¯ç”¨æ¨¡å‹

```bash
gpu-deploy list
```

### `ps` - æŸ¥çœ‹è¿è¡Œä¸­çš„æœåŠ¡

```bash
gpu-deploy ps [--server NAME]
```

### `stop` - åœæ­¢æœåŠ¡

```bash
gpu-deploy stop [--server NAME] [--port PORT]
```

---

## ğŸ”§ æ‰‹åŠ¨ä½¿ç”¨ï¼ˆæ— è„šæœ¬ï¼‰

å¦‚æœä¸æƒ³ç”¨å°è£…è„šæœ¬ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨åŸå§‹å‘½ä»¤ï¼š

### æ£€æŸ¥ GPU

```bash
ssh <user>@<host> nvidia-smi
```

### æ£€æŸ¥ç«¯å£

```bash
ssh <user>@<host> "lsof -i :<port> 2>/dev/null || echo 'ç«¯å£å¯ç”¨'"
```

### éƒ¨ç½²æ¨¡å‹ï¼ˆDeepSeek R1 32Bï¼‰

```bash
ssh <user>@<host> "tmux new-session -d -s vllm '
source /data/miniconda3/etc/profile.d/conda.sh && \
conda activate vllm && \
cd /data/models/llm && \
vllm serve /data/models/llm/deepseek/DeepSeek-R1-Distill-Qwen-32B-AWQ/ \
  --tensor-parallel-size 4 \
  --max-model-len 102400 \
  --dtype half \
  --port 8111 \
  --served-model-name gpt-4o-mini
'"
```

---

## ğŸ“¦ æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹

åœ¨ `~/.config/gpu-deploy/models.json` ä¸­æ·»åŠ ï¼š

```json
{
  "my-model": {
    "name": "My Awesome Model",
    "path": "/path/to/model",
    "tensor_parallel_size": 2,
    "max_model_len": 8192,
    "dtype": "half",
    "port": 8111,
    "served_model_name": "my-model"
  }
}
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **éƒ¨ç½²å‰æ£€æŸ¥** - æ€»æ˜¯å…ˆè¿è¡Œ `check` ç¡®è®¤èµ„æºå¯ç”¨
2. **åå°è¿è¡Œ** - å»ºè®®ä½¿ç”¨ tmux/screen ä¿æŒæœåŠ¡è¿è¡Œ
3. **ç«¯å£ç®¡ç†** - ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒç«¯å£
4. **æ˜¾å­˜ä¼°ç®—** - 7B æ¨¡å‹çº¦éœ€ 8-10GBï¼Œ32B çº¦éœ€ 10-14GB/å¡

---

## ğŸ”— ç›¸å…³é“¾æ¥

- vLLM æ–‡æ¡£: https://docs.vllm.ai
- æ¨¡å‹ä¸‹è½½: https://huggingface.co/models
- é—®é¢˜åé¦ˆ: https://github.com/your-username/gpu-deploy-skill

---

**ç”± OpenClaw ç¤¾åŒºè´¡çŒ® ğŸ¦**
