import os
import json
import copy
import asyncio
from typing import Annotated, TypedDict, Optional

# LangGraph related
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# Model related
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import ToolNode


# --- Tools that need automatic username injection ---
USER_INJECTED_TOOLS = {
    # File management tools
    "list_files", "read_file", "write_file", "append_file", "delete_file",
    # Command execution tools
    "run_command", "run_python_code",
    # Alarm management tools
    "add_alarm", "list_alarms", "delete_alarm",
    # Bark push notification tools
    "set_push_key", "send_push_notification", "get_push_status",
    "set_public_url", "get_public_url", "clear_public_url",
    # Telegram push notification tools
    "set_telegram_chat_id", "send_telegram_message", "get_telegram_status", "remove_telegram_config",
    # OASIS forum tools
    "post_to_oasis", "check_oasis_discussion", "cancel_oasis_discussion",
    "list_oasis_topics",
    "list_oasis_sessions",
    "list_oasis_experts", "add_oasis_expert", "update_oasis_expert", "delete_oasis_expert",
    "set_oasis_workflow", "list_oasis_workflows", "yaml_to_layout",
    # Session management tools
    "list_sessions", "get_current_session",
    # LLM API access tools
    "call_llm_api", "send_internal_message",
    # Group chat tools
    "send_to_group",
}

# Tools that need session_id auto-injected (in addition to username)
SESSION_INJECTED_TOOLS = {
    "add_alarm": "session_id",
    "post_to_oasis": "notify_session",
    "get_current_session": "current_session_id",
    "send_telegram_message": "source_session",
    "send_internal_message": "source_session",
    "send_to_group": "source_session",
}


# --- State definition ---
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    trigger_source: str
    enabled_tools: Optional[list[str]]
    user_id: Optional[str]
    session_id: Optional[str]
    # å¤–éƒ¨è°ƒç”¨æ–¹ä¼ å…¥çš„ tools å®šä¹‰ï¼ˆOpenAI function calling æ ¼å¼ï¼‰
    # å½“ LLM é€‰æ‹©è°ƒç”¨è¿™äº›å·¥å…·æ—¶ï¼Œä¸­æ–­å›¾æ‰§è¡Œå¹¶ä»¥ tool_calls æ ¼å¼è¿”å›ç»™è°ƒç”¨æ–¹
    external_tools: Optional[list[dict]]


class UserAwareToolNode:
    """
    Custom tool node:
    1. Reads thread_id from RunnableConfig, auto-injects as username for file/command tools
    2. Intercepts calls to disabled tools at runtime, returns error ToolMessage
    """
    def __init__(self, tools, get_mcp_tools_fn):
        self.tool_node = ToolNode(tools)
        self._get_mcp_tools = get_mcp_tools_fn

    async def __call__(self, state, config: RunnableConfig):
        # Get user_id directly from state (injected by mainagent) instead of
        # parsing thread_id, because user_id itself may contain the separator.
        user_id = state.get("user_id") or "anonymous"

        last_message = state["messages"][-1]
        if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
            return {"messages": []}

        # Get currently enabled tool set
        enabled_names = state.get("enabled_tools")
        if enabled_names is not None:
            enabled_set = set(enabled_names)
        else:
            enabled_set = None  # None = all allowed

        # Separate blocked and allowed calls
        modified_message = copy.deepcopy(last_message)
        blocked_calls = []
        allowed_calls = []
        for tc in modified_message.tool_calls:
            if enabled_set is not None and tc["name"] not in enabled_set:
                blocked_calls.append(tc)
                print(f">>> [tools] ğŸš« æ‹¦æˆªç¦ç”¨å·¥å…·è°ƒç”¨: {tc['name']}")
            else:
                if tc["name"] in USER_INJECTED_TOOLS:
                    tc["args"]["username"] = user_id
                # Auto-inject session_id for tools that need it (only if not already set by LLM)
                if tc["name"] in SESSION_INJECTED_TOOLS:
                    param_name = SESSION_INJECTED_TOOLS[tc["name"]]
                    if not tc["args"].get(param_name):
                        tc["args"][param_name] = state.get("session_id") or "default"
                allowed_calls.append(tc)
                print(f">>> [tools] âœ… è°ƒç”¨å·¥å…·: {tc['name']}")

        result_messages = []

        # For blocked tools, return error ToolMessages directly
        for tc in blocked_calls:
            result_messages.append(
                ToolMessage(
                    content=f"âŒ å·¥å…· '{tc['name']}' å½“å‰å·²è¢«ç¦ç”¨ã€‚è¿™é€šå¸¸æ˜¯ä¸ºäº†ä¿æŠ¤æ‚¨çš„ç³»ç»Ÿå®‰å…¨æˆ–ä¼˜åŒ–å½“å‰ä¼šè¯èµ„æºã€‚å¦‚æœæ‚¨ç¡®å®éœ€è¦æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨ç®¡ç†é¢æ¿ä¸­å°†å…¶å¼€å¯ã€‚åŒæ—¶ï¼Œæ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„æœ€ç»ˆç›®æ ‡ï¼Œæˆ‘ä¼šå°è¯•ç”¨å…¶ä»–å·²å¯ç”¨çš„å·¥å…·ä¸ºæ‚¨å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆã€‚",
                    tool_call_id=tc["id"],
                )
            )

        # For allowed tools, execute normally via ToolNode
        if allowed_calls:
            modified_message.tool_calls = allowed_calls
            modified_state = {**state, "messages": state["messages"][:-1] + [modified_message]}
            tool_result = await self.tool_node.ainvoke(modified_state, config)
            result_messages.extend(tool_result.get("messages", []))

        return {"messages": result_messages}


class MiniTimeAgent:
    """
    Encapsulates the full LangGraph agent: MCP tool loading, graph building,
    invoke/stream interface, task & tool-state management.
    """

    def __init__(self, src_dir: str, db_path: str):
        """
        Args:
            src_dir:  Path to src/ directory (where mcp_*.py live)
            db_path:  Path to SQLite checkpoint database
        """
        self._src_dir = src_dir
        self._db_path = db_path

        # Populated during startup
        self._mcp_tools: list = []
        self._agent_app = None
        self._mcp_client: Optional[MultiServerMCPClient] = None
        self._memory = None
        self._memory_ctx = None

        # Per-user state
        self._active_tasks: dict[str, asyncio.Task] = {}
        self._task_lock = asyncio.Lock()
        self._user_last_tool_state: dict[str, frozenset[str]] = {}

        # Per-thread lock: é˜²æ­¢ system_trigger å’Œç”¨æˆ·å¯¹è¯å¹¶å‘æ“ä½œåŒä¸€ checkpoint
        self._thread_locks: dict[str, asyncio.Lock] = {}
        self._thread_locks_guard = asyncio.Lock()
        # è®°å½•å½“å‰æŒæœ‰é”çš„æ¥æº: thread_id â†’ "user" | "system"
        self._thread_busy_source: dict[str, str] = {}

        # ç³»ç»Ÿè§¦å‘äº§ç”Ÿçš„æ–°æ¶ˆæ¯è®¡æ•°ï¼ˆthread_id â†’ countï¼‰ï¼Œå‰ç«¯å¯è½®è¯¢
        self._pending_system_messages: dict[str, int] = {}

        # å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½ prompt æ¨¡æ¿
        self._prompts = self._load_prompts()

    # ------------------------------------------------------------------
    # Prompt loader (å¯åŠ¨æ—¶è¯»å–ä¸€æ¬¡)
    # ------------------------------------------------------------------
    @staticmethod
    def _load_prompts() -> dict[str, str]:
        """ä» data/prompts/ åŠ è½½æ‰€æœ‰ prompt æ¨¡æ¿æ–‡ä»¶ï¼ŒæœåŠ¡å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ã€‚"""
        prompts_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "prompts")
        prompt_files = {
            "base_system": "base_system.txt",
            "base_system_subagent": "base_system_subagent.txt",
            "system_trigger": "system_trigger.txt",
            "tool_status": "tool_status.txt",
        }
        loaded = {}
        for key, filename in prompt_files.items():
            filepath = os.path.join(prompts_dir, filename)
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    loaded[key] = f.read().strip()
                print(f"[prompts] âœ… å·²åŠ è½½ {filename}")
            except FileNotFoundError:
                print(f"[prompts] âš ï¸ æœªæ‰¾åˆ° {filepath}ï¼Œå°†ä½¿ç”¨å†…ç½®é»˜è®¤å€¼")
                loaded[key] = ""

        # è®°å½• user_files æ ¹ç›®å½•è·¯å¾„ï¼ˆç”¨æˆ·ç”»åƒå­˜åœ¨å„ç”¨æˆ·ç›®å½•ä¸‹ï¼‰
        loaded["_user_files_dir"] = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "data", "user_files"
        )

        return loaded

    def _get_user_profile(self, user_id: str) -> str:
        """ä» data/user_files/{user_id}/user_profile.txt è¯»å–ç”¨æˆ·ç”»åƒã€‚"""
        user_files_dir = self._prompts.get("_user_files_dir", "")
        fpath = os.path.join(user_files_dir, user_id, "user_profile.txt")
        try:
            with open(fpath, "r", encoding="utf-8") as f:
                return f.read().strip()
        except FileNotFoundError:
            return ""

    def _get_user_skills(self, user_id: str) -> str:
        """
        ä» data/user_files/{user_id}/skills_manifest.json è¯»å–ç”¨æˆ·çš„ skill listï¼Œ
        å¹¶è¿”å›æ ¼å¼åŒ–çš„ skill ä¿¡æ¯å­—ç¬¦ä¸²ã€‚
        å³ä½¿æ²¡æœ‰ skillï¼Œä¹Ÿä¼šè¿”å›ä½ç½®ä¿¡æ¯ã€‚
        """
        user_files_dir = self._prompts.get("_user_files_dir", "")
        manifest_path = os.path.join(user_files_dir, user_id, "skills_manifest.json")
        skills_dir = os.path.join(user_files_dir, user_id, "skills")

        skills_manifest = []
        try:
            with open(manifest_path, "r", encoding="utf-8") as f:
                raw = json.load(f)
                # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šç›´æ¥åˆ—è¡¨ [...] æˆ– {"skills": [...]}
                if isinstance(raw, list):
                    skills_manifest = raw
                elif isinstance(raw, dict):
                    skills_manifest = raw.get("skills", [])
        except (FileNotFoundError, json.JSONDecodeError):
            pass

        # æ ¼å¼åŒ– skill ä¿¡æ¯ï¼ˆå³ä½¿ä¸ºç©ºä¹Ÿè¿”å›ä½ç½®ä¿¡æ¯ï¼‰
        skill_lines = ["\nã€ç”¨æˆ·æŠ€èƒ½åˆ—è¡¨ã€‘"]
        skill_lines.append(f"æŠ€èƒ½æ¸…å•æ–‡ä»¶ä½ç½®: {manifest_path}")
        skill_lines.append(f"æŠ€èƒ½æ–‡ä»¶ç›®å½•ä½ç½®: {skills_dir}")

        if skills_manifest:
            skill_lines.append("å¯ç”¨æŠ€èƒ½ï¼š")
            for skill in skills_manifest:
                if not isinstance(skill, dict):
                    continue
                skill_name = skill.get("name", "æœªå‘½åæŠ€èƒ½")
                skill_desc = skill.get("description", "æ— æè¿°")
                skill_file = skill.get("file", "")
                skill_lines.append(f"  - {skill_name}: {skill_desc}")
                if skill_file:
                    skill_lines.append(f"    æ–‡ä»¶: {os.path.join(skills_dir, skill_file)}")
            skill_lines.append("å¦‚éœ€ä½¿ç”¨æŸä¸ªæŠ€èƒ½ï¼Œè¯·ä½¿ç”¨æ–‡ä»¶ç®¡ç†å·¥å…·è¯»å–å¯¹åº”çš„æŠ€èƒ½æ–‡ä»¶ã€‚")
        else:
            skill_lines.append("å½“å‰æš‚æ— å·²æ³¨å†Œçš„æŠ€èƒ½ã€‚")
            skill_lines.append("å¦‚éœ€æ·»åŠ æŠ€èƒ½ï¼Œè¯·åœ¨æŠ€èƒ½æ¸…å•æ–‡ä»¶ä¸­æ·»åŠ æŠ€èƒ½ä¿¡æ¯ã€‚")

        return "\n".join(skill_lines)

    # ------------------------------------------------------------------
    # Properties
    # ------------------------------------------------------------------
    @property
    def mcp_tools(self) -> list:
        return self._mcp_tools

    @property
    def agent_app(self):
        return self._agent_app

    # ------------------------------------------------------------------
    # Lifecycle
    # ------------------------------------------------------------------
    async def startup(self):
        """Initialize MCP client, load tools, build LangGraph workflow."""
        # 1. Open checkpoint DB
        self._memory_ctx = AsyncSqliteSaver.from_conn_string(self._db_path)
        self._memory = await self._memory_ctx.__aenter__()

        # 2. Start MCP servers
        self._mcp_client = MultiServerMCPClient({
            "scheduler_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_scheduler.py")],
                "transport": "stdio",
            },
            "search_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_search.py")],
                "transport": "stdio",
            },
            "file_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_filemanager.py")],
                "transport": "stdio",
            },
            "commander_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_commander.py")],
                "transport": "stdio",
            },
            "oasis_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_oasis.py")],
                "transport": "stdio",
            },
            "bark_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_bark.py")],
                "transport": "stdio",
            },
            "session_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_session.py")],
                "transport": "stdio",
            },
            "telegram_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_telegram.py")],
                "transport": "stdio",
            },
            "llmapi_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_llmapi.py")],
                "transport": "stdio",
            },
        })

        # 3. Fetch tool definitions (new API: no context manager needed)
        self._mcp_tools = await self._mcp_client.get_tools()

        # 4. Build LangGraph workflow
        # æ”¶é›†æ‰€æœ‰å†…éƒ¨ MCP å·¥å…·åç§°ï¼Œç”¨äºæ¡ä»¶è·¯ç”±
        self._internal_tool_names = frozenset(t.name for t in self._mcp_tools)

        workflow = StateGraph(AgentState)
        workflow.add_node("chatbot", self._call_model)
        workflow.add_node("tools", UserAwareToolNode(self._mcp_tools, lambda: self._mcp_tools))
        workflow.add_edge(START, "chatbot")
        workflow.add_conditional_edges("chatbot", self._should_continue)
        workflow.add_edge("tools", "chatbot")

        self._agent_app = workflow.compile(checkpointer=self._memory)
        print("--- Agent æœåŠ¡å·²å¯åŠ¨ï¼Œå¤–éƒ¨å®šæ—¶/ç”¨æˆ·è¾“å…¥åŒå…¼å®¹å°±ç»ª ---")

    async def shutdown(self):
        """Clean up MCP client and checkpoint DB."""
        if self._memory_ctx:
            try:
                await self._memory_ctx.__aexit__(None, None, None)
            except Exception:
                pass

    # ------------------------------------------------------------------
    # Model factory
    # ------------------------------------------------------------------
    # æ¨¡å‹å -> å‚å•† æ˜ å°„å·²ç§»è‡³ src/llm_factory.pyï¼ˆå…¨å±€å…±äº«ï¼‰

    @staticmethod
    def _get_model() -> BaseChatModel:
        from llm_factory import create_chat_model
        return create_chat_model()

    # ------------------------------------------------------------------
    # Conditional edge: route internal tools vs external tools vs end
    # ------------------------------------------------------------------
    def _should_continue(self, state: AgentState) -> str:
        """
        æ¡ä»¶è·¯ç”±ï¼š
        - æ—  tool_calls â†’ "end" (æ­£å¸¸ç»“æŸ)
        - æ‰€æœ‰ tool_calls éƒ½æ˜¯å†…éƒ¨å·¥å…· â†’ "tools" (ç»§ç»­å†…éƒ¨å¾ªç¯)
        - å­˜åœ¨å¤–éƒ¨å·¥å…·è°ƒç”¨ â†’ "end" (ä¸­æ–­è¿”å› tool_calls ç»™è°ƒç”¨æ–¹)
        """
        last_msg = state["messages"][-1]
        if not hasattr(last_msg, "tool_calls") or not last_msg.tool_calls:
            return END

        for tc in last_msg.tool_calls:
            if tc["name"] not in self._internal_tool_names:
                # å‘ç°å¤–éƒ¨å·¥å…·è°ƒç”¨ï¼Œä¸­æ–­å¾ªç¯è®©è°ƒç”¨æ–¹å¤„ç†
                print(f">>> [route] ğŸ”€ å¤–éƒ¨å·¥å…·è°ƒç”¨æ£€æµ‹: {tc['name']}ï¼Œä¸­æ–­è¿”å›ç»™è°ƒç”¨æ–¹")
                return END
        return "tools"

    # ------------------------------------------------------------------
    # Core graph node
    # ------------------------------------------------------------------
    async def _call_model(self, state: AgentState):
        """LangGraph node: invoke LLM with dynamic tool binding & tool-state notification."""

        # Dynamic tool binding based on enabled_tools + external_tools
        all_tools = self._mcp_tools
        enabled_names = state.get("enabled_tools")
        if enabled_names is not None:
            filtered_tools = [t for t in all_tools if t.name in enabled_names]
        else:
            filtered_tools = all_tools

        # å°†å¤–éƒ¨å·¥å…·å®šä¹‰ï¼ˆOpenAI function formatï¼‰è½¬ä¸º LangChain å¯ç»‘å®šçš„æ ¼å¼
        external_tools_defs = state.get("external_tools") or []
        bind_tools_list: list = list(filtered_tools)
        external_tool_names: set[str] = set()
        for ext_tool in external_tools_defs:
            # æ”¯æŒ OpenAI æ ‡å‡†æ ¼å¼: {"type":"function","function":{...}} æˆ–ç®€åŒ–æ ¼å¼ {"name":...,"parameters":...}
            if ext_tool.get("type") == "function":
                func_def = ext_tool.get("function", {})
            else:
                func_def = ext_tool
            if func_def.get("name"):
                external_tool_names.add(func_def["name"])
                # ä»¥ OpenAI function æ ¼å¼ä¼ å…¥ bind_toolsï¼ˆLangChain æ”¯æŒ dict æ ¼å¼ï¼‰
                bind_tools_list.append({
                    "type": "function",
                    "function": {
                        "name": func_def["name"],
                        "description": func_def.get("description", ""),
                        "parameters": func_def.get("parameters", {"type": "object", "properties": {}}),
                    },
                })

        base_model = self._get_model()
        llm = base_model.bind_tools(bind_tools_list) if bind_tools_list else base_model

        # --- KV-Cache-friendly tool state management ---
        all_names = sorted(t.name for t in all_tools)
        all_tool_list_str = ", ".join(all_names)

        # åˆ¤æ–­æ˜¯å¦ä¸º subagent ä¼šè¯ï¼ˆsession_id ä»¥ "oasis_" å¼€å¤´ï¼‰
        session_id = state.get("session_id", "")
        is_subagent = session_id.startswith("oasis_") if session_id else False

        if is_subagent:
            # Subagent æ¨¡å¼ï¼šç²¾ç®€ promptï¼Œæ— ç”¨æˆ·ç”»åƒ/æŠ€èƒ½ï¼Œåªåˆ—å·¥å…·
            base_prompt = (
                self._prompts["base_system_subagent"] + "\n\n"
                f"ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘\n{all_tool_list_str}\n"
            )
        else:
            base_prompt = (
                self._prompts["base_system"] + "\n\n"
                f"ã€é»˜è®¤å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘\n{all_tool_list_str}\n"
                "ä»¥ä¸Šå·¥å…·é»˜è®¤å…¨éƒ¨å¯ç”¨ã€‚å¦‚æœåç»­æœ‰å·¥å…·çŠ¶æ€å˜æ›´ï¼Œç³»ç»Ÿä¼šå¦è¡Œé€šçŸ¥ã€‚\n"
            )

        # Detect tool state change
        current_enabled = frozenset(enabled_names) if enabled_names is not None else frozenset(all_names)
        user_id = state.get("user_id", "__global__")

        # ä»…ä¸» agent ä¼šè¯æ³¨å…¥ç”¨æˆ·ç”»åƒå’ŒæŠ€èƒ½åˆ—è¡¨
        if not is_subagent:
            # æ³¨å…¥ç”¨æˆ·ä¸“å±ç”»åƒ
            user_profile = self._get_user_profile(user_id)
            if user_profile:
                base_prompt += f"\n{user_profile}\n"

            # æ³¨å…¥ç”¨æˆ·æŠ€èƒ½åˆ—è¡¨ï¼ˆæ€»æ˜¯æ˜¾ç¤ºä½ç½®ä¿¡æ¯ï¼‰
            base_prompt += self._get_user_skills(user_id) + "\n"

        last_state = self._user_last_tool_state.get(user_id)

        tool_status_prompt = ""
        if last_state is not None and current_enabled != last_state:
            all_names_set = set(all_names)
            enabled_set = set(current_enabled)
            disabled_names_set = all_names_set - enabled_set
            tool_status_prompt = self._prompts["tool_status"].format(
                enabled_tools=', '.join(sorted(enabled_set & all_names_set)) if (enabled_set & all_names_set) else 'æ— ',
                disabled_tools=', '.join(sorted(disabled_names_set)) if disabled_names_set else 'æ— ',
            )
        elif last_state is None and enabled_names is not None:
            all_names_set = set(all_names)
            enabled_set = set(current_enabled)
            disabled_names_set = all_names_set - enabled_set
            if disabled_names_set:
                tool_status_prompt = self._prompts["tool_status"].format(
                    enabled_tools=', '.join(sorted(enabled_set & all_names_set)) if (enabled_set & all_names_set) else 'æ— ',
                    disabled_tools=', '.join(sorted(disabled_names_set)),
                )

        # Update cache
        self._user_last_tool_state[user_id] = current_enabled

        history_messages = list(state["messages"])

        # æ¯æ¬¡è¿›å…¥å‰æ¸…ç†ï¼šç§»é™¤æœ«å°¾ä¸å®Œæ•´çš„ tool_callsï¼ˆæœ‰ AIMessage å¸¦ tool_calls ä½†ç¼ºå°‘ ToolMessage å›å¤ï¼‰
        # ä½†ä¿ç•™å¤–éƒ¨å·¥å…·çš„æœªå›å¤ tool_callsï¼ˆå®ƒä»¬æ­£ç­‰å¾…è°ƒç”¨æ–¹å›ä¼ ç»“æœï¼‰
        history_messages = self._sanitize_messages(history_messages, external_tool_names)

        # æ¸…ç†å†å²æ¶ˆæ¯ä¸­çš„å¤šæ¨¡æ€å†…å®¹ï¼ˆfile/image/audio partsï¼‰ï¼Œåªä¿ç•™æ–‡æœ¬
        # é¿å…æ—§çš„äºŒè¿›åˆ¶é™„ä»¶åœ¨åç»­è½®æ¬¡åå¤å‘é€ç»™ LLM å¯¼è‡´ä¸Šæ¸¸ API æŠ¥é”™
        # æ³¨æ„ï¼šä¿ç•™æœ€åä¸€æ¡ HumanMessage çš„å¤šæ¨¡æ€å†…å®¹ï¼ˆå½“å‰è½®ç”¨æˆ·è¾“å…¥ï¼‰
        if len(history_messages) > 1:
            history_messages = self._strip_multimodal_parts(history_messages[:-1]) + [history_messages[-1]]

        # å¦‚æœæ˜¯ç³»ç»Ÿè§¦å‘ï¼Œä¸”æœ€åä¸€æ¡ä¸æ˜¯ ToolMessageï¼ˆéå·¥å…·å›è°ƒè½®ï¼‰ï¼Œç»™å®ƒåŠ ä¸Šç³»ç»Ÿè§¦å‘è¯´æ˜
        is_system = state.get("trigger_source") == "system"
        if is_system and history_messages and isinstance(history_messages[-1], HumanMessage):
            original_text = history_messages[-1].content
            system_trigger_prompt = self._prompts["system_trigger"].format(
                original_text=original_text
            )
            history_messages = history_messages[:-1] + [HumanMessage(content=system_trigger_prompt)]

        # æ­£å¸¸å¯¹è¯æµç¨‹ï¼ˆç”¨æˆ·å’Œç³»ç»Ÿè§¦å‘å…±ç”¨ï¼‰
        if tool_status_prompt and len(history_messages) >= 1:
            last_msg = history_messages[-1]
            # å¦‚æœæœ€åä¸€æ¡æ˜¯å¤šæ¨¡æ€ contentï¼ˆlistï¼‰ï¼Œå°†é€šçŸ¥æ’å…¥ä¸ºç¬¬ä¸€ä¸ª text part
            if isinstance(last_msg.content, list):
                notification = {"type": "text", "text": f"[ç³»ç»Ÿé€šçŸ¥] {tool_status_prompt}\n\n---\n"}
                augmented_content = [notification] + list(last_msg.content)
                augmented_msg = HumanMessage(content=augmented_content)
            else:
                augmented_content = f"[ç³»ç»Ÿé€šçŸ¥] {tool_status_prompt}\n\n---\n{last_msg.content}"
                augmented_msg = HumanMessage(content=augmented_content)
            input_messages = (
                [SystemMessage(content=base_prompt)]
                + history_messages[:-1]
                + [augmented_msg]
            )
        else:
            input_messages = [SystemMessage(content=base_prompt)] + history_messages

        # # === DEBUG: dump full raw input to file for diagnosis ===
        # try:
        #     import json, datetime, os as _os
        #     _dump_dir = _os.path.join(_os.path.dirname(_os.path.dirname(_os.path.abspath(__file__))), "test")
        #     _dump_path = _os.path.join(_dump_dir, "llm_input_dump.txt")
        #     with open(_dump_path, "w", encoding="utf-8") as _f:
        #         _f.write(f"=== LLM INPUT DUMP @ {datetime.datetime.now().isoformat()} ===\n")
        #         _f.write(f"Thread: {state.get('user_id','?')}#{state.get('session_id','?')}\n")
        #         _f.write(f"Total messages: {len(input_messages)}\n")
        #         _f.write(f"LLM model: {llm.model_name if hasattr(llm, 'model_name') else '?'}\n")
        #         _f.write(f"LLM base_url: {llm.openai_api_base if hasattr(llm, 'openai_api_base') else '?'}\n\n")
        #         # Dump each message as full dict via langchain serialization
        #         from langchain_openai.chat_models.base import _convert_message_to_dict
        #         for _i, _m in enumerate(input_messages):
        #             _f.write(f"--- [{_i}] {type(_m).__name__} ---\n")
        #             try:
        #                 _d = _convert_message_to_dict(_m)
        #                 _f.write(json.dumps(_d, ensure_ascii=False, indent=2))
        #             except Exception as _e:
        #                 _f.write(f"(serialization error: {_e})\n")
        #                 _f.write(f"raw __dict__: {_m.__dict__}")
        #             _f.write("\n\n")
        # except Exception:
        #     pass
        # # === END DEBUG ===

        response = await llm.ainvoke(input_messages)

        # --- æ£€æµ‹ tool_calls arguments æ˜¯å¦ä¸ºåˆæ³• JSONï¼ˆæˆªæ–­/è¶…é•¿ä¼šå¯¼è‡´ä¸å®Œæ•´ï¼‰---
        import json as _json
        for _tc_list_name in ("tool_calls", "invalid_tool_calls"):
            for _tc in getattr(response, _tc_list_name, None) or []:
                _args = _tc.get("args") if _tc_list_name == "tool_calls" else _tc.get("args", "")
                # å…œåº•ï¼šargs ç¼ºå¤±(None) æˆ–ç©ºå­—ç¬¦ä¸² â†’ è§†ä¸ºç©ºå‚æ•° {}ï¼Œä¸æŠ¥é”™
                if _args is None or _args == "" or _args == {}:
                    if _tc_list_name == "tool_calls":
                        _tc["args"] = {}
                    continue
                # tool_calls çš„ args å·²è¢« LangChain è§£æä¸º dictï¼›å¦‚æœä»æ˜¯ str è¯´æ˜è§£æå¤±è´¥
                # invalid_tool_calls çš„ args æ˜¯åŸå§‹ str
                if isinstance(_args, str):
                    try:
                        _json.loads(_args)
                    except (ValueError, TypeError):
                        import logging
                        logging.getLogger("agent.call_model").warning(
                            "LLM è¿”å›çš„ tool_call arguments ä¸æ˜¯åˆæ³• JSON (å¯èƒ½è¢«æˆªæ–­), "
                            "name=%s, id=%s, args_len=%d, å‰¥ç¦» tool_calls æ”¹ä¸ºçº¯æ–‡æœ¬å›å¤",
                            _tc.get("name", "?"), _tc.get("id", "?"), len(_args) if _args else 0,
                        )
                        # å°†æ— æ•ˆçš„ tool_call æ›¿æ¢ä¸ºé”™è¯¯ ToolMessageï¼Œä¿æŒæ¶ˆæ¯åºåˆ—åˆæ³•
                        _tc_id = _tc.get("id", "unknown")
                        _tc_name = _tc.get("name", "unknown")
                        from langchain_core.messages import ToolMessage
                        error_tool_msg = ToolMessage(
                            content=f"æ— æ•ˆtoolæ ¼å¼: {_tc_name} çš„å‚æ•°è¢«æˆªæ–­ï¼Œä¸æ˜¯åˆæ³•JSON",
                            tool_call_id=_tc_id,
                        )
                        # ä¿ç•™åŸå§‹ AIMessageï¼ˆå¸¦ tool_callsï¼‰ï¼Œåè·Ÿé”™è¯¯ ToolMessage
                        return {"messages": [response, error_tool_msg]}

        return {"messages": [response]}

    # ------------------------------------------------------------------
    # Public interface: tools info
    # ------------------------------------------------------------------
    @staticmethod
    def _sanitize_messages(messages: list, external_tool_names: set[str] | None = None) -> list:
        """
        æ¸…ç†æ¶ˆæ¯åˆ—è¡¨ï¼Œç¡®ä¿æ¯æ¡å¸¦ tool_calls çš„ AI æ¶ˆæ¯åé¢éƒ½æœ‰å¯¹åº”çš„ ToolMessageã€‚

        ä¸¤è½®æ‰«æï¼š
        1. æœ«å°¾æˆªæ–­ï¼šä»åå¾€å‰ç§»é™¤æ‚¬ç©ºçš„ tool_calls AIMessageï¼ˆä¿ç•™å¤–éƒ¨å·¥å…·ç­‰å¾…å›ä¼ ï¼‰
        2. å…¨åºåˆ—æ‰«æï¼šä¸­é—´çš„æ‚¬ç©º tool_calls AIMessage â†’ å»æ‰ tool_calls åªä¿ç•™ content

        åŒæ—¶æ£€æŸ¥ invalid_tool_callsï¼ˆå¦‚æˆªæ–­çš„ argumentsï¼‰ï¼Œå› ä¸ºåºåˆ—åŒ–æ—¶ä¹Ÿä¼šå˜æˆ tool_calls å‘ç»™ APIã€‚
        """
        import logging
        _log = logging.getLogger("agent.sanitize")

        if not external_tool_names:
            external_tool_names = set()

        def _get_all_tc(msg):
            """è·å– AIMessage ä¸Šæ‰€æœ‰ tool_calls + invalid_tool_calls"""
            tc_list = list(getattr(msg, "tool_calls", None) or [])
            for itc in (getattr(msg, "invalid_tool_calls", None) or []):
                tc_list.append({"id": itc.get("id", ""), "name": itc.get("name", ""), **itc})
            return tc_list

        # æ”¶é›†æ‰€æœ‰å·²å­˜åœ¨çš„ tool_call_id å›å¤
        answered_ids = set()
        for msg in messages:
            if isinstance(msg, ToolMessage) and hasattr(msg, "tool_call_id"):
                answered_ids.add(msg.tool_call_id)

        # --- ç¬¬ä¸€è½®ï¼šä»æœ«å°¾æˆªæ–­æ‚¬ç©ºçš„ tool_calls ---
        clean = list(messages)
        while clean:
            last = clean[-1]
            if not isinstance(last, AIMessage):
                break
            all_tc = _get_all_tc(last)
            if not all_tc:
                break
            pending_ids = {tc["id"] for tc in all_tc if tc.get("id")}
            if pending_ids.issubset(answered_ids):
                break
            # æ£€æŸ¥æœªå›å¤çš„æ˜¯å¦å…¨å±äºå¤–éƒ¨å·¥å…·
            unanswered = [tc for tc in all_tc if tc.get("id") not in answered_ids]
            if external_tool_names and all(tc["name"] in external_tool_names for tc in unanswered):
                break
            _log.warning("sanitize: æˆªæ–­æœ«å°¾æ‚¬ç©º AIMessage, tool_calls=%s",
                         [tc.get("name") for tc in all_tc])
            clean.pop()

        # --- ç¬¬äºŒè½®ï¼šå…¨åºåˆ—æ‰«æï¼Œä¿®å¤ä¸­é—´çš„æ‚¬ç©º tool_calls ---
        result = []
        for msg in clean:
            if isinstance(msg, AIMessage):
                all_tc = _get_all_tc(msg)
                if all_tc:
                    pending_ids = {tc["id"] for tc in all_tc if tc.get("id")}
                    if not pending_ids.issubset(answered_ids):
                        # ä¸­é—´å‡ºç°æ‚¬ç©º tool_calls â†’ å»æ‰ tool_callsï¼Œåªä¿ç•™ content
                        _log.warning(
                            "sanitize: ä¸­é—´æ‚¬ç©º AIMessage, å‰¥ç¦» tool_calls=%s, content=%s",
                            [tc.get("name") for tc in all_tc],
                            str(msg.content)[:100],
                        )
                        result.append(AIMessage(content=msg.content or "ï¼ˆå·¥å…·è°ƒç”¨å¼‚å¸¸ï¼Œå·²æ¸…ç†ï¼‰"))
                        continue
            result.append(msg)

        return result

    @staticmethod
    def _strip_multimodal_parts(messages: list) -> list:
        """
        å°†æ‰€æœ‰ HumanMessage ä¸­çš„å¤šæ¨¡æ€ contentï¼ˆlist æ ¼å¼ï¼‰è½¬ä¸ºçº¯æ–‡æœ¬ã€‚
        - type:"text" çš„ part ä¿ç•™æ–‡æœ¬
        - type:"file" ä¸­çš„åª’ä½“æ–‡ä»¶ï¼ˆè§†é¢‘/éŸ³é¢‘ï¼‰ä¿ç•™åŸå§‹ file part
        - type:"file" ä¸­çš„å…¶ä»–æ–‡ä»¶æ›¿æ¢ä¸º "[ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶: {filename}]"
        - type:"image_url" æ›¿æ¢ä¸º "[ç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡]"
        - type:"input_audio" æ›¿æ¢ä¸º "[ç”¨æˆ·å‘é€äº†è¯­éŸ³]"
        - å…¶ä»–æœªçŸ¥ type ä¸¢å¼ƒ
        """
        _MEDIA_EXTS = {".avi", ".mp4", ".mkv", ".mov", ".webm", ".mp3", ".wav", ".flac", ".ogg", ".aac"}

        result = []
        for msg in messages:
            if isinstance(msg, HumanMessage) and isinstance(msg.content, list):
                new_parts = []  # å¯èƒ½æ··åˆ str å’Œ dictï¼ˆä¿ç•™çš„ file partï¼‰
                for part in msg.content:
                    if not isinstance(part, dict):
                        new_parts.append(str(part))
                        continue
                    ptype = part.get("type", "")
                    if ptype == "text":
                        new_parts.append(part.get("text", ""))
                    elif ptype == "file":
                        fname = part.get("file", {}).get("filename", "é™„ä»¶")
                        ext = os.path.splitext(fname)[1].lower() if fname else ""
                        if ext in _MEDIA_EXTS:
                            # åª’ä½“æ–‡ä»¶ï¼šä¿ç•™åŸå§‹ file part
                            new_parts.append(part)
                        else:
                            new_parts.append(f"[ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶: {fname}]")
                    elif ptype == "image_url":
                        new_parts.append("[ç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡]")
                    elif ptype == "input_audio":
                        new_parts.append("[ç”¨æˆ·å‘é€äº†è¯­éŸ³]")

                # å¦‚æœåªå‰©çº¯æ–‡æœ¬ï¼Œåˆå¹¶ä¸º strï¼›å¦åˆ™ä¿æŒ list æ ¼å¼
                has_dict = any(isinstance(p, dict) for p in new_parts)
                if has_dict:
                    # ä¿æŒ content list æ ¼å¼ï¼ŒæŠŠçº¯æ–‡æœ¬ wrap æˆ text part
                    content_list = []
                    for p in new_parts:
                        if isinstance(p, dict):
                            content_list.append(p)
                        elif p:
                            content_list.append({"type": "text", "text": p})
                    result.append(HumanMessage(content=content_list or [{"type": "text", "text": "(ç©ºæ¶ˆæ¯)"}]))
                else:
                    combined = "\n".join(p for p in new_parts if isinstance(p, str) and p)
                    result.append(HumanMessage(content=combined or "(ç©ºæ¶ˆæ¯)"))
            else:
                result.append(msg)
        return result

    def get_tools_info(self) -> list[dict]:
        """Return serializable tool metadata list."""
        return [{"name": t.name, "description": t.description or ""} for t in self._mcp_tools]

    # ------------------------------------------------------------------
    # Public interface: task management
    # ------------------------------------------------------------------
    async def cancel_task(self, user_id: str):
        """Cancel the active streaming task for a user."""
        async with self._task_lock:
            task = self._active_tasks.get(user_id)
            if task and not task.done():
                task.cancel()
                try:
                    await asyncio.wait_for(asyncio.shield(task), timeout=5.0)
                except (asyncio.CancelledError, asyncio.TimeoutError, Exception):
                    pass
            self._active_tasks.pop(user_id, None)

    def register_task(self, user_id: str, task: asyncio.Task):
        """Register an active streaming task for a user."""
        self._active_tasks[user_id] = task

    def unregister_task(self, user_id: str):
        """Remove a finished task from the registry."""
        self._active_tasks.pop(user_id, None)

    # ------------------------------------------------------------------
    # Thread lock: é˜²æ­¢åŒä¸€ thread çš„å¹¶å‘ checkpoint æ“ä½œ
    # ------------------------------------------------------------------
    async def get_thread_lock(self, thread_id: str) -> asyncio.Lock:
        """è·å–æŒ‡å®š thread çš„é”ï¼ˆæ‡’åˆ›å»ºï¼‰ã€‚"""
        async with self._thread_locks_guard:
            if thread_id not in self._thread_locks:
                self._thread_locks[thread_id] = asyncio.Lock()
            return self._thread_locks[thread_id]

    def add_pending_system_message(self, thread_id: str):
        """æ ‡è®°è¯¥ thread æœ‰æ–°çš„ç³»ç»Ÿè§¦å‘æ¶ˆæ¯ã€‚"""
        self._pending_system_messages[thread_id] = \
            self._pending_system_messages.get(thread_id, 0) + 1

    def consume_pending_system_messages(self, thread_id: str) -> int:
        """æ¶ˆè´¹å¹¶è¿”å›å¾…å¤„ç†çš„ç³»ç»Ÿæ¶ˆæ¯è®¡æ•°ï¼Œå½’é›¶ã€‚"""
        count = self._pending_system_messages.pop(thread_id, 0)
        return count

    def has_pending_system_messages(self, thread_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœªè¯»çš„ç³»ç»Ÿè§¦å‘æ¶ˆæ¯ã€‚"""
        return self._pending_system_messages.get(thread_id, 0) > 0

    def is_thread_busy(self, thread_id: str) -> bool:
        """æ£€æŸ¥è¯¥ thread çš„é”æ˜¯å¦è¢«å ç”¨ï¼ˆæœ‰æ“ä½œè¿›è¡Œä¸­ï¼‰ã€‚"""
        lock = self._thread_locks.get(thread_id)
        return lock is not None and lock.locked()

    def set_thread_busy_source(self, thread_id: str, source: str):
        """è®¾ç½®å½“å‰æŒæœ‰é”çš„æ¥æºï¼ˆ"user" æˆ– "system"ï¼‰ã€‚"""
        self._thread_busy_source[thread_id] = source

    def clear_thread_busy_source(self, thread_id: str):
        """æ¸…é™¤é”æ¥æºè®°å½•ã€‚"""
        self._thread_busy_source.pop(thread_id, None)

    def get_thread_busy_source(self, thread_id: str) -> str:
        """è¿”å›é”æ¥æº: "user"ã€"system"ã€æˆ– "" (æœªå ç”¨)ã€‚"""
        if not self.is_thread_busy(thread_id):
            return ""
        return self._thread_busy_source.get(thread_id, "unknown")

    def get_all_thread_status(self, prefix: str) -> dict[str, dict]:
        """è¿”å›æŒ‡å®šå‰ç¼€ä¸‹æ‰€æœ‰å·²çŸ¥ thread çš„çŠ¶æ€ã€‚"""
        result = {}
        for thread_id, lock in self._thread_locks.items():
            if not thread_id.startswith(prefix):
                continue
            busy = lock.locked()
            result[thread_id] = {
                "busy": busy,
                "source": self._thread_busy_source.get(thread_id, "") if busy else "",
                "pending_system": self._pending_system_messages.get(thread_id, 0),
            }
        return result
