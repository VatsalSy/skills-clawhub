#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HF Daily Papers Generator ‚Äî OFR ÂÆöÂà∂Áâà
ÁîüÊàê Markdown / Telegram Ê†ºÂºèÁöÑËÆ∫ÊñáÊé®ËçêÊä•Âëä
6 ‰∏™ OFR Áõ∏ÂÖ≥È¢ÜÂüüÂÖ≥ÈîÆËØçÁ≠õÈÄâ
"""

import os
import re
import urllib.request
import json
import sys
from datetime import datetime

# Proxy: env var > default Clash
_proxy = os.environ.get('HF_DAILY_PAPERS_PROXY', 'http://127.0.0.1:7890')
if _proxy:
    os.environ['HTTP_PROXY'] = _proxy
    os.environ['HTTPS_PROXY'] = _proxy

# ‚îÄ‚îÄ OFR 6 Â§ßÈ¢ÜÂüüÂÖ≥ÈîÆËØç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CATEGORIES = {
    'üé¨ Restoration & Enhancement': [
        'restoration', 'restoring', 'denoising', 'denoise',
        'deblur', 'deblurring', 'super-resolution', 'super resolution',
        'inpainting', 'scratch', 'flicker', 'deflicker',
        'old film', 'film grain', 'degradation', 'artifact removal',
        'image enhancement', 'video enhancement', 'low-light',
        'color restoration', 'colorization', 'film restoration',
    ],
    'üéûÔ∏è Video & Temporal': [
        'video', 'temporal', 'optical flow', 'frame interpolation',
        'video generation', 'video editing', 'video inpainting',
        'temporal consistency', 'recurrent', 'propagation',
        'multi-frame', 'sequence', 'motion',
    ],
    '‚ö° Efficient Architecture': [
        'efficient', 'lightweight', 'pruning', 'quantization',
        'distillation', 'mobile', 'fast', 'real-time',
        'computation', 'flops', 'memory efficient', 'sparse',
        'acceleration', 'compression',
    ],
    'üî≠ Vision Backbone & Attention': [
        'transformer', 'attention', 'self-attention', 'cross-attention',
        'vision transformer', 'vit', 'swin', 'deformable',
        'convolution', 'cnn', 'backbone', 'encoder', 'decoder',
        'mamba', 'state space', 'ssm', 'linear attention',
    ],
    'üåä Frequency & Wavelet': [
        'wavelet', 'frequency', 'fourier', 'fft', 'dct',
        'spectral', 'haar', 'dwt', 'idwt',
        'frequency domain', 'high-frequency', 'low-frequency',
        'band', 'subband',
    ],
    'üé® Diffusion & Generative Prior': [
        'diffusion', 'generative', 'gan', 'generation',
        'text-to-image', 'text-to-video', 'stable diffusion',
        'score-based', 'flow matching', 'rectified flow',
        'autoregressive', 'prior', 'latent',
    ],
}


def fetch_hf_papers():
    """Ëé∑Âèñ HF Daily Papers"""
    print("üì• Ëé∑Âèñ HF Daily Papers...")
    try:
        req = urllib.request.Request(
            "https://huggingface.co/papers",
            headers={'User-Agent': 'Mozilla/5.0'}
        )
        with urllib.request.urlopen(req, timeout=30) as r:
            html = r.read().decode()
    except Exception as e:
        print(f"  ‚ö†Ô∏è HF È°µÈù¢Ëé∑ÂèñÂ§±Ë¥•: {e}")
        return []

    paper_ids = list(set(re.findall(r'href="/papers/([0-9]+\.[0-9]+)"', html)))[:30]
    print(f"  ÊâæÂà∞ {len(paper_ids)} ÁØá HF ËÆ∫Êñá ID")

    papers = []
    for i, pid in enumerate(paper_ids):
        try:
            url = f"https://huggingface.co/api/papers/{pid}"
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, timeout=10) as r:
                data = json.loads(r.read().decode())
                papers.append({
                    'pid': pid,
                    'title': data.get('title', 'N/A')[:120],
                    'upvotes': data.get('upvotes', 0),
                    'summary': data.get('summary', '')[:600].lower(),
                    'source': 'hf',
                    'url': f'https://huggingface.co/papers/{pid}',
                })
        except Exception:
            continue
        if (i + 1) % 10 == 0:
            print(f"  Â∑≤Â§ÑÁêÜ {i+1}/{len(paper_ids)}...")

    papers.sort(key=lambda x: x['upvotes'], reverse=True)
    return papers


def fetch_arxiv_papers(max_results=50):
    """Ëé∑Âèñ arXiv CS.CV ÊúÄÊñ∞ËÆ∫ÊñáÔºàË°•ÂÖÖ HF ÁÉ≠Ê¶úÁõ≤Âå∫Ôºâ"""
    import xml.etree.ElementTree as ET

    print("üì• Ëé∑Âèñ arXiv CS.CV ÊúÄÊñ∞ËÆ∫Êñá...")
    url = (
        f'http://export.arxiv.org/api/query?'
        f'search_query=cat:cs.CV&sortBy=submittedDate'
        f'&sortOrder=descending&max_results={max_results}'
    )
    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, timeout=30) as r:
            xml_data = r.read().decode()
    except Exception as e:
        print(f"  ‚ö†Ô∏è arXiv Ëé∑ÂèñÂ§±Ë¥•: {e}")
        return []

    ns = {
        'atom': 'http://www.w3.org/2005/Atom',
        'arxiv': 'http://arxiv.org/schemas/atom',
    }
    root = ET.fromstring(xml_data)
    entries = root.findall('atom:entry', ns)
    print(f"  ÊâæÂà∞ {len(entries)} ÁØá arXiv ËÆ∫Êñá")

    papers = []
    for entry in entries:
        try:
            arxiv_id_raw = entry.find('atom:id', ns).text  # http://arxiv.org/abs/2602.22033v1
            pid = arxiv_id_raw.split('/')[-1].split('v')[0]  # 2602.22033
            title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')[:120]
            summary = entry.find('atom:summary', ns).text.strip().replace('\n', ' ')[:600].lower()
            papers.append({
                'pid': pid,
                'title': title,
                'upvotes': 0,  # arXiv Ê≤°Êúâ upvote
                'summary': summary,
                'source': 'arxiv',
                'url': f'https://arxiv.org/abs/{pid}',
            })
        except Exception:
            continue

    return papers


def fetch_papers():
    """ÂêàÂπ∂ HF + arXiv Ê∫êÔºåÂéªÈáç"""
    hf = fetch_hf_papers()
    arxiv = fetch_arxiv_papers(max_results=50)

    # ÂéªÈáçÔºö‰ª• pid ‰∏∫ÂáÜÔºåHF ‰ºòÂÖàÔºàÊúâ upvotesÔºâ
    seen = {p['pid'] for p in hf}
    merged = list(hf)
    for p in arxiv:
        if p['pid'] not in seen:
            seen.add(p['pid'])
            merged.append(p)

    print(f"\n  ÂêàÂπ∂: HF {len(hf)} + arXiv {len(arxiv)} ‚Üí ÂéªÈáçÂêé {len(merged)}")
    # HF ËÆ∫ÊñáÊåâ upvotes ÊéíÂú®ÂâçÈù¢ÔºåarXiv Ë°•ÂÖÖÂú®ÂêéÈù¢
    merged.sort(key=lambda x: x['upvotes'], reverse=True)
    return merged


def classify_papers(papers):
    """Êåâ 6 ‰∏™ OFR È¢ÜÂüüÂàÜÁ±ªÔºà‰∏ÄÁØáÂèØÂ±ûÂ§ö‰∏™È¢ÜÂüüÔºâ"""
    result = {cat: [] for cat in CATEGORIES}
    uncategorized = []

    for p in papers:
        text = (p['title'] + ' ' + p['summary']).lower()
        matched = False
        for cat, keywords in CATEGORIES.items():
            if any(kw in text for kw in keywords):
                result[cat].append(p)
                matched = True
        if not matched:
            uncategorized.append(p)

    return result, uncategorized


def generate_markdown(classified, uncategorized, output_dir, timestamp):
    """ÁîüÊàê Markdown Êñá‰ª∂"""
    date_str = datetime.now().strftime('%Y-%m-%d')
    md_file = os.path.join(output_dir, f"{date_str}.md")

    lines = [f"# OFR ÁßëÁ†îÊó•Êä• ‚Äî HF Daily Papers\n"]
    lines.append(f"**Êó•Êúü**: {date_str}  **ÁîüÊàêÊó∂Èó¥**: {timestamp}\n")
    lines.append('---\n')

    total_relevant = sum(len(v) for v in classified.values())
    lines.append(f'> OFR Áõ∏ÂÖ≥ËÆ∫Êñá: **{total_relevant}** ÁØáÔºàÂéªÈáçÂâçÔºâ\n\n')

    for cat, papers in classified.items():
        if not papers:
            continue
        lines.append(f'## {cat} ({len(papers)} ÁØá)\n')
        for p in papers[:10]:
            src_tag = 'ü§ó' if p.get('source') == 'hf' else 'üìÑ'
            up_str = f' ‚¨ÜÔ∏è{p["upvotes"]}' if p['upvotes'] > 0 else ''
            lines.append(
                f'- {src_tag} [{p["title"]}]({p["url"]})'
                f'{up_str}\n'
            )
        lines.append('\n')

    if uncategorized:
        lines.append(f'## üìã ÂÖ∂‰ªñÁÉ≠Èó® ({len(uncategorized)} ÁØá)\n')
        for p in uncategorized[:5]:
            src_tag = 'ü§ó' if p.get('source') == 'hf' else 'üìÑ'
            up_str = f' ‚¨ÜÔ∏è{p["upvotes"]}' if p['upvotes'] > 0 else ''
            lines.append(
                f'- {src_tag} [{p["title"]}]({p["url"]})'
                f'{up_str}\n'
            )

    lines.append('\n---\n*Generated by OpenClaw HF Daily Papers Skill (OFR Edition)*\n')

    with open(md_file, 'w', encoding='utf-8') as f:
        f.writelines(lines)
    print(f"‚úÖ Markdown: {md_file}")
    return md_file


def generate_telegram(classified, uncategorized, output_dir, timestamp):
    """ÁîüÊàê Telegram Ê†ºÂºèÊñáÊú¨"""
    date_str = datetime.now().strftime('%Y-%m-%d')
    tg_file = os.path.join(output_dir, f"{date_str}.telegram.txt")

    lines = [f"üì∞ OFR ÁßëÁ†îÊó•Êä• ‚Äî {date_str}\n\n"]

    total_relevant = sum(len(v) for v in classified.values())
    if total_relevant == 0:
        lines.append("‰ªäÊó• HF ÁÉ≠Ê¶úÊöÇÊó† OFR Áõ∏ÂÖ≥ËÆ∫Êñá„ÄÇ\n")
    else:
        for cat, papers in classified.items():
            if not papers:
                continue
            lines.append(f"{cat} ({len(papers)})\n")
            for p in papers[:5]:
                src_tag = 'ü§ó' if p.get('source') == 'hf' else 'üìÑ'
                up_str = f' ‚¨ÜÔ∏è{p["upvotes"]}' if p['upvotes'] > 0 else ''
                lines.append(
                    f"  {src_tag} {p['title']}\n"
                    f"    {up_str} {p['url']}\n"
                )
            lines.append('\n')

    if uncategorized:
        top3 = uncategorized[:3]
        lines.append(f"üìã ÂÖ∂‰ªñÁÉ≠Èó®\n")
        for p in top3:
            src_tag = 'ü§ó' if p.get('source') == 'hf' else 'üìÑ'
            up_str = f' ‚¨ÜÔ∏è{p["upvotes"]}' if p['upvotes'] > 0 else ''
            lines.append(
                f"  {src_tag} {p['title']}\n"
                f"    {up_str} {p['url']}\n"
            )

    with open(tg_file, 'w', encoding='utf-8') as f:
        f.writelines(lines)
    print(f"‚úÖ Telegram: {tg_file}")
    return tg_file


def main():
    do_telegram = '--telegram' in sys.argv
    do_pdf = '--pdf' in sys.argv

    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'recommendations')
    os.makedirs(output_dir, exist_ok=True)

    papers = fetch_papers()
    classified, uncategorized = classify_papers(papers)

    total_relevant = sum(len(v) for v in classified.values())
    print(f"\nüìä Total: {len(papers)} | OFR-related: {total_relevant} | Other: {len(uncategorized)}")
    for cat, ps in classified.items():
        if ps:
            print(f"  {cat}: {len(ps)}")

    # Always generate markdown
    generate_markdown(classified, uncategorized, output_dir, timestamp)

    # Telegram format
    if do_telegram:
        generate_telegram(classified, uncategorized, output_dir, timestamp)

    # PDF (optional)
    if do_pdf:
        try:
            from fpdf import FPDF
            # PDF generation omitted for brevity ‚Äî use markdown instead
            print("‚ö†Ô∏è PDF generation not yet implemented in OFR edition. Use markdown.")
        except ImportError:
            print("‚ö†Ô∏è fpdf not installed. Run: pip3 install fpdf")


if __name__ == '__main__':
    main()
