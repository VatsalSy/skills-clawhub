#!/usr/bin/env python3
"""
í•œêµ­ ì •ë¶€ì§€ì›ì‚¬ì—… í†µí•© ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
Skills: korean-gov-programs v1.0.0

ìˆ˜ì§‘ ì†ŒìŠ¤:
  1. ê¸°ì—…ë§ˆë‹¹(BizInfo) - ì†Œìƒê³µì¸ ì§€ì›ì‚¬ì—…  [âœ… ë™ì‘]
  2. ê¸°ì—…ë§ˆë‹¹(BizInfo) - ê¸°ìˆ ì°½ì—…/R&D í•„í„°  [âœ… ë™ì‘]
  3. NIA í•œêµ­ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›             [âœ… ë™ì‘]
  4. ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨(SEMAS)            [âš ï¸ JS í•„ìš”, ìŠ¤í‚µ]
  5. ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€(MSS)                    [âš ï¸ JS í•„ìš”, ìŠ¤í‚µ]
  6. K-Startup                              [âš ï¸ JS í•„ìš”, ìŠ¤í‚µ]
  7. Innopolis ì—°êµ¬ê°œë°œíŠ¹êµ¬ì§„í¥ì¬ë‹¨          [âš ï¸ JS í•„ìš”, ìŠ¤í‚µ]

ì¶œë ¥ (APPEND ì „ìš©):
  {output}/soho_programs.jsonl   - ì†Œìƒê³µì¸ ì§€ì›ì‚¬ì—…
  {output}/gov_programs.jsonl    - ì •ë¶€ R&D / ê¸°ìˆ ì°½ì—…
  {output}/.checkpoint.json      - ì²´í¬í¬ì¸íŠ¸

ì‚¬ìš©ë²•:
  python3 collect.py --output ./data
  python3 collect.py --output ./data --max-pages 5
"""

import sys
import os
import json
import time
import re
import argparse
import urllib.request
import urllib.parse
from datetime import datetime

# â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SLEEP_SEC = 0.8

DEFAULT_UA = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/122.0.0.0 Safari/537.36"
)

HEADERS = {
    "User-Agent": os.environ.get("GOV_SCRAPER_UA", DEFAULT_UA),
    "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}


# â”€â”€ ê³µí†µ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def log(msg: str):
    ts = datetime.now().strftime("%H:%M:%S")
    print(f"[{ts}] {msg}", flush=True)


def load_checkpoint(checkpoint_file: str) -> dict:
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_checkpoint(checkpoint_file: str, data: dict):
    with open(checkpoint_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_existing_titles(output_file: str) -> set:
    titles = set()
    if os.path.exists(output_file):
        with open(output_file, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        rec = json.loads(line)
                        titles.add(rec.get("title", ""))
                    except Exception:
                        pass
    return titles


def append_record(output_file: str, rec: dict):
    """APPEND ì „ìš© â€” ê¸°ì¡´ íŒŒì¼ ì ˆëŒ€ ë®ì–´ì“°ì§€ ì•ŠìŒ"""
    with open(output_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")


def fetch_url(url: str, timeout: int = 15) -> str | None:
    """urllib.requestë§Œ ì‚¬ìš© (requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì‚¬ìš©)"""
    try:
        req = urllib.request.Request(url, headers=HEADERS)
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            charset = resp.headers.get_content_charset() or "utf-8"
            return resp.read().decode(charset, errors="replace")
    except Exception as e:
        log(f"  âš ï¸  fetch ì‹¤íŒ¨ {url[:80]}: {e}")
        return None


def make_record(
    title: str,
    category: str,
    source: str,
    url: str,
    amount: str = "",
    deadline: str = "",
    description: str = ""
) -> dict:
    return {
        "title": title,
        "category": category,
        "source": source,
        "url": url,
        "amount": amount,
        "deadline": deadline,
        "description": description,
        "collected_at": datetime.now().isoformat(),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Source 1: ê¸°ì—…ë§ˆë‹¹(BizInfo) â€” ì†Œìƒê³µì¸ ì§€ì›ì‚¬ì—…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def crawl_bizinfo_soho(
    output_file: str,
    existing_titles: set,
    checkpoint: dict,
    max_pages: int = 10
) -> int:
    source = "ê¸°ì—…ë§ˆë‹¹(BizInfo)"
    category = "ì†Œìƒê³µì¸"
    base_url = "https://www.bizinfo.go.kr/sii/siia/selectSIIA200View.do"
    count = 0

    start_page = checkpoint.get("bizinfo_soho_page", 1)
    log(f"[BizInfo-ì†Œìƒê³µì¸] p{start_page}~{start_page + max_pages - 1} ìˆ˜ì§‘...")

    for page in range(start_page, start_page + max_pages):
        url = f"{base_url}?rows=15&cpage={page}"
        html = fetch_url(url)
        if not html:
            break

        items = re.findall(
            r'<a[^>]+href=\s*"([^"]*selectSIIA200Detail[^"]*)"\s[^>]*>\s*\n?\s*([\wê°€-í£\(\)\[\]ã€Œã€,\sÂ·.\-\'/]+)',
            html
        )

        if not items:
            log(f"  p{page}: ë°ì´í„° ì—†ìŒ, ì¢…ë£Œ")
            break

        deadlines = re.findall(r'(\d{4}-\d{2}-\d{2})\s*~\s*(\d{4}-\d{2}-\d{2})', html)

        new_on_page = 0
        for i, (href, raw_title) in enumerate(items):
            title = raw_title.strip()
            if not title or len(title) < 4 or title.isdigit():
                continue
            if title in existing_titles:
                continue

            full_url = f"https://www.bizinfo.go.kr{href}" if href.startswith("/") else href
            pid_m = re.search(r'pblancId=([^&"]+)', href)
            pid = pid_m.group(1) if pid_m else ""
            deadline = f"~{deadlines[i][1]}" if i < len(deadlines) else ""

            rec = make_record(title, category, source, full_url,
                              deadline=deadline,
                              description=f"pblancId={pid}" if pid else "")
            append_record(output_file, rec)
            existing_titles.add(title)
            count += 1
            new_on_page += 1

        log(f"  p{page}: {new_on_page}ê±´ ì‹ ê·œ (ëˆ„ì  {count}ê±´)")
        checkpoint["bizinfo_soho_page"] = page + 1

        if new_on_page == 0:
            break
        time.sleep(SLEEP_SEC)

    return count


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Source 2: ê¸°ì—…ë§ˆë‹¹(BizInfo) â€” ê¸°ìˆ ì°½ì—…/R&D í•„í„°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TECH_KEYWORDS = [
    'ì°½ì—…', 'ê¸°ìˆ ', 'R&D', 'í˜ì‹ ', 'ìŠ¤íƒ€íŠ¸ì—…', 'ë²¤ì²˜', 'ì—°êµ¬', 'ê°œë°œ',
    'ë””ì§€í„¸', 'AI', 'ICT', 'ì •ë³´', 'ë°”ì´ì˜¤', 'ì œì¡°', 'ìŠ¤ë§ˆíŠ¸',
]


def crawl_bizinfo_gov(
    output_file: str,
    existing_titles: set,
    checkpoint: dict,
    max_pages: int = 5
) -> int:
    source = "ê¸°ì—…ë§ˆë‹¹(BizInfo) ê¸°ìˆ ì°½ì—…"
    category = "ê¸°ìˆ ì°½ì—…"
    base_url = "https://www.bizinfo.go.kr/sii/siia/selectSIIA200View.do"
    count = 0

    start_page = checkpoint.get("bizinfo_gov_page", 1)
    log(f"[BizInfo-ê¸°ìˆ ì°½ì—…] p{start_page}~{start_page + max_pages - 1} ìˆ˜ì§‘...")

    for page in range(start_page, start_page + max_pages):
        url = f"{base_url}?rows=15&cpage={page}"
        html = fetch_url(url)
        if not html:
            break

        items = re.findall(
            r'<a[^>]+href=\s*"([^"]*selectSIIA200Detail[^"]*)"\s[^>]*>\s*\n?\s*([\wê°€-í£\(\)\[\]ã€Œã€,\sÂ·.\-\'/]+)',
            html
        )

        if not items:
            break

        deadlines = re.findall(r'(\d{4}-\d{2}-\d{2})\s*~\s*(\d{4}-\d{2}-\d{2})', html)

        new_on_page = 0
        for i, (href, raw_title) in enumerate(items):
            title = raw_title.strip()
            if not title or len(title) < 4 or title.isdigit():
                continue
            if not any(kw in title for kw in TECH_KEYWORDS):
                continue
            if title in existing_titles:
                continue

            full_url = f"https://www.bizinfo.go.kr{href}" if href.startswith("/") else href
            deadline = f"~{deadlines[i][1]}" if i < len(deadlines) else ""

            rec = make_record(title, category, source, full_url, deadline=deadline)
            append_record(output_file, rec)
            existing_titles.add(title)
            count += 1
            new_on_page += 1

        log(f"  p{page}: {new_on_page}ê±´ ì‹ ê·œ (ëˆ„ì  {count}ê±´)")
        checkpoint["bizinfo_gov_page"] = page + 1

        if new_on_page == 0 and len(items) == 0:
            break
        time.sleep(SLEEP_SEC)

    return count


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Source 3: NIA í•œêµ­ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def crawl_nia(
    output_file: str,
    existing_titles: set,
    checkpoint: dict,
    max_pages: int = 5
) -> int:
    source = "NIA í•œêµ­ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›"
    category = "ì •ë³´í™”ì‚¬ì—…"
    cb_idx = "78336"
    base_url = f"https://www.nia.or.kr/site/nia_kor/ex/bbs/List.do?cbIdx={cb_idx}"
    view_url_tpl = "https://www.nia.or.kr/site/nia_kor/ex/bbs/View.do?cbIdx={cb}&bcIdx={bc}"
    count = 0

    start_page = checkpoint.get("nia_page", 1)
    log(f"[NIA] p{start_page}~{start_page + max_pages - 1} ìˆ˜ì§‘...")

    for page in range(start_page, start_page + max_pages):
        url = f"{base_url}&pageNo={page}"
        html = fetch_url(url)
        if not html:
            break

        items = re.findall(
            r'onclick="doBbsFView\(\'(\d+)\',\'(\d+)\',\'[^\']*\',\'[^\']*\'\)[^"]*"[^>]+title="([^"]+)"',
            html
        )

        if not items:
            log(f"  p{page}: íŒ¨í„´ ì—†ìŒ (êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥)")
            break

        new_on_page = 0
        for cb, bc, raw_title in items:
            title = re.sub(r'^[\[\(][^\]\)]+[\]\)]\s*', '', raw_title)
            title = re.sub(r'-ì²¨ë¶€íŒŒì¼\s*ìˆìŒ.*$', '', title)
            title = re.sub(r'\(ìƒˆ\s*ê¸€\).*$', '', title)
            title = title.strip() or raw_title.strip()

            if not title or len(title) < 3:
                continue
            if title in existing_titles:
                continue

            full_url = view_url_tpl.format(cb=cb, bc=bc)
            rec = make_record(title, category, source, full_url,
                              description=raw_title[:100])
            append_record(output_file, rec)
            existing_titles.add(title)
            count += 1
            new_on_page += 1

        log(f"  p{page}: {new_on_page}ê±´ ì‹ ê·œ (ëˆ„ì  {count}ê±´)")
        checkpoint["nia_page"] = page + 1

        if new_on_page == 0:
            break
        time.sleep(SLEEP_SEC)

    return count


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# JS ë Œë”ë§ í•„ìš” ì†ŒìŠ¤ â€” ìŠ¤í‚µ ìŠ¤í…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def skip_js_required(name: str, url: str, checkpoint: dict):
    log(f"[{name}] âš ï¸  JS ë Œë”ë§ í•„ìš” â€” ìŠ¤í‚µ")
    log(f"  Selenium/Playwright í™˜ê²½ì—ì„œ ë³„ë„ ìˆ˜ì§‘ í•„ìš”: {url}")
    checkpoint[name.lower().replace(" ", "_")] = "skipped_js_required"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="í•œêµ­ ì •ë¶€ì§€ì›ì‚¬ì—… í†µí•© ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸"
    )
    parser.add_argument(
        "--output", default="./data",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./data)"
    )
    parser.add_argument(
        "--max-pages", type=int, default=10,
        help="ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 10)"
    )
    args = parser.parse_args()

    output_dir = os.path.abspath(args.output)
    os.makedirs(output_dir, exist_ok=True)

    soho_file       = os.path.join(output_dir, "soho_programs.jsonl")
    gov_file        = os.path.join(output_dir, "gov_programs.jsonl")
    checkpoint_file = os.path.join(output_dir, ".checkpoint.json")

    log("=" * 60)
    log("ğŸ” í•œêµ­ ì •ë¶€ì§€ì›ì‚¬ì—… í†µí•© ìˆ˜ì§‘ ì‹œì‘")
    log(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    log("=" * 60)

    checkpoint = load_checkpoint(checkpoint_file)

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)
    soho_titles = load_existing_titles(soho_file)
    gov_titles  = load_existing_titles(gov_file)

    log(f"ê¸°ì¡´ soho_programs: {len(soho_titles)}ê±´")
    log(f"ê¸°ì¡´ gov_programs:  {len(gov_titles)}ê±´")
    log("")

    total_soho = 0
    total_gov  = 0

    # â”€â”€ ì†Œìƒê³µì¸ ì§€ì›ì‚¬ì—… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    log("â”€â”€ ì†Œìƒê³µì¸ ì§€ì›ì‚¬ì—… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # BizInfo ì†Œìƒê³µì¸
    try:
        n = crawl_bizinfo_soho(soho_file, soho_titles, checkpoint, args.max_pages)
        total_soho += n
        log(f"âœ… BizInfo-ì†Œìƒê³µì¸: {n}ê±´ ì¶”ê°€")
    except Exception as e:
        log(f"âŒ BizInfo-ì†Œìƒê³µì¸ ì˜¤ë¥˜: {e}")
        import traceback; traceback.print_exc()

    save_checkpoint(checkpoint_file, checkpoint)
    time.sleep(SLEEP_SEC)

    # SEMAS (ìŠ¤í‚µ)
    skip_js_required(
        "SEMAS ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨",
        "https://www.semas.or.kr/web/board/boardList.do",
        checkpoint
    )

    # â”€â”€ ì •ë¶€ R&D / ê¸°ìˆ ì°½ì—… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    log("")
    log("â”€â”€ ì •ë¶€ R&D / ê¸°ìˆ ì°½ì—… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # BizInfo ê¸°ìˆ ì°½ì—…
    try:
        n = crawl_bizinfo_gov(gov_file, gov_titles, checkpoint, args.max_pages // 2)
        total_gov += n
        log(f"âœ… BizInfo-ê¸°ìˆ ì°½ì—…: {n}ê±´ ì¶”ê°€")
    except Exception as e:
        log(f"âŒ BizInfo-ê¸°ìˆ ì°½ì—… ì˜¤ë¥˜: {e}")
        import traceback; traceback.print_exc()

    save_checkpoint(checkpoint_file, checkpoint)
    time.sleep(SLEEP_SEC)

    # NIA
    try:
        n = crawl_nia(gov_file, gov_titles, checkpoint, args.max_pages // 2)
        total_gov += n
        log(f"âœ… NIA: {n}ê±´ ì¶”ê°€")
    except Exception as e:
        log(f"âŒ NIA ì˜¤ë¥˜: {e}")
        import traceback; traceback.print_exc()

    save_checkpoint(checkpoint_file, checkpoint)

    # JS í•„ìš” ì†ŒìŠ¤ ìŠ¤í‚µ
    skip_js_required("MSS ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€",    "https://www.mss.go.kr/", checkpoint)
    skip_js_required("K-Startup",             "https://www.k-startup.go.kr/", checkpoint)
    skip_js_required("Innopolis ì—°êµ¬ê°œë°œíŠ¹êµ¬", "https://www.innopolis.or.kr/", checkpoint)
    skip_js_required("KISED ì°½ì—…ì§„í¥ì›",       "https://www.kised.or.kr/", checkpoint)

    # â”€â”€ ìµœì¢… ì €ì¥ & ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    save_checkpoint(checkpoint_file, checkpoint)

    log("")
    log("=" * 60)
    log(f"ğŸ ìˆ˜ì§‘ ì™„ë£Œ")
    log(f"   ì†Œìƒê³µì¸ ì‹ ê·œ: {total_soho}ê±´  (ì´ {len(soho_titles)}ê±´)")
    log(f"   R&D/ê¸°ìˆ ì°½ì—… ì‹ ê·œ: {total_gov}ê±´  (ì´ {len(gov_titles)}ê±´)")
    log(f"   ì¶œë ¥: {output_dir}")
    log("=" * 60)
    log("")
    log("ğŸ“ JS ë Œë”ë§ í•„ìš” ì‚¬ì´íŠ¸ (Selenium/Playwright í•„ìš”):")
    log("   - SEMAS: https://www.semas.or.kr/web/board/boardList.do")
    log("   - MSS:   https://www.mss.go.kr/")
    log("   - K-Startup: https://www.k-startup.go.kr/")
    log("   - Innopolis: https://www.innopolis.or.kr/")
    log("   - KISED: https://www.kised.or.kr/")


if __name__ == "__main__":
    main()
