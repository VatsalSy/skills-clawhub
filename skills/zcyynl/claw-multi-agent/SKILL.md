---
name: claw-multi-agent
description: Multi-agent parallel orchestration for OpenClaw. Spawn AI agents as a team â€” parallel research, multi-model comparison, code pipelines. Proven 50-65% time savings. Trigger words: multi-agent, parallel agents, swarm, spawn multiple agents, parallel research, compare models, deep research, comprehensive research, detailed investigation, thorough analysis, research multiple topics, å¤šæ™ºèƒ½ä½“, å¤šä¸ªAgent, å¹¶è¡Œè°ƒç ”, å¹¶è¡Œæœç´¢, åŒæ—¶æœç´¢, åŒæ—¶è°ƒç ”, æ·±åº¦è°ƒç ”, è¯¦ç»†è°ƒç ”, å…¨é¢è°ƒç ”, æ·±åº¦ç ”ç©¶, è¯¦ç»†æ£€ç´¢, å¤šè§’åº¦åˆ†æ, å…¨é¢åˆ†æ, å¤šä¸ªæ¨¡å‹, è®©å¤šä¸ªAI, åˆ†åˆ«æœç´¢, åŒæ—¶æœç´¢, ç»„å»ºå›¢é˜Ÿ, Agentå°é˜Ÿ, å¤šAgent.
---

# claw-multi-agent ğŸ

> **Replace one AI with a team of AIs. Turn serial into parallel. Turn hours into minutes.**

---

## What can it do?

| Scenario | Example | Speedup |
|----------|---------|---------|
| **Parallel research** | Search 5 frameworks simultaneously, each writes a report | ~65% âš¡ |
| **Multi-model compare** | Ask Claude, Gemini, Kimi the same question at the same time | ~50% âš¡ |
| **Code pipeline** | Plan â†’ Code â†’ Review, auto hand-off in sequence | Quality â†‘ |
| **Batch processing** | Translate / analyze / summarize multiple docs in parallel | Scales linearly |

---

## âš¡ Get started in 30 seconds

Just say something like:

- "Research LangChain, CrewAI, and AutoGen in parallel"
- "Have multiple agents search these topics and write a combined report"
- "Compare how Claude and Gemini answer this question"
- "Use multi-agent mode to do this research"

---

## ğŸ­ Interaction Style â€” How to Talk to the User

This is the recommended pattern. Every multi-agent run must follow this interaction pattern.

### Step 0 â€” Announce skill activation FIRST

**âš ï¸ Iron rule: The activation announcement must be your FIRST reply after receiving the task â€” before reading any files, before investigating, before spawning.**

**Why this matters**: Reading files, researching background, and spawning all take time. If you do those first, users see long silence. Worse: context compression can happen during that time, and the announcement will never be sent.

**Correct order**: Receive task â†’ Send announcement immediately â†’ Then read files / spawn / wait

The very first thing to say when this skill is triggered â€” before any planning or spawning:

```
ğŸ **claw-multi-agent å·²å”¤é†’**
å¤šæ™ºèƒ½ä½“å¹¶è¡Œæ¨¡å¼å¯åŠ¨ï¼Œæˆ‘æ¥ç»„å»º Agent å°é˜Ÿå¤„ç†è¿™ä¸ªä»»åŠ¡ã€‚
```

This tells the user the skill is active and sets expectations for what's about to happen.

### Before spawning â€” announce the plan

Right after the activation announcement, present the plan BEFORE calling sessions_spawn:

```
ğŸš€ [N]ä¸ªæ–¹å‘åŒæ—¶å¼€æï¼Œå…¨é¢è¦†ç›–ä½ çš„é—®é¢˜ã€‚

ğŸ“‹ ä»»åŠ¡è§„åˆ’ï¼š
ğŸ” ç ”ç©¶å‘˜Aï¼ˆGLMï¼‰â€” [ä¸€å¥è¯ä»»åŠ¡æè¿°]
ğŸ” ç ”ç©¶å‘˜Bï¼ˆGLMï¼‰â€” [ä¸€å¥è¯ä»»åŠ¡æè¿°]
ğŸ“Š åˆ†æå¸ˆï¼ˆKimiï¼‰â€” å…ˆç­‰å‰[N]ä¸ªç»“æœï¼Œå•ç‹¬å¬å”¤ï¼ˆnote when sequentialï¼‰

æ¨¡å¼ï¼šğŸ¯ æŒ‡æŒ¥å®˜æ¨¡å¼ï¼ˆè”ç½‘æœç´¢ï¼‰
é¢„è®¡è€—æ—¶ï¼š~[X]sï¼ˆ[N] Agent å¹¶è¡Œ[ï¼Œåˆ†æå¸ˆä¸²è¡Œè·Ÿè¿›]ï¼‰
æ­£åœ¨æ´¾å‡º Agent å°é˜Ÿ...
```

**Role emoji reference:**
| Role | Emoji | Example |
|------|-------|---------|
| Researcher | ğŸ” | ğŸ” ç ”ç©¶å‘˜Aï¼ˆGLMï¼‰â€” Research XX |
| Analyst | ğŸ“Š | ğŸ“Š åˆ†æå¸ˆï¼ˆKimiï¼‰â€” Deep comparison |
| Writer | âœï¸ | âœï¸ å†™ä½œè€…ï¼ˆGeminiï¼‰â€” Draft the report |
| Coder | ğŸ’» | ğŸ’» ç¨‹åºå‘˜ï¼ˆKimiï¼‰â€” Implement the logic |
| Reviewer | ğŸ” | ğŸ” å®¡æ ¸å‘˜ï¼ˆGLMï¼‰â€” Quality check |
| Planner | ğŸ“‹ | ğŸ“‹ è§„åˆ’å¸ˆï¼ˆSonnetï¼‰â€” Break down tasks |

**Key rules:**
- âœ… Always list each agent with: emoji + role + **model name** + one-line task
- âœ… State the mode (æŒ‡æŒ¥å®˜/æµæ°´çº¿/æ··åˆ) and estimated time
- âœ… End announcement with: `æ­£åœ¨æ´¾å‡º Agent å°é˜Ÿ...`
- âœ… Note sequential agents as: "å…ˆç­‰å‰Nä¸ªç»“æœï¼Œå•ç‹¬å¬å”¤"
- âŒ Never silently call sessions_spawn without announcing

### While waiting â€” brief note

After spawning, say one line:
```
â³ å­ Agent å·²å…¨éƒ¨å‡ºå‘ï¼Œç­‰ç»“æœå›æ¥...
```

### After results â€” structured output (not raw dump)

**Never paste sub-agent raw output directly.** Always digest and restructure by content logic â€” NOT by agent order.

**Recommended output order:**

```
1. æ‰§è¡Œç»Ÿè®¡å¡ â† å…ˆè®©ç”¨æˆ·çŸ¥é“è·‘äº†ä»€ä¹ˆ
2. æ ¸å¿ƒç»“è®ºï¼ˆ3-5æ¡æœ€é‡è¦å‘ç°ï¼‰â† æœ€æœ‰ä»·å€¼çš„æ”¾æœ€å‰é¢
3. åˆ†ä¸»é¢˜å±•å¼€ç»†èŠ‚ï¼ˆæŒ‰å†…å®¹é€»è¾‘ç»„ç»‡ï¼Œä¸æŒ‰å­Agenté¡ºåºï¼‰â† è¯»èµ·æ¥æ˜¯ä¸€ç¯‡å®Œæ•´æ–‡ç« 
4. ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®® â† è½åœ°ç»“å°¾
```

**ç»Ÿè®¡å¡æ ¼å¼ï¼š**
```
## ğŸ“Š æ‰§è¡Œç»Ÿè®¡
| Agent | æ¨¡å‹ | è€—æ—¶ | çŠ¶æ€ |
|-------|------|------|------|
| ğŸ” ç ”ç©¶å‘˜A | GLM | 58s | âœ… |
| ğŸ” ç ”ç©¶å‘˜B | GLM | 62s | âœ… |
| ğŸ“Š åˆ†æå¸ˆ  | Kimi | 45s | âœ… |
ä¸²è¡Œéœ€è¦çº¦ 165s â†’ å¹¶è¡Œå®é™… 62sï¼ŒèŠ‚çœ **62%** âš¡
```

**âŒ Wrong â€” agent order:**
```
å­Agent1çš„ç»“æœ...
å­Agent2çš„ç»“æœ...
å­Agent3çš„ç»“æœ...  â† è¯»è€…è¦è‡ªå·±æ‹¼å›¾ï¼Œä½“éªŒå·®
```

**âœ… Right â€” content logic:**
```
## æ ¸å¿ƒç»“è®º
1. æœ€é‡è¦å‘ç°Aï¼ˆæ¥è‡ªå¤šä¸ªAgentç»¼åˆï¼‰
2. æœ€é‡è¦å‘ç°B
...

## è¯¦ç»†åˆ†æï¼š[ä¸»é¢˜1]
...ï¼ˆæ•´åˆæ‰€æœ‰ç›¸å…³Agentçš„å†…å®¹ï¼‰

## è¯¦ç»†åˆ†æï¼š[ä¸»é¢˜2]
...

## ä¸‹ä¸€æ­¥å»ºè®®
...
```

**The main agent rewrites everything in its own words.** Sub-agent outputs are raw material, not the final answer.

### After results â€” deliver the report (channel-aware)

**Always save to file first. Then deliver based on the current channel.**

```python
# Step 1: Always save to file first
write("/workspace/projects/{topic-slug}/report.md", content)
```

**Then choose delivery method by channel:**

| Channel | Delivery method |
|---------|----------------|
| **feishu** + has `feishu-all-operations` skill | Create Feishu doc â†’ send link (best UX) |
| **feishu** + no Feishu skill | `message(filePath=..., filename="report.md")` â€” send as attachment |
| **Discord / Telegram / Slack** | `message(message=...)` â€” Markdown renders normally |
| **Other / unknown** | Save file + tell the user the path |

**Why this matters:** Feishu chat does NOT render Markdown. Sending raw Markdown text shows `##`, `|---|` symbols. Always use attachment or doc link on Feishu.

```python
# Feishu (no Feishu doc skill): send as attachment
message(action="send", filePath="/workspace/projects/{topic-slug}/report.md", filename="report.md")

# Discord/Telegram: send markdown directly
message(action="send", message=report_content)
```

**End with one line:**
```
éœ€è¦è°ƒæ•´æŸä¸ªæ–¹å‘ï¼Œæˆ–æ¨é€åˆ°é£ä¹¦æ–‡æ¡£å—ï¼Ÿ
```

**Rules:**
- âœ… Always save `.md` file first â€” regardless of channel
- âœ… Check current channel before deciding how to send
- âŒ Never paste >300 words of Markdown text on Feishu â€” it won't render
- âŒ Never just say "æŠ¥å‘Šå·²ä¿å­˜è‡³ /path/xxx" â€” user can't open server paths
- âŒ Never ask "è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†æˆæ–‡æ¡£ï¼Ÿ" â€” just do it

### Sequential vs parallel â€” analyst must wait for researchers

**Critical:** Agents spawned in the same round run in parallel and share NO context with each other.

```
âŒ Wrong: spawn researcher-A + researcher-B + analyst all at once
          â†’ analyst has no data, returns empty

âœ… Right: 
  Round 1: spawn researcher-A + researcher-B (parallel, independent)
  Wait for both to return...
  Round 2: main agent consolidates research results
           â†’ then either: main agent writes analysis itself
           â†’ or: spawn analyst with research results injected as context
```

**Best practice: Any agent that depends on another agent's output should be spawned in a later round, after collecting the dependency.**

---

## ğŸ¤– Model Selection Guide â€” Which Model for Which Role

Always pick the right model for each agent. State the model explicitly in the announcement.

### Model roster

| æ¨¡å‹ | åˆ«å | ç‰¹ç‚¹ | é€‚åˆè§’è‰² |
|------|------|------|---------|
| `glm` | GLM | ä¾¿å®œã€é€Ÿåº¦å¿«ã€ä¸­æ–‡å¥½ | æœç´¢ã€ç®€å•è°ƒç ”ã€çŠ¶æ€æ£€æŸ¥ |
| `kimi` | Kimi | é•¿ä¸Šä¸‹æ–‡ï¼ˆ128kï¼‰ã€ä»£ç å¼º | æ·±åº¦åˆ†æã€ä»£ç ã€é•¿æ–‡æ•´åˆ |
| `gemini` | Gemini | åˆ›æ„å¥½ã€å¤šæ¨¡æ€ | å†™ä½œã€æ–‡æ¡ˆã€å›¾åƒç†è§£ |
| `sonnet` | Claude Sonnet | å‡è¡¡ã€å·¥å…·è°ƒç”¨ç¨³ | å¤æ‚æ¨ç†ã€è§„åˆ’ã€å®¡æ ¸ |
| `opus` | Claude Opus | æœ€å¼ºæ¨ç† | æå¤æ‚åˆ†æã€æ¶æ„è®¾è®¡ |

### Role â†’ Model mapping (default)

| è§’è‰² | é»˜è®¤æ¨¡å‹ | åŸå›  |
|------|---------|------|
| ğŸ” ç ”ç©¶å‘˜ / Researcher | **GLM** | è½»é‡æœç´¢ï¼Œå¤Ÿç”¨ä¸”ä¾¿å®œ |
| ğŸ“Š åˆ†æå¸ˆ / Analyst | **Kimi** | é•¿ä¸Šä¸‹æ–‡ï¼Œå¤„ç†å¤§é‡èµ„æ–™ |
| âœï¸ å†™ä½œè€… / Writer | **Gemini** | åˆ›æ„å†™ä½œæ•ˆæœæœ€å¥½ |
| ğŸ’» ç¨‹åºå‘˜ / Coder | **Kimi** | é•¿ä¸Šä¸‹æ–‡ä»£ç ç†è§£ |
| ğŸ” å®¡æ ¸å‘˜ / Reviewer | **GLM** | ç®€å•åˆ¤æ–­ï¼Œä¸éœ€é‡ç‚® |
| ğŸ“‹ è§„åˆ’å¸ˆ / Planner | **Sonnet** | ç»“æ„åŒ–è§„åˆ’èƒ½åŠ›å¼º |
| ğŸ§ æ‰¹è¯„è€… / Critic | **Sonnet** | é€»è¾‘ä¸¥è°¨ï¼ŒæŒ‘æˆ˜å‡è®¾ |

### When to override defaults

- ä»»åŠ¡å¾ˆç®€å• â†’ é™çº§åˆ° GLMï¼ˆçœæˆæœ¬ï¼‰
- éœ€è¦æœ€é«˜è´¨é‡ â†’ å‡çº§åˆ° Opus
- ç”¨æˆ·æ˜ç¡®æŒ‡å®šæ¨¡å‹ â†’ ç…§ç”¨æˆ·è¯´çš„æ¥
- å¤šæ¨¡å‹å¯¹æ¯”åœºæ™¯ â†’ æ¯ä¸ª Agent ç”¨ä¸åŒæ¨¡å‹ï¼Œåœ¨å…¬å‘Šé‡Œè¯´æ˜

### Always announce the model

In the pre-spawn announcement, every agent line must include the model:
```
âœ… è¿™æ ·ï¼šğŸ” ç ”ç©¶å‘˜Aï¼ˆGLMï¼‰â€” è°ƒç ” LangChain
âŒ è¿™æ ·ï¼šğŸ” ç ”ç©¶å‘˜A â€” è°ƒç ” LangChain
```

---

## Step 0: Always plan first (dynamic agent count)

**Never hardcode how many agents to spawn.** The right number depends on the task complexity. Always start with a planning step:

```
1. Analyze the task â†’ identify subtopics / dimensions
2. Decide: how many agents? which roles? which mode?
3. Spawn accordingly (could be 2, could be 10)
4. Consolidate results
```

**Example planning output:**
```
Task: "Research the top AI agent frameworks"
â†’ Plan: 5 researchers (one per framework) + 1 analyst for comparison
â†’ Mode: Orchestrator (needs web search)
â†’ Spawn: 5 parallel sub-agents
```

**The number of agents should match the task, not a template.**

---

## Three modes â€” auto-routed by intent

**You don't need to say which mode.** Just describe the task. The skill reads these two signals:

1. **Need web search / real-time info?** â†’ use sessions_spawn (has tools)
2. **Want multiple draft versions to compare?** â†’ spawn parallel writers

```
User says anything
        â†“
  Wants multiple versions / drafts / angles?
        YES â”€â”€â†’ Also needs web search?
        â”‚              YES â†’ ğŸ”€ Hybrid Mode   (search first, then N drafts)
        â”‚              NO  â†’ ğŸ”„ Pipeline Mode (N drafts in parallel, pure text)
        â”‚
        NO  â”€â”€â†’ Needs web search / file ops?
                       YES â†’ ğŸ¯ Orchestrator Mode (sessions_spawn, parallel)
                       NO  â†’ ğŸ”„ Pipeline Mode     (pure text, faster)
```

**Trigger signals the skill listens for:**

| Signal | Examples | Mode triggered |
|--------|---------|---------------|
| Multi-draft intent | "å‡ ä¸ªç‰ˆæœ¬", "å¤šä¸ªè§’åº¦", "è®©æˆ‘æŒ‘", "å„è‡ªå†™", "different styles" | Pipeline or Hybrid |
| Search intent | "æœç´¢", "æœ€æ–°", "è°ƒç ”", "è”ç½‘", "search", "latest" | Orchestrator or Hybrid |
| Both | "æœç´¢åç»™æˆ‘å‡ ç‰ˆæŠ¥å‘Š", "research then write multiple drafts" | **Hybrid** |
| Neither | "ç¿»è¯‘", "åˆ†æ", "å†™ä½œ", plain text tasks | Pipeline |

You can also check with the router directly:
```bash
python scripts/router.py mode "æœç´¢ç«å“èµ„æ–™ï¼Œå¸®æˆ‘å†™3ä¸ªç‰ˆæœ¬çš„åˆ†æ"
# â†’ ğŸ”€ HYBRID
python scripts/router.py mode "è°ƒç ”LangChainå¹¶å†™ä¸€ä»½æŠ¥å‘Š"
# â†’ ğŸ¯ ORCHESTRATOR
python scripts/router.py mode "ç”¨ä¸‰ä¸ªè§’åº¦åˆ†æè¿™ä¸ªæ–¹æ¡ˆ"
# â†’ ğŸ”„ PIPELINE
```

---

## ğŸ¯ Orchestrator Mode (with tools, truly parallel)

Sub-agents launched via `sessions_spawn`. Each has full OpenClaw tools: web search, file read/write, code execution.

**âš¡ How parallelism works:**
Call multiple `sessions_spawn` in the **same tool-call round** â€” OpenClaw executes them simultaneously. All sub-agents run at once; the main agent collects all results when they finish.

```
Same round â†’ parallel execution:

sessions_spawn(task="Search LangChain...") â”€â”€â”
sessions_spawn(task="Search CrewAI...")    â”€â”€â”¤â†’ all run simultaneously
sessions_spawn(task="Search AutoGen...")   â”€â”€â”˜
sessions_spawn(task="Search LangGraph...") â”€â”˜

â†“  (all finish, main agent receives all 4 results)

Main agent consolidates â†’ writes full report
```

**Sequential = spawn one, wait for result, then spawn next.** Use this only when a later task depends on an earlier result (e.g. write report AFTER research is done).

**How to spawn â€” always include role, model hint, and what to return:**

```python
# Parallel research: spawn all 4 in the same round â†’ they run simultaneously
sessions_spawn({
    "task": "[CONTEXT] Comparing AI agent frameworks for a tech team report.\n\n[YOUR TASK] Search LangChain: architecture, pros/cons, GitHub stars, latest version. Return 5 bullet points â‰¤100 words each. Do NOT write a full report.",
    "label": "ğŸ” researcher-langchain [model: default]"
})
sessions_spawn({
    "task": "[CONTEXT] Same report.\n\n[YOUR TASK] Search CrewAI: architecture, pros/cons, GitHub stars, latest version. Return 5 bullet points â‰¤100 words each.",
    "label": "ğŸ” researcher-crewai [model: default]"
})
sessions_spawn({
    "task": "[CONTEXT] Same report.\n\n[YOUR TASK] Search AutoGen: architecture, pros/cons, GitHub stars, latest version. Return 5 bullet points â‰¤100 words each.",
    "label": "ğŸ” researcher-autogen [model: default]"
})
sessions_spawn({
    "task": "[CONTEXT] Same report.\n\n[YOUR TASK] Search LangGraph: architecture, pros/cons, GitHub stars, latest version. Return 5 bullet points â‰¤100 words each.",
    "label": "ğŸ” researcher-langgraph [model: default]"
})
# All 4 run in parallel â†’ when all return, main agent consolidates and writes report
```

**Mixed: parallel then sequential** (most common pattern):
```python
# Phase 1: parallel research (spawn all at once)
sessions_spawn({"task": "[CONTEXT] ...\n\n[TASK] Search LangChain. 5 bullets â‰¤100 words.", "label": "ğŸ” researcher-langchain"})
sessions_spawn({"task": "[CONTEXT] ...\n\n[TASK] Search CrewAI. 5 bullets â‰¤100 words.", "label": "ğŸ” researcher-crewai"})
sessions_spawn({"task": "[CONTEXT] ...\n\n[TASK] Search AutoGen. 5 bullets â‰¤100 words.", "label": "ğŸ” researcher-autogen"})

# Phase 2: after all 3 return â†’ main agent writes report (sequential, depends on research)
# (main agent does this directly, no need to spawn a writer)
```

**Key rules:**
- âœ… **Same round = parallel**: spawn multiple agents at once for independent tasks
- âœ… **Sequential**: spawn one, wait for result, then spawn next â€” only when tasks depend on each other
- âœ… Sub-agents return **summaries only** (â‰¤100 words per point)
- âœ… Main agent **writes the full report** (avoids token limit failures)
- âœ… Label each agent clearly: role + what model it's using
- âŒ Don't ask a sub-agent to both search AND write a long report

---

## ğŸ”„ Pipeline Mode (pure text, any task)

Runs agents via Python CLI. **No web search, but works for any pure-text task**: writing, analysis, translation, multi-model comparison, brainstorming, code generation.

```bash
cd ~/.openclaw/skills/claw-multi-agent

# Parallel: multiple agents tackle different angles simultaneously
python run.py --mode parallel \
  --agents "fast:ğŸ” researcher:summarize the pros of microservice architecture" \
           "fast:ğŸ” researcher:summarize the cons of microservice architecture" \
           "fast:ğŸ” researcher:list real-world companies using microservices and outcomes" \
           "smart:ğŸ“Š analyst:compare microservices vs monolith for a 10-person startup" \
  --aggregation synthesize

# Sequential: chain agents, each builds on the previous output
python run.py --mode sequential \
  --agents "fast:ğŸ“‹ planner:break down how to build a REST API in Python" \
           "smart:ğŸ’» coder:implement the API based on the plan above" \
           "fast:ğŸ” reviewer:review the code for bugs and security issues" \
  --aggregation last

# Auto-route: router classifies task and picks tiers automatically
python run.py --auto-route --task "write a technical blog post about GRPO vs PPO"

# Dry-run: preview the plan without executing
python run.py --dry-run \
  --agents "fast:researcher:research X" "smart:writer:write report"
```

**Pipeline mode works great for:**
- Multi-angle analysis (spawn one agent per dimension)
- Multi-model comparison (same task, different models)
- Code pipeline (plan â†’ code â†’ review)
- Batch writing (translate/summarize N documents in parallel)

---

## ğŸ”€ Hybrid Mode (search + multi-draft)

Best of both worlds: sub-agents search the web (with tools), then multiple writers generate parallel drafts from the research.

**When it kicks in:** user wants both real-time research AND multiple versions to compare.

```
Phase 1 (Orchestrator â€” with tools, parallel):
  sessions_spawn(search topic A) â”€â”€â”
  sessions_spawn(search topic B) â”€â”€â”¤ â†’ all run simultaneously
  sessions_spawn(search topic C) â”€â”€â”˜
  â†“ research summaries collected

Phase 2 (Pipeline â€” pure text, parallel):
  openclaw agent (writer style 1) â”€â”€â”
  openclaw agent (writer style 2) â”€â”€â”¤ â†’ all run simultaneously
  openclaw agent (writer style 3) â”€â”€â”˜
  â†“ 3 draft versions returned

Main agent: compare drafts â†’ pick best or synthesize
```

**CLI usage:**
```bash
# Auto: router detects hybrid intent and runs both phases
python run.py --mode hybrid --task "è°ƒç ”ä¸»æµAIæ¡†æ¶ï¼Œç»™æˆ‘3ä¸ªä¸åŒé£æ ¼çš„å¯¹æ¯”æŠ¥å‘Š" --num-drafts 3

# Auto-mode: let router decide the mode automatically
python run.py --auto-mode --task "æœç´¢ç«å“èµ„æ–™åå†™å‡ ä¸ªç‰ˆæœ¬çš„åˆ†æ"
```

**In conversation (sessions_spawn approach):**
```python
# Phase 1: parallel research (spawn all at once)
sessions_spawn({"task": "[CONTEXT] ...\n\n[TASK] Search LangChain. 5 bullets.", "label": "ğŸ” research-langchain"})
sessions_spawn({"task": "[CONTEXT] ...\n\n[TASK] Search CrewAI. 5 bullets.", "label": "ğŸ” research-crewai"})
sessions_spawn({"task": "[CONTEXT] ...\n\n[TASK] Search AutoGen. 5 bullets.", "label": "ğŸ” research-autogen"})

# After all 3 return â†’ Phase 2: main agent writes 3 draft versions itself
# (or spawn 3 pipeline agents with research as context)
```

---

## Smart Router

Built-in task classifier. Auto-picks the right tier based on keywords:

```bash
python scripts/router.py classify "write a Python web scraper"
# â†’ Tier: CODE  (routes to smart model)

python scripts/router.py classify "research the latest LLM papers"
# â†’ Tier: RESEARCH  (routes to fast model)

python scripts/router.py spawn --json --multi "research X and write a report"
# â†’ splits into 2 tasks: RESEARCH + CREATIVE
```

| Tier | Model | Used for |
|------|-------|---------|
| `FAST` | default (light) | Simple queries, status, translation, search |
| `CODE` | default (smart) | Programming, debugging, implementation |
| `RESEARCH` | default (light) | Research, search, compare, survey |
| `CREATIVE` | default (smart) | Writing, articles, documentation |
| `REASONING` | default (best) | Architecture, logic, complex analysis |

---

## contextSharing: Give sub-agents background

Sub-agents start as fresh sessions â€” they don't know your goal. Add a `[CONTEXT]` block.

**Pattern 1: recent** (recommended â€” works for 95% of cases)
```
[CONTEXT] User is comparing AI agent frameworks for a team report. Audience: engineers.

[YOUR TASK] Search LangChain pros and cons. Return 5 bullet points â‰¤100 words each.
```

**Pattern 2: summary** (sequential tasks â€” pass prior results forward)
```
[PRIOR FINDINGS]
- LangChain: richest ecosystem, steep curve
- CrewAI: clean role separation...

[YOUR TASK] Based on above, search AutoGen. Return 3 unique points not covered above.
```

**Pattern 3: full** (complex background â€” let agent read a file)
```
[CONTEXT FILE] Read /workspace/research/context.md for full background.

[YOUR TASK] Search latest Test-Time Compute Scaling advances. Return 3 summaries.
```

**Reuse context across parallel agents:**
```python
BG = "Researching RL post-training for ML engineers. Topics: GRPO/DAPO/PPO, veRL."

sessions_spawn({"task": f"[CONTEXT] {BG}\n\n[TASK] Search GRPO vs PPO benchmarks. 5 bullets â‰¤100 words.", "label": "ğŸ” researcher-grpo [model: default]"})
sessions_spawn({"task": f"[CONTEXT] {BG}\n\n[TASK] Search DAPO design. 5 bullets â‰¤100 words.", "label": "ğŸ” researcher-dapo [model: default]"})
sessions_spawn({"task": f"[CONTEXT] {BG}\n\n[TASK] Search veRL architecture. 5 bullets â‰¤100 words.", "label": "ğŸ” researcher-verl [model: default]"})
```

---

## Execution summary â€” always output this

After every multi-agent run, print a standard card:

```
## ğŸ“Š Execution Summary

Mode: ğŸ¯ Orchestrator Mode (sessions_spawn, with tools)

| Agent | Role | Model | Time | Status |
|-------|------|-------|------|--------|
| ğŸ” researcher-langchain | Researcher | default | 22s | âœ… |
| ğŸ” researcher-crewai    | Researcher | default | 19s | âœ… |
| ğŸ” researcher-autogen   | Researcher | default | 24s | âœ… |
| ğŸ” researcher-langgraph | Researcher | default | 21s | âœ… |
| âœï¸ main (consolidate)   | Writer     | default | 38s | âœ… |

Agents spawned: 4  |  Parallel time: ~24s  |  Serial equivalent: ~86s  |  Saved: ~62s (72%)
```

**Always include:**
- Mode (Orchestrator / Pipeline + Sequential/Parallel)
- Each agent's role emoji + name + model used
- Actual elapsed time per agent
- Total parallel time vs serial equivalent

---

## Preset roles

| Role | Emoji | Best for |
|------|-------|---------|
| `researcher` | ğŸ” | Web search, info gathering |
| `writer` | âœï¸ | Reports, documentation, articles |
| `coder` | ğŸ’» | Code writing, debugging, implementation |
| `analyst` | ğŸ“Š | Data analysis, comparison, statistics |
| `reviewer` | ğŸ” | Code / content review, QA |
| `planner` | ğŸ“‹ | Task planning, decomposition |
| `critic` | ğŸ§ | Risk analysis, devil's advocate |

---

## âš ï¸ Gotchas

### Gotcha 0: Reading files before announcing (most common mistake)
Investigating context before sending the activation announcement causes long silence and risks losing the announcement entirely due to context compression.

- âŒ Receive task â†’ read operators.py â†’ read README â†’ announce â†’ spawn
- âœ… Receive task â†’ **announce immediately** (can say "analyzing task...") â†’ read files â†’ spawn

### Gotcha 1: Sub-agent output token limit
Sub-agents have a ~4096 token output cap. Exceeded â†’ tool args truncated â†’ file writes silently fail.

- âŒ "search AND write a 2000-word report"
- âœ… Sub-agent returns summaries; **main agent writes the report**

### Gotcha 2: Orchestrator Mode has no tools in Pipeline Mode
`python run.py` processes have no `web_search`, `exec`, etc.

- âŒ Pipeline mode: "search the latest news on X"
- âœ… Anything needing real web access â†’ Orchestrator Mode

### Gotcha 3: Parallel agents can't depend on each other
Agents spawned in the same round run simultaneously.

- âŒ Agent-2: "based on Agent-1's results..."
- âœ… Parallel = independent; sequential = chained

### Gotcha 4: Don't hardcode agent count
Match agents to the task, not to a template.

- âŒ Always spawn exactly 3 agents
- âœ… Plan first, then decide: simple task â†’ 2 agents, complex â†’ 8+ agents

---

## Pipeline mode quick reference

```bash
python run.py
  --mode parallel|sequential
  --agents "tier_or_model:ğŸ­role:task description"   # repeatable, any number
  --aggregation synthesize|compare|concatenate|last
  --timeout 300
  --dry-run          # preview without executing
  --auto-route       # router picks tiers automatically
  --list-models      # show current model config
```

| Aggregation | Effect |
|-------------|--------|
| `synthesize` | Main agent summarizes all outputs (default) |
| `compare` | Side-by-side of each agent's output |
| `concatenate` | Outputs joined in order |
| `last` | Final agent's output only (sequential) |
