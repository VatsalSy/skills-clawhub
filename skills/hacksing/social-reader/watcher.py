"""
æŽ¨ç‰¹ Watcher èŠ‚ç‚¹ â€” ä¿¡æºå·¡é€»ä¸Žé˜²é‡ç¼“å­˜

ä½¿ç”¨æ–¹å¼ï¼š
  1. æ‰‹åŠ¨æ¨¡å¼ï¼šå°†æŽ¨æ–‡ URL å†™å…¥ input_urls.txtï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼Œè¿è¡Œè„šæœ¬è‡ªåŠ¨æŠ“å–
  2. ç¨‹åºè°ƒç”¨ï¼šè°ƒç”¨ watch() å‡½æ•°ï¼Œä¼ å…¥ URL åˆ—è¡¨

è¾“å‡ºï¼špending_tweets.jsonï¼ˆæ´—å‡€çš„æ–°æŽ¨æ–‡åˆ—è¡¨ï¼Œä¾› Processor æ¶ˆè´¹ï¼‰
"""

import json
import os
import sys
from datetime import datetime
from fetcher import get_tweet

# æ–‡ä»¶è·¯å¾„å¸¸é‡
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SEEN_IDS_FILE = os.path.join(BASE_DIR, "seen_ids.json")
INPUT_URLS_FILE = os.path.join(BASE_DIR, "input_urls.txt")
PENDING_FILE = os.path.join(BASE_DIR, "pending_tweets.json")


def load_seen_ids():
    """åŠ è½½å·²è¯»æŽ¨æ–‡ ID é›†åˆ"""
    if os.path.exists(SEEN_IDS_FILE):
        with open(SEEN_IDS_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()


def save_seen_ids(seen_ids):
    """æŒä¹…åŒ–å·²è¯»æŽ¨æ–‡ ID"""
    with open(SEEN_IDS_FILE, "w", encoding="utf-8") as f:
        json.dump(list(seen_ids), f, ensure_ascii=False, indent=2)


def load_input_urls():
    """ä»Ž input_urls.txt è¯»å–å¾…æŠ“å–çš„ URL åˆ—è¡¨"""
    if not os.path.exists(INPUT_URLS_FILE):
        return []
    with open(INPUT_URLS_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip() and not line.startswith("#")]
    return urls


def extract_id_from_url(url):
    """ä»Ž URL ä¸­æå–æŽ¨æ–‡ IDï¼ˆå¤ç”¨ fetcher çš„é€»è¾‘ï¼‰"""
    from fetcher import extract_tweet_id
    return extract_tweet_id(url)


def watch(urls=None):
    """
    æ ¸å¿ƒå·¡é€»é€»è¾‘ï¼š
    1. æŽ¥æ”¶ URL åˆ—è¡¨ï¼ˆé»˜è®¤ä»Ž input_urls.txt è¯»å–ï¼‰
    2. è¿‡æ»¤å·²è¯» ID
    3. é€æ¡æŠ“å–æ–°æŽ¨æ–‡
    4. è¾“å‡ºåˆ° pending_tweets.json
    
    è¿”å›žï¼šæ–°æŠ“å–çš„æŽ¨æ–‡æ•°é‡
    """
    if urls is None:
        urls = load_input_urls()

    if not urls:
        print("âš  æ²¡æœ‰å¾…å¤„ç†çš„ URLï¼Œè¯·åœ¨ input_urls.txt ä¸­æ·»åŠ æŽ¨æ–‡é“¾æŽ¥", file=sys.stderr)
        return 0

    seen_ids = load_seen_ids()
    new_tweets = []
    skipped = 0
    failed = 0

    print(f"ðŸ“¡ å¼€å§‹å·¡é€»ï¼Œå…± {len(urls)} æ¡ URL...")

    for i, url in enumerate(urls, 1):
        tweet_id = extract_id_from_url(url)
        if not tweet_id:
            print(f"  [{i}/{len(urls)}] âœ— æ— æ³•è§£æž ID: {url}")
            failed += 1
            continue

        if tweet_id in seen_ids:
            print(f"  [{i}/{len(urls)}] â†’ å·²è¯»è·³è¿‡: {tweet_id}")
            skipped += 1
            continue

        print(f"  [{i}/{len(urls)}] æŠ“å–ä¸­: {tweet_id}...", end=" ")
        result = get_tweet(url)

        if result.get("success"):
            result["tweet_id"] = tweet_id
            result["original_url"] = url
            result["fetched_at"] = datetime.now().isoformat()
            new_tweets.append(result)
            seen_ids.add(tweet_id)
            print("âœ“")
        else:
            print(f"âœ— {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            failed += 1

    # æŒä¹…åŒ–ç»“æžœ
    save_seen_ids(seen_ids)

    # è¿½åŠ æ¨¡å¼ï¼šå¦‚æžœ pending æ–‡ä»¶å·²æœ‰å†…å®¹ï¼Œåˆå¹¶è¿›åŽ»
    existing = []
    if os.path.exists(PENDING_FILE):
        with open(PENDING_FILE, "r", encoding="utf-8") as f:
            try:
                existing = json.load(f)
            except json.JSONDecodeError:
                existing = []

    all_pending = existing + new_tweets
    with open(PENDING_FILE, "w", encoding="utf-8") as f:
        json.dump(all_pending, f, ensure_ascii=False, indent=2)

    print(f"\nðŸ“Š å·¡é€»å®Œæˆ: æ–°å¢ž {len(new_tweets)} | è·³è¿‡ {skipped} | å¤±è´¥ {failed}")
    print(f"   å¾…å¤„ç†é˜Ÿåˆ—: {len(all_pending)} æ¡ â†’ {PENDING_FILE}")

    return len(new_tweets)


if __name__ == "__main__":
    # æ”¯æŒå‘½ä»¤è¡Œç›´æŽ¥ä¼ å…¥ URL
    if len(sys.argv) > 1:
        watch(sys.argv[1:])
    else:
        watch()
