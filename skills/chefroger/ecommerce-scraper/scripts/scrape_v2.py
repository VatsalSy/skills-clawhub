#!/usr/bin/env python3
"""
ç”µå•†çˆ¬è™«è„šæœ¬ v2 - æ”¯æŒç™»å½•å’ŒCookieç®¡ç†
"""
import argparse
import json
import time
import os
import sys
from datetime import datetime

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    print("âŒ éœ€è¦å®‰è£…Playwright: pip install playwright && playwright install chromium")
    sys.exit(1)


class EcommerceScraper:
    """ç”µå•†çˆ¬è™«ç±» - æ”¯æŒç™»å½•"""
    
    def __init__(self, headless=False):  # é»˜è®¤éæ— å¤´ï¼Œæ–¹ä¾¿æ‰«ç 
        self.headless = headless
        self.playwright = None
        self.browser = None
        self.context = None
        self.cookies_file = 'data/cookies.json'
    
    def __enter__(self):
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )
        # åŠ è½½ä¿å­˜çš„cookie
        self._load_cookies()
        return self
    
    def __exit__(self, *args):
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
    
    def _load_cookies(self):
        """åŠ è½½ä¿å­˜çš„cookie"""
        if os.path.exists(self.cookies_file):
            print(f"ğŸ“‚ åŠ è½½Cookie: {self.cookies_file}")
    
    def _save_cookies(self, cookies):
        """ä¿å­˜cookieåˆ°æ–‡ä»¶"""
        os.makedirs('data', exist_ok=True)
        with open(self.cookies_file, 'w') as f:
            json.dump(cookies, f)
        print(f"ğŸ’¾ Cookieå·²ä¿å­˜åˆ° {self.cookies_file}")
    
    def create_page(self, user_agent=None):
        """åˆ›å»ºæ–°é¡µé¢"""
        self.context = self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            locale='zh-CN',
            timezone_id='Asia/Shanghai',
            user_agent=user_agent or 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        # åŠ è½½cookie
        if os.path.exists(self.cookies_file):
            with open(self.cookies_file, 'r') as f:
                cookies = json.load(f)
                self.context.add_cookies(cookies)
                print("âœ… Cookieå·²åŠ è½½")
        
        page = self.context.new_page()
        
        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
            Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en-US', 'en']});
            window.chrome = {runtime: {}};
        """)
        
        return page
    
    def login_jd(self, page):
        """ç™»å½•äº¬ä¸œ"""
        print("ğŸ“± æ‰“å¼€äº¬ä¸œç™»å½•é¡µ...")
        page.goto('https://passport.jd.com/new/login.aspx', wait_until='networkidle', timeout=60000)
        
        print("âš ï¸ è¯·æ‰«ç ç™»å½•ï¼")
        print("â³ ç­‰å¾…ç™»å½•æˆåŠŸ...")
        
        # ç­‰å¾…ç™»å½•æˆåŠŸ (æ£€æµ‹ç”¨æˆ·åå‡ºç°)
        try:
            page.wait_for_selector('#username', timeout=120000)  # 2åˆ†é’Ÿç­‰å¾…æ‰«ç 
            # å¦‚æœæ‰¾åˆ°ç”¨æˆ·åå…ƒç´ ï¼Œè¯´æ˜å·²ç™»å½•
            print("âœ… ç™»å½•æˆåŠŸï¼")
            
            # ä¿å­˜cookie
            cookies = self.context.cookies()
            self._save_cookies(cookies)
            return True
        except:
            # ä¹Ÿå¯èƒ½æ˜¯ç›´æ¥è·³è½¬åˆ°é¦–é¡µäº†
            page.goto('https://www.jd.com/', wait_until='networkidle')
            if 'jd.com' in page.url:
                cookies = self.context.cookies()
                self._save_cookies(cookies)
                print("âœ… ç™»å½•æˆåŠŸï¼")
                return True
        
        print("âŒ ç™»å½•è¶…æ—¶")
        return False
    
    def login_taobao(self, page):
        """ç™»å½•æ·˜å®"""
        print("ğŸ“± æ‰“å¼€æ·˜å®ç™»å½•é¡µ...")
        page.goto('https://login.taobao.com/', wait_until='networkidle', timeout=60000)
        
        print("âš ï¸ è¯·æ‰«ç ç™»å½•ï¼")
        print("â³ ç­‰å¾…ç™»å½•æˆåŠŸ...")
        
        try:
            # ç­‰å¾…æ·˜å®ä¼šå‘˜åå‡ºç°
            page.wait_for_selector('.member-name, .site-nav-login', timeout=120000)
            print("âœ… ç™»å½•æˆåŠŸï¼")
            cookies = self.context.cookies()
            self._save_cookies(cookies)
            return True
        except:
            print("âŒ ç™»å½•è¶…æ—¶")
            return False
    
    def scrape_jd(self, keyword, max_pages=3):
        """çˆ¬å–äº¬ä¸œæœç´¢ç»“æœ"""
        page = self.create_page()
        
        # æ„é€ æœç´¢URL
        search_url = f"https://search.jd.com/Search?keyword={keyword}&enc=utf-8"
        print(f"ğŸ” æœç´¢: {keyword}")
        
        results = []
        
        for page_num in range(1, max_pages + 1):
            url = f"{search_url}&page={page_num*2-1}"  # JDé¡µé¢æ˜¯å¥‡æ•°
            print(f"ğŸ“„ çˆ¬å–ç¬¬ {page_num} é¡µ...")
            
            page.goto(url, wait_until='networkidle', timeout=60000)
            time.sleep(2)
            
            # ç­‰å¾…å•†å“åŠ è½½
            try:
                page.wait_for_selector('.gl-item', timeout=10000)
            except:
                print("âš ï¸ é¡µé¢å¯èƒ½éœ€è¦ç™»å½•")
                break
            
            # æå–å•†å“
            items = page.query_selector_all('.gl-item')
            print(f"   æ‰¾åˆ° {len(items)} ä¸ªå•†å“")
            
            for item in items[:30]:
                try:
                    # å°è¯•å¤šç§é€‰æ‹©å™¨
                    title_el = item.query_selector('.p-name em, .p-name a, .gl-name a')
                    price_el = item.query_selector('.p-price strong i, .price')
                    shop_el = item.query_selector('.p-shop a, .shop-name')
                    
                    product = {
                        'title': title_el.inner_text().strip() if title_el else '',
                        'price': price_el.inner_text().strip() if price_el else '',
                        'shop': shop_el.inner_text().strip() if shop_el else '',
                    }
                    
                    if product['title']:
                        results.append(product)
                except:
                    continue
            
            time.sleep(random.uniform(1, 3))
        
        return results
    
    def scrape_taobao(self, keyword, max_pages=3):
        """çˆ¬å–æ·˜å®æœç´¢ç»“æœ"""
        page = self.create_page()
        
        # æ·˜å®æœç´¢URL
        search_url = f"https://s.taobao.com/search?q={keyword}&imgfile=&initiative_id=staobaoz&ie=utf8"
        print(f"ğŸ” æœç´¢æ·˜å®: {keyword}")
        
        results = []
        
        for page_num in range(1, max_pages + 1):
            url = f"{search_url}&page={page_num}"
            print(f"ğŸ“„ çˆ¬å–ç¬¬ {page_num}/{max_pages} é¡µ...")
            
            page.goto(url, wait_until='networkidle', timeout=60000)
            time.sleep(3)
            
            # æ·˜å®å•†å“é€‰æ‹©å™¨ (å¤šç§å°è¯•)
            selectors = [
                '.item-wrap',
                '.item',
                '[data-category="auction"]',
                '.shop-hp-datalazy-item',
            ]
            
            items = []
            for sel in selectors:
                items = page.query_selector_all(sel)
                if len(items) > 5:
                    print(f"   ä½¿ç”¨é€‰æ‹©å™¨: {sel}, æ‰¾åˆ° {len(items)} ä¸ª")
                    break
            
            for item in items[:30]:
                try:
                    # æ·˜å®ç»“æ„å¤æ‚ï¼Œå¤šç§é€‰æ‹©å™¨å°è¯•
                    title = ''
                    for title_sel in ['.title', '.item-title', 'a.J_ClickStat', 'img']:
                        el = item.query_selector(title_sel)
                        if el:
                            if title_sel == 'img':
                                title = el.get_attribute('alt') or ''
                            else:
                                title = el.inner_text().strip()
                            if title:
                                break
                    
                    price = ''
                    for price_sel in ['.price', '.deal-price', '.real-price']:
                        el = item.query_selector(price_sel)
                        if el:
                            price = el.inner_text().strip()
                            if price:
                                break
                    
                    shop = ''
                    for shop_sel in ['.shop', '.shop-name', '.seller-nick']:
                        el = item.query_selector(shop_sel)
                        if el:
                            shop = el.inner_text().strip()
                            if shop:
                                break
                    
                    # è·å–é“¾æ¥
                    link = ''
                    link_el = item.query_selector('a.J_ClickStat')
                    if link_el:
                        link = link_el.get_attribute('href') or ''
                    
                    if title:
                        results.append({
                            'title': title,
                            'price': price,
                            'shop': shop,
                            'link': link
                        })
                except Exception as e:
                    continue
            
            # éšæœºå»¶è¿Ÿé˜²å°
            time.sleep(random.uniform(2, 4))
        
        return results


import random

def main():
    parser = argparse.ArgumentParser(description="ç”µå•†çˆ¬è™« - æ”¯æŒç™»å½•")
    subparsers = parser.add_subparsers(dest='command')
    
    # ç™»å½•å‘½ä»¤
    login_parser = subparsers.add_parser('login', help='ç™»å½•ç”µå•†å¹³å°')
    login_parser.add_argument('--platform', '-p', choices=['jd', 'taobao', 'pdd'], required=True)
    login_parser.add_argument('--headless', action='store_true', help='æ— å¤´æ¨¡å¼(ä¸æ¨è)')
    
    # çˆ¬å–å‘½ä»¤
    scrape_parser = subparsers.add_parser('scrape', help='çˆ¬å–å•†å“')
    scrape_parser.add_argument('--platform', '-p', choices=['jd', 'taobao', 'pdd'], required=True)
    scrape_parser.add_argument('--keyword', '-k', required=True, help='æœç´¢å…³é”®è¯')
    scrape_parser.add_argument('--max-pages', '-m', type=int, default=2, help='æœ€å¤§é¡µæ•°')
    scrape_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶')
    scrape_parser.add_argument('--headless', action='store_true', help='æ— å¤´æ¨¡å¼')
    
    args = parser.parse_args()
    
    if args.command == 'login':
        with EcommerceScraper(headless=getattr(args, 'headless', False)) as scraper:
            page = scraper.create_page()
            if args.platform == 'jd':
                scraper.login_jd(page)
            elif args.platform == 'taobao':
                scraper.login_taobao(page)
            elif args.platform == 'pdd':
                print("âš ï¸ æ‹¼å¤šå¤šç™»å½•éœ€è¦å¾®ä¿¡/QQæ‰«ç ")
                print("   è¯·æ‰‹åŠ¨ç™»å½•åæŒ‰Enter...")
                input()
    
    elif args.command == 'scrape':
        headless = getattr(args, 'headless', True)  # é»˜è®¤æ— å¤´
        
        with EcommerceScraper(headless=headless) as scraper:
            if args.platform == 'jd':
                results = scraper.scrape_jd(args.keyword, args.max_pages)
            elif args.platform == 'taobao':
                results = scraper.scrape_taobao(args.keyword, args.max_pages)
            elif args.platform == 'pdd':
                # æ‹¼å¤šå¤šéœ€è¦ç‰¹æ®Šå¤„ç†
                print("âš ï¸ æ‹¼å¤šå¤šå»ºè®®ä½¿ç”¨éæ— å¤´æ¨¡å¼")
                results = []
            
            print(f"\nğŸ“Š å…±è·å– {len(results)} ä¸ªå•†å“")
            
            # æ˜¾ç¤ºå‰10ä¸ª
            for i, p in enumerate(results[:10], 1):
                print(f"{i}. {p.get('title', '')[:40]}")
                print(f"   ğŸ’° {p.get('price', '')} | ğŸª {p.get('shop', '')}")
            
            # ä¿å­˜
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ’¾ å·²ä¿å­˜åˆ° {args.output}")
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
