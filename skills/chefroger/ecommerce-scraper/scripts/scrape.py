#!/usr/bin/env python3
"""
ç”µå•†çˆ¬è™«è„šæœ¬
æ”¯æŒ: JavaScriptæ¸²æŸ“ã€Cloudflareç»•è¿‡ã€éšè—APIå‘ç°ã€åˆ†é¡µçˆ¬å–
"""
import argparse
import json
import time
import random
import sys
from datetime import datetime

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    print("âŒ éœ€è¦å®‰è£…Playwright: pip install playwright && playwright install chromium")
    sys.exit(1)


SELECTORS = {
    'jd': {
        'product': '.gl-item',
        'title': '.p-name em',
        'price': '.p-price strong i',
        'shop': '.p-shop',
    },
    'taobao': {
        'product': '.item',
        'title': '.title',
        'price': '.price',
        'shop': '.shop',
    },
    'amazon': {
        'product': '[data-component-type="s-search-result"]',
        'title': 'h2 a span',
        'price': '.a-price-whole',
    },
    'generic': {
        'product': '[class*="product"], [class*="item"], li[class*="item"], div[class*="goods"]',
        'title': '[class*="title"], h2, h3, a[class*="title"]',
        'price': '[class*="price"], [class*="cost"], [class*="amount"], span[data-price]',
        'shop': '[class*="shop"], [class*="seller"], [class*="store"]',
        'link': 'a[href]',
        'image': 'img[src]',
    }
}


class EcommerceScraper:
    """ç”µå•†çˆ¬è™«ç±»"""
    
    def __init__(self, headless=True):
        self.headless = headless
        self.playwright = None
        self.browser = None
        self.context = None
    
    def __enter__(self):
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )
        return self
    
    def __exit__(self, *args):
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
    
    def create_page(self, user_agent=None):
        """åˆ›å»ºæ–°é¡µé¢"""
        self.context = self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            locale='zh-CN',
            timezone_id='Asia/Shanghai',
            user_agent=user_agent or 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        page = self.context.new_page()
        
        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
            Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en-US', 'en']});
            window.chrome = {runtime: {}};
        """)
        
        return page
    
    def detect_platform(self, url):
        """æ£€æµ‹ç”µå•†å¹³å°"""
        if 'jd.com' in url or 'jd.hk' in url:
            return 'jd'
        elif 'taobao.com' in url:
            return 'taobao'
        elif 'amazon.' in url:
            return 'amazon'
        return 'generic'
    
    def extract_products(self, page, platform='generic'):
        """æå–å•†å“æ•°æ®"""
        products = []
        selectors = SELECTORS.get(platform, SELECTORS['generic'])
        
        items = page.query_selector_all(selectors['product'])
        
        for item in items:
            try:
                product = {}
                
                # æ ‡é¢˜
                title_el = item.query_selector(selectors['title'])
                product['title'] = title_el.inner_text().strip() if title_el else ''
                
                # ä»·æ ¼
                price_el = item.query_selector(selectors['price'])
                product['price'] = price_el.inner_text().strip() if price_el else ''
                
                # åº—é“º
                if 'shop' in selectors:
                    shop_el = item.query_selector(selectors['shop'])
                    product['shop'] = shop_el.inner_text().strip() if shop_el else ''
                
                # é“¾æ¥
                link_el = item.query_selector(selectors.get('link', 'a'))
                if link_el:
                    href = link_el.get_attribute('href')
                    product['link'] = href if href else ''
                
                # å›¾ç‰‡
                img_el = item.query_selector(selectors.get('image', 'img'))
                if img_el:
                    src = img_el.get_attribute('src') or img_el.get_attribute('data-src') or ''
                    product['image'] = src
                
                if product.get('title'):
                    products.append(product)
                    
            except Exception as e:
                continue
        
        return products
    
    def has_next_page(self, page, page_num):
        """æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ"""
        selectors = [
            f'a:has-text("{page_num + 1}")',
            f'button:has-text("{page_num + 1}")',
            'a:has-text("ä¸‹ä¸€é¡µ")',
            'button:has-text("ä¸‹ä¸€é¡µ")',
            'a[class*="next"]',
            'button[class*="next"]',
        ]
        
        for sel in selectors:
            if page.query_selector(sel):
                return True
        return False
    
    def scrape(self, url, max_pages=1, wait_time=2):
        """çˆ¬å–é¡µé¢"""
        platform = self.detect_platform(url)
        all_products = []
        
        page = self.create_page()
        
        for page_num in range(1, max_pages + 1):
            if max_pages > 1:
                if '?' in url:
                    page_url = f"{url}&page={page_num}"
                else:
                    page_url = f"{url}?page={page_num}"
            else:
                page_url = url
            
            print(f"ğŸ“„ çˆ¬å–ç¬¬ {page_num}/{max_pages} é¡µ...")
            
            try:
                page.goto(page_url, wait_until="networkidle", timeout=30000)
                time.sleep(wait_time)
                
                products = self.extract_products(page, platform)
                print(f"   âœ… è·å– {len(products)} ä¸ªå•†å“")
                
                if not products:
                    break
                
                for p in products:
                    p['collected_at'] = datetime.now().isoformat()
                
                all_products.extend(products)
                
                if max_pages > 1 and not self.has_next_page(page, page_num):
                    print("   âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                    
            except Exception as e:
                print(f"   âŒ é”™è¯¯: {e}")
                break
        
        return all_products
    
    def discover_apis(self, url):
        """å‘ç°éšè—API"""
        api_endpoints = []
        page = self.create_page()
        
        def handle_response(response):
            url = response.url
            if 'api' in url.lower() or 'json' in url.lower() or 'data' in url.lower():
                if url not in api_endpoints and 'http' in url:
                    api_endpoints.append(url)
        
        page.on("response", handle_response)
        
        try:
            page.goto(url, wait_until="networkidle", timeout=30000)
            time.sleep(2)
        except Exception as e:
            print(f"é¡µé¢åŠ è½½å¤±è´¥: {e}")
        
        # æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        page.wait_for_timeout(1500)
        
        print(f"\nğŸ” å‘ç° {len(api_endpoints)} ä¸ªAPIç«¯ç‚¹:")
        for url in api_endpoints[:15]:
            print(f"   {url[:100]}")
        
        return api_endpoints


def scrape_page(url, headless=True, wait_time=2):
    """çˆ¬å–å•ä¸ªé¡µé¢"""
    with EcommerceScraper(headless=headless) as scraper:
        return scraper.scrape(url, max_pages=1, wait_time=wait_time)


def scrape_pagination(url, max_pages=5, headless=True, output=None):
    """åˆ†é¡µçˆ¬å–"""
    with EcommerceScraper(headless=headless) as scraper:
        products = scraper.scrape(url, max_pages=max_pages)
    
    if output:
        with open(output, 'w', encoding='utf-8') as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ° {output}")
    
    return products


def discover_api(url):
    """å‘ç°éšè—API"""
    with EcommerceScraper() as scraper:
        return scraper.discover_apis(url)


def main():
    parser = argparse.ArgumentParser(description="ç”µå•†çˆ¬è™«å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤')
    
    # scrape å‘½ä»¤
    scrape_parser = subparsers.add_parser('scrape', help='çˆ¬å–é¡µé¢')
    scrape_parser.add_argument('--url', '-u', required=True, help='ç›®æ ‡URL')
    scrape_parser.add_argument('--max-pages', '-m', type=int, default=1, help='æœ€å¤§é¡µæ•°')
    scrape_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶')
    scrape_parser.add_argument('--headless', action='store_true', default=True, help='æ— å¤´æ¨¡å¼')
    scrape_parser.add_argument('--visible', action='store_false', dest='headless', help='æ˜¾ç¤ºæµè§ˆå™¨')
    
    # api å‘½ä»¤
    api_parser = subparsers.add_parser('api', help='å‘ç°éšè—API')
    api_parser.add_argument('--url', '-u', required=True, help='ç›®æ ‡URL')
    
    args = parser.parse_args()
    
    if args.command == 'scrape':
        if args.max_pages == 1:
            products = scrape_page(args.url, headless=args.headless)
            print(f"\nâœ… è·å– {len(products)} ä¸ªå•†å“:")
            for p in products[:5]:
                print(f"   - {p.get('title', '')[:50]}: {p.get('price', '')}")
        else:
            products = scrape_pagination(args.url, max_pages=args.max_pages, 
                                         headless=args.headless, output=args.output)
            print(f"\nğŸ“Š æ€»å…±è·å– {len(products)} ä¸ªå•†å“")
    
    elif args.command == 'api':
        discover_api(args.url)
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
