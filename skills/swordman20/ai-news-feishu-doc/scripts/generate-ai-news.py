#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI æ—©æŠ¥ç”Ÿæˆå™¨
åŠŸèƒ½ï¼šRSSè®¢é˜… + æ™ºèƒ½æ‘˜è¦ + å¤šè¯­è¨€æ”¯æŒ + åª’ä½“é¢„è§ˆ
"""

import os
import sys
import re
import yaml
from datetime import datetime
from xml.etree import ElementTree as ET
import urllib.request
import urllib.error


def get_rss_feed(url):
    """è·å– RSS è®¢é˜…å†…å®¹"""
    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, timeout=30) as response:
            return response.read()
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è·å– RSS: {url} - {e}")
        return None


def extract_images(html_content, max_images=3):
    """ä» HTML æå–å›¾ç‰‡"""
    images = []
    pattern = r'<img[^>]+src="([^"]+)"[^>]*>'
    matches = re.findall(pattern, html_content)
    
    for img_url in matches[:max_images]:
        if not re.search(r'icon|logo|avatar|placeholder|1x1', img_url, re.I):
            images.append(img_url)
    
    return images


def get_smart_summary(content, max_length=200):
    """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
    # æ¸…ç† HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', content)
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&quot;', '"')
    text = text.replace('&amp;', '&')
    
    if len(text) > max_length:
        return text[:max_length] + "..."
    return text.strip()


def test_keyword_match(title, description, keywords):
    """å…³é”®è¯åŒ¹é…è¯„åˆ†"""
    text = f"{title} {description}".lower()
    score = 0
    for keyword in keywords:
        if keyword.lower() in text:
            score += 1
    return score


def parse_rss(xml_content):
    """è§£æ RSS XML"""
    articles = []
    try:
        root = ET.fromstring(xml_content)
        # å¤„ç† RSS 2.0
        if root.tag == 'rss':
            channel = root.find('channel')
            if channel is not None:
                for item in channel.findall('item'):
                    title = item.findtext('title', '')
                    link = item.findtext('link', '')
                    pub_date = item.findtext('pubDate', '')
                    desc = item.findtext('description', '')
                    articles.append({
                        'title': title,
                        'link': link,
                        'pubDate': pub_date,
                        'description': desc
                    })
        # å¤„ç† Atom
        elif 'feed' in root.tag:
            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                title = entry.findtext('{http://www.w3.org/2005/Atom}title', '')
                link = entry.find('{http://www.w3.org/2005/Atom}link')
                link_href = link.get('href', '') if link is not None else ''
                pub_date = entry.findtext('{http://www.w3.org/2005/Atom}updated', '')
                desc = entry.findtext('{http://www.w3.org/2005/Atom}summary', '')
                articles.append({
                    'title': title,
                    'link': link_href,
                    'pubDate': pub_date,
                    'description': desc
                })
    except Exception as e:
        print(f"è§£æ RSS å¤±è´¥: {e}")
    
    return articles


def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, '..', 'assets', 'ai-news-rss.yaml')
    
    today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    output_path = os.path.join(script_dir, f'AIæ—©æŠ¥_{today}.md')
    
    # è¯»å–é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print("ğŸ¦ AI æ—©æŠ¥ç”Ÿæˆå™¨å¯åŠ¨...\n")
    
    all_articles = []
    
    for feed in config.get('feeds', []):
        print(f"ğŸ“¡ æ­£åœ¨è·å–: {feed['name']} ...", end='')
        
        xml_content = get_rss_feed(feed['url'])
        if xml_content:
            articles = parse_rss(xml_content)
            max_per_feed = config.get('output', {}).get('max_articles_per_feed', 5)
            
            for article in articles[:max_per_feed]:
                # è§£ææ—¥æœŸ
                pub_date = article.get('pubDate', '')
                try:
                    # å°è¯•è§£æ RSS æ—¥æœŸæ ¼å¼
                    dt = datetime.strptime(pub_date[:16], '%a, %d %b %Y')
                    pub_date = dt.strftime('%m-%d')
                except:
                    pub_date = 'æœªçŸ¥æ—¶é—´'
                
                title = article.get('title', '')
                desc = article.get('description', '')
                
                # å…³é”®è¯åŒ¹é…
                keywords = config.get('filters', {}).get('keywords', [])
                score = test_keyword_match(title, desc, keywords)
                
                # æå–å›¾ç‰‡
                images = extract_images(desc, config.get('media', {}).get('max_images', 3))
                
                # ç”Ÿæˆæ‘˜è¦
                summary = get_smart_summary(desc, config.get('summary', {}).get('max_length', 200))
                
                all_articles.append({
                    'title': title,
                    'link': article.get('link', ''),
                    'pubDate': pub_date,
                    'summary': summary,
                    'images': images,
                    'source': feed['name'],
                    'category': feed.get('category', ''),
                    'language': feed.get('language', 'zh'),
                    'score': score
                })
            
            print(f" âœ… è·å– {len(articles[:max_per_feed])} ç¯‡")
        else:
            print(" âŒ å¤±è´¥")
    
    print(f"\nğŸ“ å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ï¼Œæ­£åœ¨ç”Ÿæˆæ—©æŠ¥...")
    
    # æ’åºå¹¶é€‰å– Top N
    all_articles.sort(key=lambda x: x['score'], reverse=True)
    total_max = config.get('output', {}).get('total_max_articles', 15)
    top_articles = all_articles[:total_max]
    
    # åˆ†ç±»
    cn_articles = [a for a in top_articles if a['language'] == 'zh']
    en_articles = [a for a in top_articles if a['language'] in ('en', 'mixed')]
    
    # ç”Ÿæˆ Markdown
    content = f"""# ğŸ“° AI æ—©æŠ¥ | {today}

> è‡ªåŠ¨é‡‡é›† {len(config.get('feeds', []))} ä¸ªä¿¡æº | ç²¾é€‰ {len(top_articles)} ç¯‡ | ç”± OpenClaw é©±åŠ¨

---

## ğŸ”¥ å¤´ç‰ˆå¤´æ¡

"""
    
    # å¤´ç‰ˆå¤´æ¡
    for article in top_articles[:3]:
        content += f"""### {article['title']}
ğŸ“ **{article['source']}** | ğŸ• {article['pubDate']}

ğŸ’¡ {article['summary']}

ğŸ”— [é˜…è¯»å…¨æ–‡]({article['link']})

"""
        if article['images']:
            for img in article['images']:
                content += f"![å›¾ç‰‡]({img})\n"
            content += "\n"
    
    # å›½å†… AI åŠ¨æ€
    if cn_articles:
        content += "---\n\n## ğŸ‡¨ğŸ‡³ å›½å†… AI åŠ¨æ€\n\n"
        for article in cn_articles[:5]:
            content += f"""### {article['title']}
**æ¥æºï¼š** {article['source']} | **æ—¶é—´ï¼š** {article['pubDate']}

{article['summary']}

ğŸ”— [é˜…è¯»åŸæ–‡]({article['link']})

"""
    
    # æµ·å¤– AI åŠ¨æ€
    if en_articles:
        content += "---\n\n## ğŸŒ æµ·å¤– AI åŠ¨æ€\n\n"
        for article in en_articles[:5]:
            content += f"""### {article['title']}
**Source:** {article['source']} | **Time:** {article['pubDate']}

{article['summary']}

ğŸ”— [Read Original]({article['link']})

"""
    
    # æ€»ç»“
    if top_articles:
        content += f"""---

## ğŸ¯ ä»Šæ—¥æ€»ç»“

> ä»Šæ—¥ AI åœˆæœ€å€¼å¾—å…³æ³¨çš„æ˜¯ï¼š**{top_articles[0]['title']}**

---

*ğŸ¦ ç”± å¤§é¾™è™¾ AI æ—©æŠ¥ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # ä¿å­˜æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"\nâœ… æ—©æŠ¥å·²ç”Ÿæˆ: {output_path}")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - å›½å†…æ–‡ç« : {len(cn_articles)} ç¯‡")
    print(f"   - æµ·å¤–æ–‡ç« : {len(en_articles)} ç¯‡")
    print(f"   - å¤´ç‰ˆå¤´æ¡: 3 ç¯‡")
    
    return output_path


if __name__ == '__main__':
    main()
