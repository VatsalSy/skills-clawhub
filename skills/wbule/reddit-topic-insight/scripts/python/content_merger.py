#!/usr/bin/env python3
"""
å†…å®¹åˆå¹¶è„šæœ¬

åŠŸèƒ½ï¼š
- è¯»å– pieces/angle-*.md æ–‡ä»¶
- æŒ‰è§’åº¦ç¼–å·æ’åºæ‹¼æ¥
- ç”Ÿæˆ content.md
- é™„åŠ æ¥æºå¸–å­è¡¨æ ¼ï¼ˆæ ‡é¢˜ã€URLã€scoreã€è¯„è®ºæ•°ï¼‰

è®¾è®¡åŸåˆ™ï¼š
- çº¯ç¡®å®šæ€§æ“ä½œï¼Œä¸ä½¿ç”¨ä»»ä½• AI ç»„ä»¶
- ä¿è¯è¾“å‡ºé¡ºåºå’Œæ ¼å¼ä¸€è‡´æ€§
- é¿å…å¯¹ SubAgent äº§å‡ºçš„å†…å®¹åšä»»ä½•ä¿®æ”¹

ç”¨æ³•ï¼š
    python3 content_merger.py \
        --pieces-dir <pieces ç›®å½•è·¯å¾„> \
        --posts-file <posts_detail.json è·¯å¾„> \
        --output <è¾“å‡ºæ–‡ä»¶è·¯å¾„>
"""

import argparse
import json
import logging
import os
import re
import sys
from datetime import datetime, timezone
from typing import Any

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


def load_json_file(file_path: str) -> dict[str, Any]:
    """åŠ è½½ JSON æ–‡ä»¶"""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


def discover_pieces(pieces_dir: str) -> list[tuple[int, str]]:
    """
    å‘ç°å¹¶æ’åº piece æ–‡ä»¶

    æ–‡ä»¶åæ ¼å¼ï¼šangle-{n}.mdï¼ˆn ä¸ºä¸¤ä½æ•°å­—ç¼–å·ï¼‰
    è¿”å›æŒ‰ç¼–å·æ’åºçš„ (ç¼–å·, æ–‡ä»¶è·¯å¾„) å…ƒç»„åˆ—è¡¨
    """
    pattern = re.compile(r"^angle-(\d+)\.md$")
    pieces: list[tuple[int, str]] = []

    if not os.path.isdir(pieces_dir):
        logger.error(f"pieces ç›®å½•ä¸å­˜åœ¨: {pieces_dir}")
        return pieces

    for filename in os.listdir(pieces_dir):
        match = pattern.match(filename)
        if match:
            angle_num = int(match.group(1))
            file_path = os.path.join(pieces_dir, filename)
            pieces.append((angle_num, file_path))

    # æŒ‰è§’åº¦ç¼–å·æ’åº
    pieces.sort(key=lambda x: x[0])
    return pieces


def read_piece_content(file_path: str) -> str:
    """è¯»å–å•ä¸ª piece æ–‡ä»¶å†…å®¹"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read().strip()
    except OSError as e:
        logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}: {e}")
        return ""


def build_source_table(posts_data: dict[str, Any]) -> str:
    """
    æ„å»ºæ¥æºå¸–å­ Markdown è¡¨æ ¼

    åŒ…å«ï¼šåºå·ã€æ ‡é¢˜ã€å­ç‰ˆå—ã€çƒ­åº¦åˆ†ã€è¯„è®ºæ•°ã€é“¾æ¥
    """
    posts = posts_data.get("posts", [])
    if not posts:
        return ""

    lines = [
        "## ğŸ“Š æ•°æ®æ¥æº",
        "",
        "| # | æ ‡é¢˜ | å­ç‰ˆå— | çƒ­åº¦åˆ† | è¯„è®ºæ•° | é“¾æ¥ |",
        "|---|------|--------|--------|--------|------|"
    ]

    for i, post in enumerate(posts, 1):
        title = post.get("title", "æ— æ ‡é¢˜")
        # æˆªæ–­æ ‡é¢˜é¿å…è¡¨æ ¼å˜å½¢
        if len(title) > 50:
            title = title[:47] + "..."
        # è½¬ä¹‰ Markdown è¡¨æ ¼ä¸­çš„ç®¡é“å­—ç¬¦
        title = title.replace("|", "\\|")

        subreddit = f"r/{post.get('subreddit', '?')}"
        heat_score = post.get("heat_score", 0)
        num_comments = post.get("num_comments", 0)
        url = post.get("url", "#")

        lines.append(
            f"| {i} | {title} | {subreddit} | {heat_score} | {num_comments} | [é“¾æ¥]({url}) |"
        )

    return "\n".join(lines)


def merge_content(
    pieces: list[tuple[int, str]],
    posts_data: dict[str, Any],
    topic: str
) -> str:
    """
    åˆå¹¶æ‰€æœ‰ piece æ–‡ä»¶ä¸ºæœ€ç»ˆçš„ content.md

    ç»“æ„ï¼š
    1. æŠ¥å‘Šå¤´éƒ¨ï¼ˆä¸»é¢˜ã€ç”Ÿæˆæ—¶é—´ã€ç»Ÿè®¡ä¿¡æ¯ï¼‰
    2. å„è§’åº¦å†…å®¹ï¼ˆæŒ‰ç¼–å·æ’åºï¼‰
    3. æ¥æºè¡¨æ ¼
    """
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")

    # æŠ¥å‘Šå¤´éƒ¨
    header = f"""# {topic} â€” Reddit ä¸»é¢˜æ´å¯ŸæŠ¥å‘Š

> ç”Ÿæˆæ—¶é—´ï¼š{timestamp}
> æ•°æ®æ¥æºï¼šReddit
> è¦†ç›–è§’åº¦ï¼š{len(pieces)} ä¸ª
> æ¶µç›–å¹³å°ï¼šX / å°çº¢ä¹¦ / å…¬ä¼—å·

---"""

    # æ‹¼æ¥å„è§’åº¦å†…å®¹
    sections = [header]
    for angle_num, file_path in pieces:
        content = read_piece_content(file_path)
        if content:
            sections.append(content)
            sections.append("\n---")
        else:
            logger.warning(f"è§’åº¦ {angle_num} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")

    # æ¥æºè¡¨æ ¼
    source_table = build_source_table(posts_data)
    if source_table:
        sections.append(source_table)

    return "\n\n".join(sections)


def main() -> None:
    parser = argparse.ArgumentParser(description="å†…å®¹åˆå¹¶è„šæœ¬")
    parser.add_argument(
        "--pieces-dir",
        required=True,
        help="pieces ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--posts-file",
        required=True,
        help="posts_detail.json è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    args = parser.parse_args()

    # å‘ç° piece æ–‡ä»¶
    pieces = discover_pieces(args.pieces_dir)
    if not pieces:
        logger.error("æœªæ‰¾åˆ°ä»»ä½• piece æ–‡ä»¶ï¼Œè¯·å…ˆå®Œæˆ Step 6ï¼ˆæˆå“ç”Ÿäº§ï¼‰")
        sys.exit(1)

    logger.info(f"å‘ç° {len(pieces)} ä¸ª piece æ–‡ä»¶:")
    for num, path in pieces:
        logger.info(f"  è§’åº¦ {num}: {os.path.basename(path)}")

    # åŠ è½½å¸–å­æ•°æ®ï¼ˆç”¨äºæ¥æºè¡¨æ ¼ï¼‰
    posts_data = load_json_file(args.posts_file)
    topic = posts_data.get("metadata", {}).get("topic", "æœªçŸ¥ä¸»é¢˜")

    # åˆå¹¶å†…å®¹
    final_content = merge_content(pieces, posts_data, topic)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # å†™å…¥æ–‡ä»¶
    with open(args.output, "w", encoding="utf-8") as f:
        f.write(final_content)

    logger.info(f"åˆå¹¶å®Œæˆï¼æœ€ç»ˆæ–‡ä»¶: {args.output}")
    logger.info(f"å…±åˆå¹¶ {len(pieces)} ä¸ªè§’åº¦ï¼Œæ€»è®¡ {len(final_content)} å­—ç¬¦")


if __name__ == "__main__":
    main()
