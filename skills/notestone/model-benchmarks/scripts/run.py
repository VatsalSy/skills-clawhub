#!/usr/bin/env python3
"""
Model Benchmarks â€” Global AI Intelligence Hub

Real-time AI model capability tracking via leaderboards (LMSYS Arena, HuggingFace, etc.) 
for intelligent compute routing and cost optimization.

Usage:
    python3 run.py fetch                           # Fetch latest benchmark data
    python3 run.py query --model gpt-4o           # Query model capabilities  
    python3 run.py recommend --task coding        # Get optimal model recommendations
    python3 run.py analyze                        # Cost efficiency analysis
    python3 run.py trends --model gpt-4o          # Performance trends over time
    
Examples:
    # Daily optimization workflow
    python3 run.py fetch && python3 run.py recommend --task coding
    
    # Find cost-efficient models
    python3 run.py analyze --sort-by efficiency --limit 5
    
    # Export data for external tools
    python3 run.py query --model claude-3.5-sonnet --format json
"""

import argparse
import json
import os
import sys
import urllib.request
import urllib.parse
import re
from datetime import datetime, timedelta
from pathlib import Path

# Skill directory
SKILL_DIR = Path(__file__).parent.parent
BENCHMARKS_DIR = SKILL_DIR / "benchmarks"
BENCHMARKS_DIR.mkdir(exist_ok=True)

# æ•°æ®æºé…ç½®
BENCHMARK_SOURCES = {
    "lmsys": {
        "name": "LMSYS Chatbot Arena",
        "url": "https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard",
        "api_url": "https://huggingface.co/api/spaces/lmsys/chatbot-arena-leaderboard/discussions",
        "capabilities": ["general", "reasoning", "creative"],
        "weight": 1.0,
    },
    "openllm": {
        "name": "Open LLM Leaderboard",
        "url": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",
        "capabilities": ["reasoning", "knowledge", "comprehension"],
        "weight": 0.8,
    },
    "bigcode": {
        "name": "BigCode Leaderboard",
        "url": "https://huggingface.co/spaces/bigcode/bigcode-leaderboard",
        "capabilities": ["coding"],
        "weight": 1.2,  # ç¼–ç¨‹èƒ½åŠ›æ›´é‡è¦
    },
    "alpaca": {
        "name": "Alpaca Eval",
        "url": "https://tatsu-lab.github.io/alpaca_eval/",
        "capabilities": ["instruction_following", "creative"],
        "weight": 0.9,
    }
}

# æ¨¡å‹åç§°æ˜ å°„ï¼ˆç»Ÿä¸€ä¸åŒå¹³å°çš„å‘½åï¼‰
MODEL_NAME_MAPPING = {
    "gpt-4o": ["gpt-4o", "gpt-4-o", "openai-gpt-4o"],
    "gpt-4o-mini": ["gpt-4o-mini", "gpt-4-o-mini", "openai-gpt-4o-mini"],
    "claude-3.5-sonnet": ["claude-3.5-sonnet", "claude-3-5-sonnet", "anthropic-claude-3.5-sonnet"],
    "gemini-2.0-flash": ["gemini-2.0-flash-001", "gemini-2-flash", "google-gemini-2.0-flash"],
}

# ä»»åŠ¡ç±»å‹ â†’ èƒ½åŠ›æ˜ å°„
TASK_CAPABILITY_MAP = {
    "coding": ["coding", "reasoning"],
    "writing": ["creative", "instruction_following"],
    "analysis": ["reasoning", "comprehension"],
    "translation": ["general", "knowledge"],
    "math": ["reasoning", "knowledge"],
    "creative": ["creative", "general"],
    "simple": ["general"],
}


def fetch_lmsys_arena():
    """æ‹‰å– LMSYS Arena æ’è¡Œæ¦œæ•°æ®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    print("ğŸŸï¸  Fetching LMSYS Chatbot Arena data...")
    
    # TODO: å®é™…å®ç°éœ€è¦è§£æ HuggingFace Space çš„æ•°æ®
    # è¿™é‡Œå…ˆæä¾›æ¨¡æ‹Ÿæ•°æ®
    mock_data = {
        "timestamp": datetime.now().isoformat(),
        "source": "lmsys",
        "models": {
            "gpt-4o": {
                "score": 1285,
                "rank": 2,
                "capabilities": {
                    "general": 92,
                    "reasoning": 89,
                    "creative": 88
                }
            },
            "claude-3.5-sonnet": {
                "score": 1322,
                "rank": 1,
                "capabilities": {
                    "general": 95,
                    "reasoning": 94,
                    "creative": 91
                }
            },
            "gpt-4o-mini": {
                "score": 1178,
                "rank": 8,
                "capabilities": {
                    "general": 85,
                    "reasoning": 82,
                    "creative": 78
                }
            },
            "gemini-2.0-flash": {
                "score": 1213,
                "rank": 6,
                "capabilities": {
                    "general": 88,
                    "reasoning": 85,
                    "creative": 83
                }
            }
        }
    }
    return mock_data


def fetch_bigcode_leaderboard():
    """æ‹‰å– BigCode ç¼–ç¨‹èƒ½åŠ›æ’è¡Œæ¦œ"""
    print("ğŸ’» Fetching BigCode Leaderboard data...")
    
    # æ¨¡æ‹Ÿç¼–ç¨‹èƒ½åŠ›æ•°æ®
    mock_data = {
        "timestamp": datetime.now().isoformat(),
        "source": "bigcode",
        "models": {
            "gpt-4o": {
                "humaneval": 88.4,
                "mbpp": 84.3,
                "capabilities": {"coding": 86}
            },
            "claude-3.5-sonnet": {
                "humaneval": 92.0,
                "mbpp": 87.8,
                "capabilities": {"coding": 90}
            },
            "gpt-4o-mini": {
                "humaneval": 75.2,
                "mbpp": 72.1,
                "capabilities": {"coding": 74}
            },
            "gemini-2.0-flash": {
                "humaneval": 79.6,
                "mbpp": 76.4,
                "capabilities": {"coding": 78}
            }
        }
    }
    return mock_data


def fetch_current_prices():
    """è·å–å½“å‰æ¨¡å‹ä»·æ ¼ï¼ˆä» OpenRouter ç­‰å¹³å°ï¼‰"""
    print("ğŸ’° Fetching current model prices...")
    
    # æ¨¡æ‹Ÿä»·æ ¼æ•°æ®ï¼ˆUSD per 1M tokensï¼‰
    mock_prices = {
        "gpt-4o": {"input": 2.50, "output": 10.00},
        "claude-3.5-sonnet": {"input": 3.00, "output": 15.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gemini-2.0-flash": {"input": 0.075, "output": 0.30},
    }
    return mock_prices


def normalize_model_name(name):
    """æ ‡å‡†åŒ–æ¨¡å‹åç§°"""
    name_lower = name.lower()
    for standard_name, aliases in MODEL_NAME_MAPPING.items():
        if any(alias.lower() in name_lower for alias in aliases):
            return standard_name
    return name


def merge_benchmark_data(all_data):
    """åˆå¹¶å¤šä¸ªæ¥æºçš„ benchmark æ•°æ®"""
    merged = {}
    
    for source_name, data in all_data.items():
        source_config = BENCHMARK_SOURCES.get(source_name, {})
        weight = source_config.get("weight", 1.0)
        
        for model_name, model_data in data.get("models", {}).items():
            normalized_name = normalize_model_name(model_name)
            
            if normalized_name not in merged:
                merged[normalized_name] = {
                    "capabilities": {},
                    "sources": [],
                    "last_updated": datetime.now().isoformat()
                }
            
            # åŠ æƒåˆå¹¶èƒ½åŠ›åˆ†æ•°
            for capability, score in model_data.get("capabilities", {}).items():
                if capability not in merged[normalized_name]["capabilities"]:
                    merged[normalized_name]["capabilities"][capability] = []
                
                merged[normalized_name]["capabilities"][capability].append({
                    "score": score,
                    "weight": weight,
                    "source": source_name
                })
            
            merged[normalized_name]["sources"].append(source_name)
    
    # è®¡ç®—åŠ æƒå¹³å‡åˆ†
    for model_name, model_data in merged.items():
        for capability, scores in model_data["capabilities"].items():
            if scores:
                weighted_sum = sum(s["score"] * s["weight"] for s in scores)
                total_weight = sum(s["weight"] for s in scores)
                model_data["capabilities"][capability] = round(weighted_sum / total_weight, 1)
    
    return merged


def calculate_cost_efficiency(capabilities, prices):
    """è®¡ç®—æ€§ä»·æ¯”åˆ†æ•°"""
    cost_efficiency = {}
    
    for model, caps in capabilities.items():
        price_data = prices.get(model, {})
        if not price_data:
            continue
            
        # ç®€å•æˆæœ¬è®¡ç®—ï¼šå‡è®¾å¹³å‡è¾“å…¥è¾“å‡ºæ¯”ä¾‹
        avg_price = (price_data["input"] + price_data["output"]) / 2
        
        # ç»¼åˆèƒ½åŠ›åˆ†æ•°ï¼ˆå„é¡¹èƒ½åŠ›åŠ æƒå¹³å‡ï¼‰
        capability_scores = list(caps.get("capabilities", {}).values())
        if capability_scores:
            avg_capability = sum(capability_scores) / len(capability_scores)
            # æ€§ä»·æ¯” = èƒ½åŠ›åˆ†æ•° / æˆæœ¬
            efficiency = avg_capability / avg_price if avg_price > 0 else 0
            cost_efficiency[model] = round(efficiency, 2)
    
    return cost_efficiency


def fetch_all_benchmarks():
    """æ‹‰å–æ‰€æœ‰ benchmark æ•°æ®"""
    all_data = {}
    
    # æ‹‰å–å„ä¸ªæ¥æºçš„æ•°æ®
    all_data["lmsys"] = fetch_lmsys_arena()
    all_data["bigcode"] = fetch_bigcode_leaderboard()
    
    # è·å–ä»·æ ¼æ•°æ®
    prices = fetch_current_prices()
    
    # åˆå¹¶æ•°æ®
    merged_capabilities = merge_benchmark_data(all_data)
    
    # è®¡ç®—æ€§ä»·æ¯”
    cost_efficiency = calculate_cost_efficiency(merged_capabilities, prices)
    
    # ç»„è£…æœ€ç»ˆæ•°æ®
    final_data = {
        "timestamp": datetime.now().isoformat(),
        "models": merged_capabilities,
        "prices": prices,
        "cost_efficiency": cost_efficiency,
        "sources": list(BENCHMARK_SOURCES.keys()),
        "version": "1.0"
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    today = datetime.now().strftime("%Y-%m-%d")
    output_file = BENCHMARKS_DIR / f"{today}.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(final_data, f, indent=2, ensure_ascii=False)
    
    # ä¹Ÿä¿å­˜ä¸ºæœ€æ–°æ•°æ®
    latest_file = BENCHMARKS_DIR / "latest.json"
    with open(latest_file, "w", encoding="utf-8") as f:
        json.dump(final_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Benchmark data saved to {output_file}")
    return final_data


def query_model(model_name):
    """æŸ¥è¯¢ç‰¹å®šæ¨¡å‹çš„èƒ½åŠ›æ•°æ®"""
    latest_file = BENCHMARKS_DIR / "latest.json"
    
    if not latest_file.exists():
        print("âŒ No benchmark data found. Run 'fetch' first.")
        return None
    
    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    normalized_name = normalize_model_name(model_name)
    model_data = data["models"].get(normalized_name)
    
    if not model_data:
        print(f"âŒ Model '{model_name}' not found in benchmark data.")
        available = list(data["models"].keys())
        print(f"Available models: {', '.join(available)}")
        return None
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    print(f"\nğŸ¤– Model: {normalized_name}")
    print(f"ğŸ“Š Capabilities:")
    
    for capability, score in model_data["capabilities"].items():
        print(f"  â€¢ {capability.capitalize()}: {score}/100")
    
    prices = data["prices"].get(normalized_name, {})
    if prices:
        print(f"ğŸ’° Pricing (per 1M tokens):")
        print(f"  â€¢ Input: ${prices['input']:.2f}")
        print(f"  â€¢ Output: ${prices['output']:.2f}")
    
    efficiency = data["cost_efficiency"].get(normalized_name)
    if efficiency:
        print(f"âš¡ Cost Efficiency: {efficiency}")
    
    return model_data


def recommend_for_task(task_type):
    """ä¸ºç‰¹å®šä»»åŠ¡æ¨èæœ€ä½³æ¨¡å‹"""
    latest_file = BENCHMARKS_DIR / "latest.json"
    
    if not latest_file.exists():
        print("âŒ No benchmark data found. Run 'fetch' first.")
        return None
    
    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # è·å–ä»»åŠ¡éœ€è¦çš„èƒ½åŠ›
    required_capabilities = TASK_CAPABILITY_MAP.get(task_type.lower(), ["general"])
    
    print(f"\nğŸ¯ Task: {task_type}")
    print(f"ğŸ“‹ Required capabilities: {', '.join(required_capabilities)}")
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹è®¡ç®—ä»»åŠ¡åŒ¹é…åº¦
    recommendations = []
    
    for model_name, model_data in data["models"].items():
        capabilities = model_data["capabilities"]
        
        # è®¡ç®—ç›¸å…³èƒ½åŠ›çš„å¹³å‡åˆ†
        relevant_scores = []
        for cap in required_capabilities:
            if cap in capabilities:
                relevant_scores.append(capabilities[cap])
        
        if not relevant_scores:
            continue
            
        avg_score = sum(relevant_scores) / len(relevant_scores)
        
        # è·å–æˆæœ¬å’Œæ•ˆç‡æ•°æ®
        efficiency = data["cost_efficiency"].get(model_name, 0)
        prices = data["prices"].get(model_name, {})
        avg_price = (prices.get("input", 0) + prices.get("output", 0)) / 2
        
        recommendations.append({
            "model": model_name,
            "task_score": avg_score,
            "cost_efficiency": efficiency,
            "avg_price": avg_price,
            "relevant_capabilities": {cap: capabilities.get(cap, 0) for cap in required_capabilities}
        })
    
    # æ’åºï¼šæŒ‰ä»»åŠ¡é€‚åˆåº¦ + æ€§ä»·æ¯”
    recommendations.sort(key=lambda x: (x["task_score"] * 0.7 + x["cost_efficiency"] * 0.3), reverse=True)
    
    print(f"\nğŸ† Top 3 recommendations:")
    for i, rec in enumerate(recommendations[:3], 1):
        print(f"{i}. {rec['model']}")
        print(f"   Task Score: {rec['task_score']:.1f}/100")
        print(f"   Cost Efficiency: {rec['cost_efficiency']:.2f}")
        print(f"   Avg Price: ${rec['avg_price']:.2f}/1M tokens")
        print()
    
    return recommendations


def sync_to_compute_router():
    """åŒæ­¥æ•°æ®åˆ° compute-router"""
    latest_file = BENCHMARKS_DIR / "latest.json"
    router_dir = Path("~/.openclaw/workspace/skills/compute-router").expanduser()
    
    if not latest_file.exists():
        print("âŒ No benchmark data found. Run 'fetch' first.")
        return
    
    if not router_dir.exists():
        print("âŒ compute-router skill not found.")
        return
    
    with open(latest_file, "r", encoding="utf-8") as f:
        benchmark_data = json.load(f)
    
    # ç”Ÿæˆ router é…ç½®
    router_config = {
        "model_tiers": {},
        "dynamic_pricing": benchmark_data["prices"],
        "task_recommendations": {},
        "last_updated": benchmark_data["timestamp"]
    }
    
    # æ ¹æ®æ€§ä»·æ¯”é‡æ–°åˆ†å±‚
    efficiency_sorted = sorted(
        benchmark_data["cost_efficiency"].items(),
        key=lambda x: x[1],
        reverse=True
    )
    
    # åŠ¨æ€åˆ†é…åˆ° 3 ä¸ªtier
    total_models = len(efficiency_sorted)
    for i, (model, efficiency) in enumerate(efficiency_sorted):
        if i < total_models // 3:
            tier = "heavy"
        elif i < 2 * total_models // 3:
            tier = "medium" 
        else:
            tier = "light"
        
        router_config["model_tiers"][model] = {
            "tier": tier,
            "efficiency": efficiency,
            "capabilities": benchmark_data["models"][model]["capabilities"]
        }
    
    # ç”Ÿæˆä»»åŠ¡æ¨èæ˜ å°„
    for task_type in TASK_CAPABILITY_MAP.keys():
        # å¿«é€Ÿæ¨èé€»è¾‘ï¼ˆå– top 1ï¼‰
        recs = []
        for model, model_data in benchmark_data["models"].items():
            required_caps = TASK_CAPABILITY_MAP[task_type]
            scores = [model_data["capabilities"].get(cap, 0) for cap in required_caps]
            if scores:
                avg_score = sum(scores) / len(scores)
                efficiency = benchmark_data["cost_efficiency"].get(model, 0)
                combined_score = avg_score * 0.7 + efficiency * 0.3
                recs.append((model, combined_score))
        
        if recs:
            recs.sort(key=lambda x: x[1], reverse=True)
            router_config["task_recommendations"][task_type] = recs[0][0]
    
    # ä¿å­˜åˆ° router ç›®å½•
    router_config_file = router_dir / "dynamic_config.json"
    with open(router_config_file, "w", encoding="utf-8") as f:
        json.dump(router_config, f, indent=2)
    
    print(f"âœ… Synced to compute-router: {router_config_file}")
    print(f"ğŸ“Š Updated {len(router_config['model_tiers'])} models")
    print(f"ğŸ¯ Generated {len(router_config['task_recommendations'])} task recommendations")


def analyze_cost_efficiency():
    """Analyze cost efficiency across all models"""
    latest_file = BENCHMARKS_DIR / "latest.json"
    
    if not latest_file.exists():
        print("âŒ No benchmark data found. Run 'fetch' first.")
        return
    
    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print("ğŸ’° Cost Efficiency Analysis")
    print("=" * 50)
    
    # Sort models by cost efficiency
    efficiency_data = []
    for model, efficiency in data["cost_efficiency"].items():
        model_info = data["models"].get(model, {})
        prices = data["prices"].get(model, {})
        avg_capability = sum(model_info.get("capabilities", {}).values()) / len(model_info.get("capabilities", {1}))
        
        efficiency_data.append({
            "model": model,
            "efficiency": efficiency,
            "avg_capability": avg_capability,
            "avg_price": (prices.get("input", 0) + prices.get("output", 0)) / 2
        })
    
    efficiency_data.sort(key=lambda x: x["efficiency"], reverse=True)
    
    print("ğŸ† Top 10 Most Cost-Efficient Models:")
    print(f"{'Model':<25} {'Efficiency':<12} {'Capability':<12} {'Price/1M':<12}")
    print("-" * 65)
    
    for model in efficiency_data[:10]:
        print(f"{model['model']:<25} {model['efficiency']:<12.2f} {model['avg_capability']:<12.1f} ${model['avg_price']:<11.2f}")


def export_json_data(model_name=None):
    """Export data in JSON format for programmatic use"""
    latest_file = BENCHMARKS_DIR / "latest.json"
    
    if not latest_file.exists():
        print("âŒ No benchmark data found. Run 'fetch' first.")
        return
    
    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    if model_name:
        normalized_name = normalize_model_name(model_name)
        model_data = data["models"].get(normalized_name)
        if model_data:
            output = {
                "model": normalized_name,
                "capabilities": model_data["capabilities"],
                "pricing": data["prices"].get(normalized_name, {}),
                "cost_efficiency": data["cost_efficiency"].get(normalized_name, 0),
                "timestamp": data["timestamp"]
            }
        else:
            output = {"error": f"Model '{model_name}' not found"}
    else:
        output = data
    
    print(json.dumps(output, indent=2))


def main():
    parser = argparse.ArgumentParser(description="AI Model Benchmarks Intelligence")
    subparsers = parser.add_subparsers(dest="command", help="Commands")
    
    # fetch å‘½ä»¤
    fetch_parser = subparsers.add_parser("fetch", help="Fetch latest benchmark data")
    fetch_parser.add_argument("--source", choices=["all", "lmsys", "bigcode", "openllm"], 
                            default="all", help="Data source to fetch")
    
    # query å‘½ä»¤
    query_parser = subparsers.add_parser("query", help="Query model capabilities")
    query_parser.add_argument("--model", required=True, help="Model name to query")
    query_parser.add_argument("--format", choices=["text", "json"], default="text", help="Output format")
    
    # recommend å‘½ä»¤
    rec_parser = subparsers.add_parser("recommend", help="Recommend optimal models for task")
    rec_parser.add_argument("--task", required=True, 
                          choices=list(TASK_CAPABILITY_MAP.keys()),
                          help="Task type")
    rec_parser.add_argument("--format", choices=["text", "json"], default="text", help="Output format")
    rec_parser.add_argument("--limit", type=int, default=3, help="Number of recommendations")
    
    # analyze å‘½ä»¤
    analyze_parser = subparsers.add_parser("analyze", help="Cost efficiency analysis")
    analyze_parser.add_argument("--sort-by", choices=["efficiency", "capability", "price"], 
                               default="efficiency", help="Sort criteria")
    analyze_parser.add_argument("--limit", type=int, default=10, help="Number of results")
    
    # export å‘½ä»¤
    export_parser = subparsers.add_parser("export", help="Export data in JSON format")
    export_parser.add_argument("--model", help="Specific model to export (optional)")
    
    args = parser.parse_args()
    
    if args.command == "fetch":
        fetch_all_benchmarks()
    elif args.command == "query":
        if args.format == "json":
            export_json_data(args.model)
        else:
            query_model(args.model)
    elif args.command == "recommend":
        recommend_for_task(args.task)
    elif args.command == "analyze":
        analyze_cost_efficiency()
    elif args.command == "export":
        export_json_data(args.model)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()