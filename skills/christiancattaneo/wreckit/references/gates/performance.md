# Gate: Performance Benchmarks

**Question:** Is the code fast enough, and did it get slower?

AI code often works on toy inputs but tanks at scale. This gate catches:
- O(nÂ²) algorithms hiding in "working" code
- Functions that take 10ms at 100 items but 60 seconds at 10,000
- Memory blowups that look fine until production load hits
- Regressions: code that worked fast last week is now 3x slower

Run `scripts/perf-benchmark.sh [path]` first.

---

## Step-by-Step Process

### 1. Find or Create Benchmarks

Look for:
- **Vitest bench** â€” `bench()` calls in test files
- **pytest-benchmark** â€” `benchmark.pedantic()` or `benchmark()` in pytest
- **cargo bench** â€” `#[bench]` functions
- **go test -bench** â€” `func BenchmarkX(b *testing.B)` functions
- **k6** â€” `k6.js` or `script.js` load test files

If none exist: write benchmarks for the 3 most critical/expensive functions.

### 2. Run Benchmarks

Run benchmarks with enough iterations to be statistically meaningful:
- Vitest bench: `vitest bench`
- pytest-benchmark: `pytest --benchmark-only`
- cargo bench: `cargo bench`
- go bench: `go test -bench=. -benchmem -count=3`
- k6: `k6 run script.js`

### 3. Compare vs Baseline

If `.wreckit/perf-baseline.json` exists from a previous run:
- Compare each benchmark's mean time to baseline
- Flag regressions > 20% slower
- Flag improvements > 20% faster (document, may want to update baseline)

If no baseline exists: write current results as the new baseline.

### 4. Complexity Analysis

For core algorithmic functions, estimate time complexity:
- Run at N=10, 100, 1000, 10000 inputs
- Calculate growth ratio: T(10N)/T(N)
  - ~1.0x â†’ O(1) or O(log n) â€” excellent
  - ~2-3x â†’ O(n) â€” expected
  - ~10x â†’ O(n log n) â€” acceptable for sort operations
  - ~100x â†’ O(nÂ²) â€” likely too slow for production at scale
  - ~1000x â†’ O(nÂ³) or worse â€” almost certainly broken

### 5. Memory Scaling

Run at increasing input sizes and track peak memory:
- Watch for linear memory growth (normal)
- Flag superlinear memory growth (possible bug)

---

## Pass/Fail Criteria

| Condition | Verdict |
|-----------|---------|
| All benchmarks within 20% of baseline | Pass âœ… |
| No baseline â€” first run | Pass âœ… (baseline created) |
| Any benchmark >20% slower than baseline | Caution âš ï¸ |
| Any benchmark >50% slower than baseline | Blocked ğŸš« |
| O(nÂ²) or worse detected for N>1000 operation | Blocked ğŸš« |
| Memory grows superlinearly | Caution âš ï¸ |
| No benchmarks and no benchmark framework | Warn âš ï¸ |

---

## Script Output

`scripts/perf-benchmark.sh [path]` outputs:
```json
{
  "framework": "vitest-bench|pytest-benchmark|cargo-bench|go-bench|none",
  "benchmarks": [
    {
      "name": "sort 1000 items",
      "mean_ms": 1.2,
      "baseline_ms": 1.0,
      "delta_pct": 20.0,
      "regression": false
    }
  ],
  "regressions": [],
  "baseline_created": true,
  "baseline_path": ".wreckit/perf-baseline.json",
  "verdict": "PASS|WARN|FAIL"
}
```
