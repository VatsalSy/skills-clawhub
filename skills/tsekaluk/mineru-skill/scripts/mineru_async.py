#!/usr/bin/env python3
"""
MinerU PDF Parser - é«˜æ€§èƒ½å¼‚æ­¥å¹¶å‘ç‰ˆæœ¬

Optimizations:
- asyncio + aiohttp: å•çº¿ç¨‹å¼‚æ­¥ï¼Œæ—  GIL å¼€é”€
- è¿æ¥æ± å¤ç”¨: å‡å°‘ TCP æ¡æ‰‹
- ä¿¡å·é‡æ§åˆ¶: ç²¾ç¡®å¹¶å‘æ•°
- è‡ªåŠ¨é‡è¯•: å¤±è´¥è‡ªåŠ¨é‡è¯• 3 æ¬¡
"""

import argparse
import asyncio
import os
import sys
import time
import zipfile
from pathlib import Path
from typing import Optional, Tuple

import aiohttp

API_BASE = "https://mineru.net/api/v4"

# å¹¶å‘æ§åˆ¶
MAX_CONCURRENT = 10
MAX_RETRIES = 3


class MinerUClient:
    """MinerU API å¼‚æ­¥å®¢æˆ·ç«¯"""
    
    def __init__(self, token: str, session: aiohttp.ClientSession):
        self.token = token
        self.session = session
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    
    def _headers(self) -> dict:
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.token}",
        }
    
    async def create_batch_upload(self, filename: str, data_id: str) -> Tuple[str, str]:
        """è·å–ä¸Šä¼ é“¾æ¥"""
        async with self.session.post(
            f"{API_BASE}/file-urls/batch",
            headers=self._headers(),
            json={
                "files": [{"name": filename, "data_id": data_id}],
                "model_version": "vlm",
                "enable_formula": True,
                "enable_table": True,
            },
        ) as resp:
            result = await resp.json()
            if result.get("code") != 0:
                raise Exception(f"API error: {result.get('msg')}")
            data = result["data"]
            return data["batch_id"], data["file_urls"][0]
    
    async def upload_file(self, upload_url: str, file_path: Path) -> bool:
        """ä¸Šä¼ æ–‡ä»¶"""
        async with self.session.put(
            upload_url,
            data=file_path.read_bytes(),
        ) as resp:
            return resp.status == 200
    
    async def wait_for_result(self, batch_id: str, timeout: int = 600) -> Optional[str]:
        """ç­‰å¾…è§£æå®Œæˆï¼Œè¿”å›ä¸‹è½½é“¾æ¥"""
        start = time.time()
        
        while time.time() - start < timeout:
            async with self.session.get(
                f"{API_BASE}/extract-results/batch/{batch_id}",
                headers=self._headers(),
            ) as resp:
                result = await resp.json()
                results = result["data"]["extract_result"]
                
                if not results:
                    await asyncio.sleep(5)
                    continue
                
                state = results[0].get("state")
                
                if state == "done":
                    return results[0].get("full_zip_url")
                elif state == "failed":
                    raise Exception(results[0].get("err_msg", "è§£æå¤±è´¥"))
                
                await asyncio.sleep(5)
        
        raise TimeoutError("ç­‰å¾…è¶…æ—¶")
    
    async def download_and_extract(self, zip_url: str, output_dir: Path, filename: str) -> Path:
        """ä¸‹è½½å¹¶è§£å‹"""
        zip_path = output_dir / f"{filename}.zip"
        
        async with self.session.get(zip_url) as resp:
            zip_path.write_bytes(await resp.read())
        
        extract_dir = output_dir / filename
        with zipfile.ZipFile(zip_path) as zf:
            zf.extractall(extract_dir)
        
        zip_path.unlink()
        
        # é‡å‘½å
        md_file = extract_dir / "full.md"
        if md_file.exists():
            md_file.rename(extract_dir / f"{filename}.md")
        
        return extract_dir
    
    async def process_file(
        self,
        file_path: Path,
        output_dir: Path,
        index: int,
        total: int,
    ) -> Tuple[bool, str]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¸¦é‡è¯•ï¼‰"""
        filename = file_path.name
        stem = file_path.stem
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if (output_dir / stem).exists():
            print(f"  [{index+1}/{total}] â­ï¸  {stem}")
            return True, stem
        
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘
            for attempt in range(MAX_RETRIES):
                try:
                    print(f"  [{index+1}/{total}] {'ğŸ”„' if attempt > 0 else 'ğŸ“¤'} {stem}")
                    
                    # 1. è·å–ä¸Šä¼ é“¾æ¥
                    batch_id, upload_url = await self.create_batch_upload(filename, stem)
                    
                    # 2. ä¸Šä¼ 
                    if not await self.upload_file(upload_url, file_path):
                        raise Exception("ä¸Šä¼ å¤±è´¥")
                    
                    # 3. ç­‰å¾…è§£æ
                    zip_url = await self.wait_for_result(batch_id)
                    
                    # 4. ä¸‹è½½è§£å‹
                    await self.download_and_extract(zip_url, output_dir, stem)
                    
                    print(f"  [{index+1}/{total}] âœ… {stem}")
                    return True, stem
                    
                except Exception as e:
                    if attempt < MAX_RETRIES - 1:
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue
                    print(f"  [{index+1}/{total}] âŒ {stem}: {e}")
                    return False, stem
        
        return False, stem


async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    token = args.token or os.environ.get("MINERU_TOKEN")
    if not token:
        print("âŒ è¯·è®¾ç½® MINERU_TOKEN")
        sys.exit(1)
    
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ–‡ä»¶
    input_dir = Path(args.dir)
    pdf_files = sorted(list(input_dir.glob("*.pdf")) + list(input_dir.glob("*.PDF")))
    
    if not pdf_files:
        print("âŒ æœªæ‰¾åˆ° PDF æ–‡ä»¶")
        sys.exit(1)
    
    # è¿‡æ»¤å·²å¤„ç†çš„
    if args.resume:
        original = len(pdf_files)
        pdf_files = [f for f in pdf_files if not (output_dir / f.stem).exists()]
        if skipped := original - len(pdf_files):
            print(f"â­ï¸  è·³è¿‡å·²å¤„ç†: {skipped} ä¸ª")
    
    if not pdf_files:
        print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ!")
        return
    
    total = len(pdf_files)
    print(f"\nğŸ“š å¼€å§‹å¤„ç† {total} ä¸ªæ–‡ä»¶ (å¼‚æ­¥å¹¶å‘: {MAX_CONCURRENT})")
    print(f"ğŸ“ è¾“å‡ºåˆ°: {output_dir}\n")
    
    start_time = time.time()
    
    # åˆ›å»º aiohttp sessionï¼ˆè¿æ¥æ± å¤ç”¨ï¼‰
    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT * 2, force_close=False)
    timeout = aiohttp.ClientTimeout(total=3600)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        client = MinerUClient(token, session)
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡ä»¶
        tasks = [
            client.process_file(f, output_dir, i, total)
            for i, f in enumerate(pdf_files)
        ]
        results = await asyncio.gather(*tasks)
    
    # ç»Ÿè®¡
    success = sum(1 for ok, _ in results if ok)
    failed = sum(1 for ok, _ in results if not ok)
    failed_files = [name for ok, name in results if not ok]
    
    elapsed = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"âœ… æˆåŠŸ: {success}")
    print(f"âŒ å¤±è´¥: {failed}")
    print(f"â±ï¸  è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸš€ é€Ÿåº¦: {total/elapsed*60:.1f} æ–‡ä»¶/åˆ†é’Ÿ")
    
    if failed_files:
        print(f"\nå¤±è´¥æ–‡ä»¶:")
        for f in failed_files:
            print(f"  - {f}")
    
    print(f"\nğŸ“ ç»“æœ: {output_dir}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dir", required=True)
    parser.add_argument("--output", required=True)
    parser.add_argument("--token")
    parser.add_argument("--workers", "-w", type=int, default=10, help="å¹¶å‘æ•°")
    parser.add_argument("--resume", action="store_true")
    
    args = parser.parse_args()
    
    global MAX_CONCURRENT
    MAX_CONCURRENT = args.workers
    
    asyncio.run(main_async(args))


if __name__ == "__main__":
    main()
