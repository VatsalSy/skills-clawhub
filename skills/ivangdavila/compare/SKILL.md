---
name: "Compare"
description: "Rigorous comparisons with confidence parity, weighted criteria, and research depth tracking."
---

## Core Principle

Comparisons fail when confidence is uneven. Only as reliable as the weakest-researched dimension.

## Protocol

```
Criteria â†’ Research Parity â†’ Confidence Check â†’ Score â†’ Present
```

### 1. Criteria

- Load domain defaults (`domains.md`)
- Overlay user preferences from memory
- If unknown: "What matters most here?"
- Output: Ranked criteria with weights (sum = 100%)

### 2. Research Parity (Critical)

**Research each item to equivalent depth before scoring.**

Track: `| Criterion | Item A sources | Item B sources |`

5 reviews for A but 1 for B? Research more for B first. Never score unbalanced data.

### 3. Confidence Check

Verify before presenting:
- Each item researched equally
- Each criterion researched equally
- Source quality comparable
- Data recency comparable

Fail any? Research more OR caveat explicitly.

### 4. Score

`Final = Î£(criterion_score Ã— weight)` â€” Show the math.

### 5. Present

```
ðŸ†š [A] vs [B]
ðŸ“Š CRITERIA: [ranked by weight]
ðŸ“ˆ SCORES: [table + confidence per row]
ðŸŽ¯ RESULT: [Winner] by [margin]
âš ï¸ CAVEATS: [imbalances]
ðŸ’¡ IF [X] MATTERS MORE: [alt winner]
```

## After

Note which criteria user focused on. Update `preferences.md` by category.

## Decline When

Research parity impossible, priorities unclear, or time insufficient. Partial > misleading.

References: `domains.md`, `confidence.md`, `traps.md`, `preferences.md`
