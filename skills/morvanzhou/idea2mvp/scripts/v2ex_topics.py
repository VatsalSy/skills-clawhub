#!/usr/bin/env python3
"""
V2EX äº§å“/å·¥å…·è¯é¢˜æœç´¢

ä» V2EX çš„äº§å“ç›¸å…³èŠ‚ç‚¹ï¼ˆåˆ†äº«åˆ›é€ ã€åˆ†äº«å‘ç°ç­‰ï¼‰è·å–è¯é¢˜ï¼Œ
å¹¶é€šè¿‡å…³é”®è¯è¿‡æ»¤ç­›é€‰å‡ºä¸å·¥å…·ã€äº§å“ã€ç‹¬ç«‹å¼€å‘ç›¸å…³çš„å†…å®¹ã€‚

æ— éœ€è®¤è¯ï¼Œæ— éœ€ Tokenã€‚

Usage:
    python3 v2ex_topics.py
    python3 v2ex_topics.py --nodes create share
    python3 v2ex_topics.py --filter "AIå·¥å…·"
    python3 v2ex_topics.py --no-filter
"""

import argparse
import json
import os
import sys
import urllib.request
import urllib.error
from datetime import datetime

TMP_DIR = os.path.join(os.getcwd(), "tmp")
RESULT_FILE = os.path.join(TMP_DIR, "v2ex_results.txt")

# äº§å“/å·¥å…·ç›¸å…³çš„æ ¸å¿ƒèŠ‚ç‚¹
PRODUCT_NODES = {
    "create": "åˆ†äº«åˆ›é€ ",
    "share": "åˆ†äº«å‘ç°",
}

# æ‰€æœ‰å¯é€‰èŠ‚ç‚¹ï¼ˆç”¨ --nodes æŒ‡å®šï¼‰
ALL_NODES = {
    "create": "åˆ†äº«åˆ›é€ ",
    "share": "åˆ†äº«å‘ç°",
    "macos": "macOS",
    "chrome": "Chrome",
    "programmer": "ç¨‹åºå‘˜",
    "app": "App æ¨è",
}

# äº§å“/å·¥å…·ç›¸å…³å…³é”®è¯ï¼ˆç”¨äºä»è¯é¢˜ä¸­ç­›é€‰å‡ºæœ‰ä»·å€¼çš„å†…å®¹ï¼‰
TOOL_KEYWORDS = [
    "å·¥å…·", "tool", "app", "å¼€æº", "ç‹¬ç«‹å¼€å‘", "side project",
    "ä¸Šçº¿", "å‘å¸ƒ", "launch", "åˆ†äº«", "æ¨è", "æ•ˆç‡",
    "æ’ä»¶", "extension", "cli", "bot", "è‡ªåŠ¨åŒ–", "è„šæœ¬",
    "åšäº†", "å†™äº†", "å¼€å‘äº†", "æäº†", "é€ äº†", "æ–°é¡¹ç›®",
    "äº§å“", "saas", "å¼€ç®±", "ä½“éªŒ", "æµ‹è¯„", "ç¥å™¨",
    "github", "chrome", "vscode", "æµè§ˆå™¨", "æ¡Œé¢",
    "å…è´¹", "ä»˜è´¹", "è®¢é˜…", "ä¹°æ–­",
]


def _ensure_tmp_dir():
    os.makedirs(TMP_DIR, exist_ok=True)


def fetch_node_topics(node_name, page=1):
    """é€šè¿‡èŠ‚ç‚¹åè·å–è¯¥èŠ‚ç‚¹ä¸‹çš„è¯é¢˜åˆ—è¡¨ã€‚"""
    url = f"https://www.v2ex.com/api/topics/show.json?node_name={node_name}&p={page}"
    headers = {"User-Agent": "idea2mvp-skill/1.0"}
    req = urllib.request.Request(url, headers=headers)
    try:
        with urllib.request.urlopen(req, timeout=15) as resp:
            return json.loads(resp.read().decode())
    except urllib.error.HTTPError as e:
        print(f"âš ï¸ V2EX API error {e.code} for node '{node_name}'", file=sys.stderr)
        return []
    except urllib.error.URLError as e:
        print(f"âš ï¸ Network error for node '{node_name}': {e.reason}", file=sys.stderr)
        return []


def is_tool_related(topic):
    """åˆ¤æ–­è¯é¢˜æ˜¯å¦ä¸å·¥å…·/äº§å“/ç‹¬ç«‹å¼€å‘ç›¸å…³ã€‚"""
    text = f"{topic.get('title', '')} {topic.get('content', '')}".lower()
    return any(kw.lower() in text for kw in TOOL_KEYWORDS)


def matches_filter(topic, filter_keyword):
    """è‡ªå®šä¹‰å…³é”®è¯è¿‡æ»¤ã€‚"""
    if not filter_keyword:
        return True
    text = f"{topic.get('title', '')} {topic.get('content', '')}".lower()
    return filter_keyword.lower() in text


def format_as_text(topics, nodes_used):
    nodes_label = ", ".join(f"{ALL_NODES.get(n, n)}({n})" for n in nodes_used)
    lines = [f"V2EX äº§å“/å·¥å…·è¯é¢˜ â€” èŠ‚ç‚¹: {nodes_label}", "=" * 60, ""]

    for i, t in enumerate(topics, 1):
        title = t.get("title", "(no title)")
        url = t.get("url", "")
        node = t.get("node", {}).get("title", "N/A")
        author = t.get("member", {}).get("username", "N/A")
        replies = t.get("replies", 0)
        created = t.get("created", "")

        date_str = ""
        if created:
            try:
                date_str = datetime.fromtimestamp(created).strftime("%Y-%m-%d %H:%M")
            except (OSError, ValueError):
                date_str = str(created)

        content_raw = t.get("content", "") or ""
        content_preview = content_raw[:200]
        if len(content_raw) > 200:
            content_preview += "..."

        lines.append(f"#{i} {title}")
        lines.append(f"  èŠ‚ç‚¹: {node}  ä½œè€…: {author}  å›å¤: {replies}  æ—¥æœŸ: {date_str}")
        if content_preview:
            lines.append(f"  {content_preview}")
        lines.append(f"  {url}")
        lines.append("")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="V2EX äº§å“/å·¥å…·è¯é¢˜æœç´¢")
    parser.add_argument(
        "--nodes", nargs="+", default=None,
        help=f"æŒ‡å®šèŠ‚ç‚¹ï¼ˆé»˜è®¤: create shareï¼‰ã€‚å¯é€‰: {', '.join(ALL_NODES.keys())}"
    )
    parser.add_argument("--filter", type=str, default=None, help="è‡ªå®šä¹‰å…³é”®è¯è¿‡æ»¤")
    parser.add_argument(
        "--no-filter", action="store_true",
        help="ä¸åšå…³é”®è¯è¿‡æ»¤ï¼Œè¿”å›èŠ‚ç‚¹ä¸‹æ‰€æœ‰è¯é¢˜"
    )
    parser.add_argument(
        "--pages", type=int, default=1,
        help="æ¯ä¸ªèŠ‚ç‚¹è·å–çš„é¡µæ•°ï¼ˆé»˜è®¤ 1ï¼Œæ¯é¡µçº¦ 20 æ¡ï¼‰"
    )
    args = parser.parse_args()

    nodes = args.nodes or list(PRODUCT_NODES.keys())
    # éªŒè¯èŠ‚ç‚¹å
    for n in nodes:
        if n not in ALL_NODES:
            print(f"âš ï¸ æœªçŸ¥èŠ‚ç‚¹ '{n}'ï¼Œå¯é€‰: {', '.join(ALL_NODES.keys())}", file=sys.stderr)
            sys.exit(1)

    all_topics = []
    for node in nodes:
        node_label = ALL_NODES.get(node, node)
        print(f"ğŸ” è·å– V2EX [{node_label}] èŠ‚ç‚¹è¯é¢˜...", file=sys.stderr, flush=True)
        for page in range(1, args.pages + 1):
            topics = fetch_node_topics(node, page=page)
            if not topics:
                break
            all_topics.extend(topics)

    if not all_topics:
        print("âŒ æœªè·å–åˆ°ä»»ä½•è¯é¢˜", file=sys.stderr)
        sys.exit(1)

    print(f"ğŸ“¦ å…±è·å– {len(all_topics)} æ¡è¯é¢˜", file=sys.stderr, flush=True)

    # è¿‡æ»¤
    if args.filter:
        filtered = [t for t in all_topics if matches_filter(t, args.filter)]
        print(f"ğŸ” å…³é”®è¯ '{args.filter}' è¿‡æ»¤å: {len(filtered)} æ¡", file=sys.stderr, flush=True)
    elif not args.no_filter:
        filtered = [t for t in all_topics if is_tool_related(t)]
        print(f"ğŸ” å·¥å…·/äº§å“å…³é”®è¯è¿‡æ»¤å: {len(filtered)} æ¡", file=sys.stderr, flush=True)
    else:
        filtered = all_topics

    if not filtered:
        print("ğŸ’¡ è¿‡æ»¤åæ— ç»“æœï¼Œå°è¯• --no-filter æˆ–æ›´æ¢ --filter å…³é”®è¯", file=sys.stderr)
        # ä»ç„¶è¾“å‡ºæœªè¿‡æ»¤çš„ç»“æœä½œä¸ºå¤‡é€‰
        print("ğŸ“‹ è¾“å‡ºæœªè¿‡æ»¤çš„åŸå§‹ç»“æœä½œä¸ºå‚è€ƒ", file=sys.stderr)
        filtered = all_topics

    # æŒ‰å›å¤æ•°é™åºæ’åˆ—ï¼ˆçƒ­åº¦æŒ‡æ ‡ï¼‰
    filtered.sort(key=lambda t: t.get("replies", 0), reverse=True)

    text = format_as_text(filtered, nodes)
    _ensure_tmp_dir()
    with open(RESULT_FILE, "w", encoding="utf-8") as f:
        f.write(text)
    print(text)
    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜åˆ° {RESULT_FILE}", file=sys.stderr)


if __name__ == "__main__":
    main()
