#!/usr/bin/env python3
"""
å°‘æ•°æ´¾æ–‡ç« æœç´¢ä¸è¯¦æƒ…è·å–

é€šè¿‡å°‘æ•°æ´¾æœç´¢ API è·å–å·¥å…·/äº§å“ç›¸å…³æ–‡ç« ï¼Œæ”¯æŒè·å–æ–‡ç« å®Œæ•´æ­£æ–‡ã€‚
æ— éœ€è®¤è¯ï¼Œæ— éœ€ Tokenï¼Œç›´æ¥ GET è¯·æ±‚å³å¯ã€‚

Usage:
    # æœç´¢æ–‡ç« åˆ—è¡¨
    python3 sspai_search.py --keyword "æ•ˆç‡å·¥å…·"
    python3 sspai_search.py --keyword "AIå·¥å…·æ¨è"
    python3 sspai_search.py --keyword "ç‹¬ç«‹å¼€å‘è€…" --limit 20

    # è·å–å•ç¯‡æ–‡ç« è¯¦æƒ…
    python3 sspai_search.py --detail 60079
    python3 sspai_search.py --detail 60079 73051 55239
"""

import argparse
import json
import os
import re
import sys
import time
import urllib.parse
import urllib.request
import urllib.error
from datetime import datetime

TMP_DIR = os.path.join(os.getcwd(), "tmp")
RESULT_FILE = os.path.join(TMP_DIR, "sspai_results.txt")

SEARCH_API = "https://sspai.com/api/v1/search/all/info/get"
DETAIL_API = "https://sspai.com/api/v1/article/info/get"
USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/145.0.0.0 Safari/537.36"
)

# é»˜è®¤æœç´¢å…³é”®è¯åˆ—è¡¨ï¼ˆä¸äº§å“/å·¥å…·å‘ç°ç›¸å…³ï¼‰
DEFAULT_KEYWORDS = ["æ•ˆç‡å·¥å…·", "ç‹¬ç«‹å¼€å‘", "å°å·¥å…·æ¨è"]


def _ensure_tmp_dir():
    os.makedirs(TMP_DIR, exist_ok=True)


def _request_json(url, referer="https://sspai.com/"):
    """å‘é€ GET è¯·æ±‚å¹¶è¿”å› JSONã€‚"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/json",
        "Referer": referer,
    }
    req = urllib.request.Request(url, headers=headers)
    try:
        with urllib.request.urlopen(req, timeout=15) as resp:
            return json.loads(resp.read().decode())
    except urllib.error.HTTPError as e:
        print(f"âš ï¸ å°‘æ•°æ´¾ API error {e.code}: {url}", file=sys.stderr)
        return None
    except urllib.error.URLError as e:
        print(f"âš ï¸ Network error: {e.reason}", file=sys.stderr)
        return None


def search_sspai(keyword, etime=None):
    """æœç´¢å°‘æ•°æ´¾æ–‡ç« ã€‚"""
    if etime is None:
        etime = int(time.time())
    params = urllib.parse.urlencode({
        "title": keyword,
        "search_types": "",
        "stime": 0,
        "etime": etime,
    })
    url = f"{SEARCH_API}?{params}"
    referer = f"https://sspai.com/search?q={urllib.parse.quote(keyword)}"
    return _request_json(url, referer)


def fetch_article_detail(article_id):
    """è·å–å•ç¯‡æ–‡ç« çš„å®Œæ•´è¯¦æƒ…ï¼ˆå«æ­£æ–‡ï¼‰ã€‚

    Args:
        article_id: æ–‡ç«  IDï¼ˆæ•°å­—ï¼‰

    Returns:
        dict with article data including 'body' (HTML), 'title', 'tags', etc.
    """
    params = urllib.parse.urlencode({
        "id": article_id,
        "support_webp": "true",
        "view": "second",
    })
    url = f"{DETAIL_API}?{params}"
    referer = f"https://sspai.com/post/{article_id}"
    resp = _request_json(url, referer)
    if not resp or resp.get("error", -1) != 0:
        return None
    return resp.get("data", {})


def html_to_text(html):
    """ç®€æ˜“ HTML â†’ çº¯æ–‡æœ¬è½¬æ¢ã€‚"""
    text = re.sub(r"<br\s*/?>", "\n", html)
    text = re.sub(r"</p>", "\n", text)
    text = re.sub(r"</h[1-6]>", "\n\n", text)
    text = re.sub(r"</li>", "\n", text)
    text = re.sub(r"<hr\s*/?>", "\n---\n", text)
    text = re.sub(r"<[^>]+>", "", text)
    # åˆå¹¶è¿ç»­ç©ºè¡Œ
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def extract_articles(response):
    """ä» API å“åº”ä¸­æå–æ–‡ç« åˆ—è¡¨ã€‚"""
    if not response or response.get("error", -1) != 0:
        return []
    data = response.get("data", {})
    return data.get("articles", [])


def format_as_text(all_articles, keywords_used):
    """å°†æ–‡ç« åˆ—è¡¨æ ¼å¼åŒ–ä¸ºçº¯æ–‡æœ¬ã€‚"""
    lines = [
        f"å°‘æ•°æ´¾æ–‡ç« æœç´¢ç»“æœ â€” å…³é”®è¯: {', '.join(keywords_used)}",
        "=" * 60,
        "",
    ]

    # å»é‡ï¼ˆåŒä¸€æ–‡ç« å¯èƒ½è¢«ä¸åŒå…³é”®è¯å‘½ä¸­ï¼‰
    seen_ids = set()
    unique_articles = []
    for a in all_articles:
        aid = a.get("id", 0)
        if aid not in seen_ids:
            seen_ids.add(aid)
            unique_articles.append(a)

    # æŒ‰ like_count é™åºæ’åˆ—
    unique_articles.sort(key=lambda a: a.get("like_count", 0), reverse=True)

    for i, a in enumerate(unique_articles, 1):
        title = a.get("title", "(no title)")
        slug = a.get("slug", "")
        aid = a.get("id", "")
        summary = a.get("summary", "")
        likes = a.get("like_count", 0)
        comments = a.get("comment_count", 0)
        released = a.get("released_time", 0)

        # æ„å»ºé“¾æ¥
        url = f"https://sspai.com/post/{aid}" if aid else ""

        date_str = ""
        if released:
            try:
                date_str = datetime.fromtimestamp(released).strftime("%Y-%m-%d")
            except (OSError, ValueError):
                date_str = str(released)

        # ä½œè€…
        author_info = a.get("author", {})
        author = author_info.get("nickname", "N/A") if author_info else "N/A"

        lines.append(f"#{i} {title}")
        lines.append(f"  ä½œè€…: {author}  ğŸ‘ {likes}  ğŸ’¬ {comments}  æ—¥æœŸ: {date_str}")
        if summary:
            # æˆªæ–­è¿‡é•¿æ‘˜è¦
            if len(summary) > 200:
                summary = summary[:200] + "..."
            lines.append(f"  {summary}")
        if url:
            lines.append(f"  {url}")
        lines.append("")

    lines.append(f"å…± {len(unique_articles)} ç¯‡æ–‡ç« ï¼ˆå·²å»é‡ã€æŒ‰ç‚¹èµæ•°æ’åºï¼‰")
    return "\n".join(lines)


def format_detail_as_text(detail):
    """å°†æ–‡ç« è¯¦æƒ…æ ¼å¼åŒ–ä¸ºçº¯æ–‡æœ¬ã€‚"""
    title = detail.get("title", "(no title)")
    aid = detail.get("id", "")
    body_html = detail.get("body", "")
    keywords = detail.get("keywords", "")
    summary = detail.get("summary", "")

    # ä½œè€…
    author_info = detail.get("author", {})
    author = author_info.get("nickname", "N/A") if author_info else "N/A"

    # äº’åŠ¨æ•°æ®
    counts = detail.get("article_count", {})
    likes = counts.get("like_count", 0)
    comments = counts.get("comment_count", 0)
    views = counts.get("views_count", 0)

    released = detail.get("released_time", 0)
    date_str = ""
    if released:
        try:
            date_str = datetime.fromtimestamp(released).strftime("%Y-%m-%d")
        except (OSError, ValueError):
            date_str = str(released)

    # æ ‡ç­¾
    tags = detail.get("tags", [])
    tag_names = [t.get("title", "") for t in tags if t.get("title")]

    body_text = html_to_text(body_html)

    lines = [
        f"{'=' * 60}",
        f"ğŸ“„ {title}",
        f"{'=' * 60}",
        f"ä½œè€…: {author}  æ—¥æœŸ: {date_str}",
        f"ğŸ‘ {likes}  ğŸ’¬ {comments}  ğŸ‘€ {views}",
    ]
    if tag_names:
        lines.append(f"æ ‡ç­¾: {', '.join(tag_names)}")
    if keywords:
        lines.append(f"å…³é”®è¯: {keywords}")
    lines.append(f"é“¾æ¥: https://sspai.com/post/{aid}")
    lines.append("")
    if summary:
        lines.append(f"æ‘˜è¦: {summary}")
        lines.append("")
    lines.append("-" * 60)
    lines.append(body_text)
    lines.append("-" * 60)
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="å°‘æ•°æ´¾æ–‡ç« æœç´¢ä¸è¯¦æƒ…è·å–")
    parser.add_argument(
        "--keyword", type=str, default=None,
        help="æœç´¢å…³é”®è¯ï¼ˆå•ä¸ªï¼‰"
    )
    parser.add_argument(
        "--keywords", nargs="+", default=None,
        help="å¤šä¸ªæœç´¢å…³é”®è¯"
    )
    parser.add_argument(
        "--limit", type=int, default=None,
        help="é™åˆ¶è¾“å‡ºæ–‡ç« æ•°é‡"
    )
    parser.add_argument(
        "--detail", nargs="+", type=int, default=None,
        help="è·å–æ–‡ç« è¯¦æƒ…ï¼Œä¼ å…¥ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡ç«  ID"
    )
    parser.add_argument(
        "--output", type=str, default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ tmp/sspai_results.txtï¼‰"
    )
    args = parser.parse_args()

    _ensure_tmp_dir()

    # æ¨¡å¼ä¸€ï¼šè·å–æ–‡ç« è¯¦æƒ…
    if args.detail:
        all_text = []
        for aid in args.detail:
            print(f"ğŸ“– è·å–æ–‡ç« è¯¦æƒ…: {aid}...", file=sys.stderr, flush=True)
            detail = fetch_article_detail(aid)
            if detail:
                text = format_detail_as_text(detail)
                all_text.append(text)
                print(f"  â†’ {detail.get('title', '?')}", file=sys.stderr, flush=True)
            else:
                print(f"  âš ï¸ æœªèƒ½è·å–æ–‡ç«  {aid}", file=sys.stderr)
            if len(args.detail) > 1:
                time.sleep(0.5)

        if not all_text:
            print("âŒ æœªèƒ½è·å–ä»»ä½•æ–‡ç« è¯¦æƒ…", file=sys.stderr)
            sys.exit(1)

        output = "\n\n".join(all_text)
        detail_file = args.output or os.path.join(TMP_DIR, "sspai_detail.txt")
        with open(detail_file, "w", encoding="utf-8") as f:
            f.write(output)
        print(output)
        print(f"\nğŸ“„ è¯¦æƒ…å·²ä¿å­˜åˆ° {detail_file}", file=sys.stderr)
        return

    # æ¨¡å¼äºŒï¼šæœç´¢æ–‡ç« åˆ—è¡¨
    if args.keyword:
        keywords = [args.keyword]
    elif args.keywords:
        keywords = args.keywords
    else:
        keywords = DEFAULT_KEYWORDS

    all_articles = []
    for kw in keywords:
        print(f"ğŸ” æœç´¢å°‘æ•°æ´¾: {kw}...", file=sys.stderr, flush=True)
        resp = search_sspai(kw)
        articles = extract_articles(resp)
        print(f"  â†’ è·å– {len(articles)} ç¯‡æ–‡ç« ", file=sys.stderr, flush=True)
        all_articles.extend(articles)

        # å¤šå…³é”®è¯æœç´¢é—´åŠ çŸ­æš‚å»¶è¿Ÿ
        if len(keywords) > 1:
            time.sleep(0.5)

    if not all_articles:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æ–‡ç« ", file=sys.stderr)
        sys.exit(1)

    # é™åˆ¶æ•°é‡
    if args.limit:
        # å…ˆå»é‡æ’åºå†æˆªæ–­
        seen_ids = set()
        unique = []
        for a in all_articles:
            aid = a.get("id", 0)
            if aid not in seen_ids:
                seen_ids.add(aid)
                unique.append(a)
        unique.sort(key=lambda a: a.get("like_count", 0), reverse=True)
        all_articles = unique[:args.limit]

    text = format_as_text(all_articles, keywords)

    output_file = args.output or RESULT_FILE
    _ensure_tmp_dir()
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(text)

    print(text)
    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜åˆ° {output_file}", file=sys.stderr)


if __name__ == "__main__":
    main()
