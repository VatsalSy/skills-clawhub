#!/usr/bin/env python3
"""
Product Hunt Trending - é€šè¿‡å®˜æ–¹ API v2 è·å–çƒ­é—¨äº§å“

éœ€è¦ç¯å¢ƒå˜é‡ï¼šPRODUCTHUNT_TOKENï¼ˆDeveloper Tokenï¼‰
è·å–æ–¹å¼ï¼šhttps://www.producthunt.com/v2/oauth/applications â†’ åˆ›å»ºåº”ç”¨ â†’ è·å– Developer Token

ä½¿ç”¨æ–¹å¼ï¼š
  python3 producthunt_trending.py
  python3 producthunt_trending.py --days 3 --limit 20
  python3 producthunt_trending.py --topic productivity --days 7

ç»“æœè‡ªåŠ¨ä¿å­˜åˆ° tmp/ph_results.txt
"""

import argparse
import json
import os
import sys
import urllib.request
import urllib.error
from datetime import datetime, timedelta, timezone

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from utils import load_env

API_URL = "https://api.producthunt.com/v2/api/graphql"
TMP_DIR = os.path.join(os.getcwd(), "tmp")
RESULT_FILE = os.path.join(TMP_DIR, "ph_results.txt")

GRAPHQL_QUERY = """
{
  posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s", topic: "%s") {
    nodes {
      id
      name
      tagline
      description
      votesCount
      createdAt
      featuredAt
      website
      url
    }
    pageInfo {
      hasNextPage
      endCursor
    }
  }
}
"""

GRAPHQL_QUERY_NO_TOPIC = """
{
  posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
    nodes {
      id
      name
      tagline
      description
      votesCount
      createdAt
      featuredAt
      website
      url
    }
    pageInfo {
      hasNextPage
      endCursor
    }
  }
}
"""


def fetch_posts(token, date_after, date_before, topic=None, limit=30):
    """é€šè¿‡ PH API v2 è·å–äº§å“åˆ—è¡¨ã€‚"""
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}",
        "User-Agent": "Idea2MVP/1.0",
    }

    all_posts = []
    cursor = ""
    has_next = True

    while has_next and len(all_posts) < limit:
        if topic:
            query = GRAPHQL_QUERY % (date_after, date_before, cursor, topic)
        else:
            query = GRAPHQL_QUERY_NO_TOPIC % (date_after, date_before, cursor)

        body = json.dumps({"query": query}).encode("utf-8")
        req = urllib.request.Request(API_URL, data=body, headers=headers)

        try:
            with urllib.request.urlopen(req, timeout=15) as resp:
                data = json.loads(resp.read().decode())
        except urllib.error.HTTPError as e:
            if e.code == 401:
                print("âŒ Token æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥ PRODUCTHUNT_TOKEN ç¯å¢ƒå˜é‡", file=sys.stderr)
            elif e.code == 429:
                print("âŒ API è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•", file=sys.stderr)
            else:
                print(f"âŒ API é”™è¯¯ {e.code}: {e.read().decode()[:200]}", file=sys.stderr)
            return None
        except urllib.error.URLError as e:
            print(f"âŒ ç½‘ç»œé”™è¯¯: {e.reason}", file=sys.stderr)
            return None

        if "errors" in data:
            print(f"âŒ GraphQL é”™è¯¯: {data['errors'][0].get('message', '')}", file=sys.stderr)
            return None

        posts_data = data.get("data", {}).get("posts", {})
        nodes = posts_data.get("nodes", [])
        all_posts.extend(nodes)

        page_info = posts_data.get("pageInfo", {})
        has_next = page_info.get("hasNextPage", False)
        cursor = page_info.get("endCursor", "")

    # æŒ‰ç¥¨æ•°æ’åºå– top N
    all_posts.sort(key=lambda x: x.get("votesCount", 0), reverse=True)
    return all_posts[:limit]


def format_as_text(posts, date_after, date_before, topic=None):
    """å°†äº§å“åˆ—è¡¨æ ¼å¼åŒ–ä¸ºçº¯æ–‡æœ¬æ‘˜è¦ã€‚"""
    title = f"Product Hunt çƒ­é—¨äº§å“ ({date_after} ~ {date_before})"
    if topic:
        title += f" [topic: {topic}]"
    lines = [title, "=" * 50, ""]

    for i, p in enumerate(posts, 1):
        desc = p.get("description", "") or ""
        if len(desc) > 150:
            desc = desc[:150] + "..."
        votes = p.get("votesCount", 0)
        featured = "âœ… Featured" if p.get("featuredAt") else ""

        lines.append(f"#{i} {p['name']} ({votes} votes) {featured}".strip())
        lines.append(f"  {p.get('tagline', '')}")
        if desc:
            lines.append(f"  {desc}")
        if p.get("website"):
            lines.append(f"  Website: {p['website']}")
        lines.append(f"  {p.get('url', '')}")
        lines.append("")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Product Hunt çƒ­é—¨äº§å“è·å–ï¼ˆå®˜æ–¹ API v2ï¼‰")
    parser.add_argument("--days", type=int, default=1,
                        help="è·å–æœ€è¿‘ N å¤©çš„äº§å“ (default: 1)")
    parser.add_argument("--limit", type=int, default=15,
                        help="å±•ç¤ºäº§å“æ•°é‡ä¸Šé™ (default: 15)")
    parser.add_argument("--topic", type=str, default=None,
                        help="æŒ‰ topic ç­›é€‰ï¼Œå¦‚ productivity, developer-tools, artificial-intelligence")
    args = parser.parse_args()

    load_env()
    token = os.environ.get("PRODUCTHUNT_TOKEN")
    if not token:
        print(
            "âš ï¸ æœªé…ç½® PRODUCTHUNT_TOKENï¼Œæ— æ³•ä½¿ç”¨ APIã€‚\n"
            "è¯·æ”¹ç”¨ web_search æœç´¢ Product Hunt ç›¸å…³ä¿¡æ¯ï¼Œæ¨èæœç´¢è¯ï¼š\n"
            "  - \"Product Hunt\" best new tools {å½“æœˆ} {å½“å¹´}\n"
            "  - \"Product Hunt\" trending productivity tools {å½“å¹´}\n"
            "  - site:producthunt.com top products this week\n\n"
            "å¦‚éœ€é…ç½® Tokenï¼š\n"
            "  1. è®¿é—® https://www.producthunt.com/v2/oauth/applications\n"
            "  2. åˆ›å»ºåº”ç”¨ï¼Œè·å– Developer Token\n"
            "  3. åœ¨é¡¹ç›®å·¥ä½œç›®å½•åˆ›å»º .env.idea2mvp æ–‡ä»¶ï¼Œå†™å…¥ï¼šPRODUCTHUNT_TOKEN=your_token",
            file=sys.stderr,
        )
        sys.exit(1)

    now = datetime.now(timezone.utc)
    date_before = (now - timedelta(days=1)).strftime("%Y-%m-%d")
    date_after = (now - timedelta(days=args.days)).strftime("%Y-%m-%d")

    topic_str = f" [topic: {args.topic}]" if args.topic else ""
    print(f"ğŸ” è·å– Product Hunt çƒ­é—¨äº§å“ ({date_after} ~ {date_before}){topic_str}...", file=sys.stderr)

    posts = fetch_posts(token, date_after, date_before, args.topic, args.limit)
    if posts is None:
        sys.exit(1)

    if not posts:
        print("ğŸ’¡ è¯¥æ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°äº§å“", file=sys.stderr)
        sys.exit(1)

    text = format_as_text(posts, date_after, date_before, args.topic)
    os.makedirs(TMP_DIR, exist_ok=True)
    with open(RESULT_FILE, "w", encoding="utf-8") as f:
        f.write(text)
    print(text)
    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜åˆ° {RESULT_FILE}", file=sys.stderr)


if __name__ == "__main__":
    main()
